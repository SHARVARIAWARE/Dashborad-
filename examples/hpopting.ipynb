{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2CyFMk0IRav"
      },
      "source": [
        "# Running hyperparameter optimization on Chemprop model using RayTune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMQ8AbQSIRaw"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/chemprop/chemprop/blob/main/examples/hpopting.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cuUWVxBIIRaw",
        "outputId": "887c8cd9-40c0-41a7-a127-ae2b6f3593af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'chemprop'...\n",
            "remote: Enumerating objects: 24987, done.\u001b[K\n",
            "remote: Counting objects: 100% (187/187), done.\u001b[K\n",
            "remote: Compressing objects: 100% (143/143), done.\u001b[K\n",
            "remote: Total 24987 (delta 115), reused 44 (delta 44), pack-reused 24800 (from 3)\u001b[K\n",
            "Receiving objects: 100% (24987/24987), 817.53 MiB | 25.45 MiB/s, done.\n",
            "Resolving deltas: 100% (17934/17934), done.\n",
            "/content/chemprop\n",
            "Processing /content/chemprop\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting lightning>=2.0 (from chemprop==2.1.2)\n",
            "  Downloading lightning-2.5.1.post0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from chemprop==2.1.2) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from chemprop==2.1.2) (2.2.2)\n",
            "Collecting rdkit (from chemprop==2.1.2)\n",
            "  Downloading rdkit-2024.9.6-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from chemprop==2.1.2) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from chemprop==2.1.2) (1.15.2)\n",
            "Requirement already satisfied: torch>=2.1 in /usr/local/lib/python3.11/dist-packages (from chemprop==2.1.2) (2.6.0+cu124)\n",
            "Collecting astartes[molecules] (from chemprop==2.1.2)\n",
            "  Downloading astartes-1.3.0-py3-none-any.whl.metadata (26 kB)\n",
            "Collecting ConfigArgParse (from chemprop==2.1.2)\n",
            "  Downloading ConfigArgParse-1.7-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from chemprop==2.1.2) (13.9.4)\n",
            "Collecting descriptastorus (from chemprop==2.1.2)\n",
            "  Downloading descriptastorus-2.8.0-py3-none-any.whl.metadata (364 bytes)\n",
            "Collecting ray[tune] (from chemprop==2.1.2)\n",
            "  Downloading ray-2.45.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.11/dist-packages (from chemprop==2.1.2) (0.2.7)\n",
            "Collecting optuna (from chemprop==2.1.2)\n",
            "  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.11/dist-packages (from lightning>=2.0->chemprop==2.1.2) (6.0.2)\n",
            "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0->chemprop==2.1.2) (2025.3.2)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning>=2.0->chemprop==2.1.2)\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.11/dist-packages (from lightning>=2.0->chemprop==2.1.2) (24.2)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning>=2.0->chemprop==2.1.2)\n",
            "  Downloading torchmetrics-1.7.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from lightning>=2.0->chemprop==2.1.2) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from lightning>=2.0->chemprop==2.1.2) (4.13.2)\n",
            "Collecting pytorch-lightning (from lightning>=2.0->chemprop==2.1.2)\n",
            "  Downloading pytorch_lightning-2.5.1.post0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->chemprop==2.1.2) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->chemprop==2.1.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->chemprop==2.1.2) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.1->chemprop==2.1.2)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.1->chemprop==2.1.2)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.1->chemprop==2.1.2)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.1->chemprop==2.1.2)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.1->chemprop==2.1.2)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.1->chemprop==2.1.2)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.1->chemprop==2.1.2)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.1->chemprop==2.1.2)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.1->chemprop==2.1.2)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->chemprop==2.1.2) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->chemprop==2.1.2) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->chemprop==2.1.2) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.1->chemprop==2.1.2)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->chemprop==2.1.2) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->chemprop==2.1.2) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1->chemprop==2.1.2) (1.3.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from astartes[molecules]->chemprop==2.1.2) (0.9.0)\n",
            "Collecting aimsim-core (from astartes[molecules]->chemprop==2.1.2)\n",
            "  Downloading aimsim_core-2.2.2-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting pandas-flavor (from descriptastorus->chemprop==2.1.2)\n",
            "  Downloading pandas_flavor-0.7.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from hyperopt->chemprop==2.1.2) (1.17.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from hyperopt->chemprop==2.1.2) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from hyperopt->chemprop==2.1.2) (3.1.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (from hyperopt->chemprop==2.1.2) (0.10.9.7)\n",
            "Collecting alembic>=1.5.0 (from optuna->chemprop==2.1.2)\n",
            "  Downloading alembic-1.15.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna->chemprop==2.1.2)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna->chemprop==2.1.2) (2.0.40)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->chemprop==2.1.2) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->chemprop==2.1.2) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->chemprop==2.1.2) (2025.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray[tune]; extra == \"hpopt\"->chemprop==2.1.2) (8.1.8)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from ray[tune]; extra == \"hpopt\"->chemprop==2.1.2) (4.23.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray[tune]; extra == \"hpopt\"->chemprop==2.1.2) (1.1.0)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.11/dist-packages (from ray[tune]; extra == \"hpopt\"->chemprop==2.1.2) (5.29.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from ray[tune]; extra == \"hpopt\"->chemprop==2.1.2) (2.32.3)\n",
            "Collecting tensorboardX>=1.9 (from ray[tune]; extra == \"hpopt\"->chemprop==2.1.2)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: pyarrow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from ray[tune]; extra == \"hpopt\"->chemprop==2.1.2) (18.1.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit->chemprop==2.1.2) (11.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->chemprop==2.1.2) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->chemprop==2.1.2) (2.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->chemprop==2.1.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->chemprop==2.1.2) (3.6.0)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna->chemprop==2.1.2) (1.1.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0->chemprop==2.1.2) (3.11.15)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning>=2.0->chemprop==2.1.2) (75.2.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->chemprop==2.1.2) (0.1.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna->chemprop==2.1.2) (3.2.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from aimsim-core->astartes[molecules]->chemprop==2.1.2) (5.9.5)\n",
            "Collecting padelpy (from aimsim-core->astartes[molecules]->chemprop==2.1.2)\n",
            "  Downloading padelpy-0.1.16-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting mhfp (from aimsim-core->astartes[molecules]->chemprop==2.1.2)\n",
            "  Downloading mhfp-1.9.6-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting mordredcommunity (from aimsim-core->astartes[molecules]->chemprop==2.1.2)\n",
            "  Downloading mordredcommunity-2.0.6-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting multiprocess>=0.70 (from aimsim-core->astartes[molecules]->chemprop==2.1.2)\n",
            "  Downloading multiprocess-0.70.18-py311-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1->chemprop==2.1.2) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray[tune]; extra == \"hpopt\"->chemprop==2.1.2) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray[tune]; extra == \"hpopt\"->chemprop==2.1.2) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray[tune]; extra == \"hpopt\"->chemprop==2.1.2) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray[tune]; extra == \"hpopt\"->chemprop==2.1.2) (0.24.0)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.11/dist-packages (from pandas-flavor->descriptastorus->chemprop==2.1.2) (2025.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->ray[tune]; extra == \"hpopt\"->chemprop==2.1.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->ray[tune]; extra == \"hpopt\"->chemprop==2.1.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->ray[tune]; extra == \"hpopt\"->chemprop==2.1.2) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->ray[tune]; extra == \"hpopt\"->chemprop==2.1.2) (2025.4.26)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0->chemprop==2.1.2) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0->chemprop==2.1.2) (1.3.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0->chemprop==2.1.2) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0->chemprop==2.1.2) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0->chemprop==2.1.2) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0->chemprop==2.1.2) (1.20.0)\n",
            "Collecting dill>=0.4.0 (from multiprocess>=0.70->aimsim-core->astartes[molecules]->chemprop==2.1.2)\n",
            "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Downloading lightning-2.5.1.post0-py3-none-any.whl (819 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.0/819.0 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m106.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\n",
            "Downloading descriptastorus-2.8.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rdkit-2024.9.6-cp311-cp311-manylinux_2_28_x86_64.whl (34.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.3/34.3 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.15.2-py3-none-any.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.7.1-py3-none-any.whl (961 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aimsim_core-2.2.2-py3-none-any.whl (187 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.8/187.8 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astartes-1.3.0-py3-none-any.whl (38 kB)\n",
            "Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading pandas_flavor-0.7.0-py3-none-any.whl (8.4 kB)\n",
            "Downloading pytorch_lightning-2.5.1.post0-py3-none-any.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.1/823.1 kB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ray-2.45.0-cp311-cp311-manylinux2014_x86_64.whl (68.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.4/68.4 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.18-py311-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mhfp-1.9.6-py3-none-any.whl (9.7 kB)\n",
            "Downloading mordredcommunity-2.0.6-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.0/176.0 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading padelpy-0.1.16-py3-none-any.whl (20.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: chemprop\n",
            "  Building wheel for chemprop (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chemprop: filename=chemprop-2.1.2-py3-none-any.whl size=112008 sha256=1208cdc961785a04ad1791efe1f09e0b0c9d3f12307ced8b357b2d4a41aa6679\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-f9vyu0tr/wheels/1e/b1/74/9347ccba982e11cbbdcb34bdf296d15c1fea4f8d3df8e0d8c3\n",
            "Successfully built chemprop\n",
            "Installing collected packages: mhfp, tensorboardX, rdkit, padelpy, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, dill, ConfigArgParse, colorlog, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, mordredcommunity, alembic, optuna, nvidia-cusolver-cu12, astartes, aimsim-core, ray, pandas-flavor, torchmetrics, descriptastorus, pytorch-lightning, lightning, chemprop\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed ConfigArgParse-1.7 aimsim-core-2.2.2 alembic-1.15.2 astartes-1.3.0 chemprop-2.1.2 colorlog-6.9.0 descriptastorus-2.8.0 dill-0.4.0 lightning-2.5.1.post0 lightning-utilities-0.14.3 mhfp-1.9.6 mordredcommunity-2.0.6 multiprocess-0.70.18 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 optuna-4.3.0 padelpy-0.1.16 pandas-flavor-0.7.0 pytorch-lightning-2.5.1.post0 ray-2.45.0 rdkit-2024.9.6 tensorboardX-2.6.2.2 torchmetrics-1.7.1\n",
            "/content/chemprop/examples\n"
          ]
        }
      ],
      "source": [
        "# Install chemprop from GitHub if running in Google Colab\n",
        "import os\n",
        "\n",
        "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
        "    try:\n",
        "        import chemprop\n",
        "    except ImportError:\n",
        "        !git clone https://github.com/chemprop/chemprop.git\n",
        "        %cd chemprop\n",
        "        !pip install \".[hpopt]\"\n",
        "        %cd examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I39lT6sUIRax"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1s58uOgYIRax"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "from lightning import pytorch as pl\n",
        "import ray\n",
        "from ray import tune\n",
        "from ray.train import CheckpointConfig, RunConfig, ScalingConfig\n",
        "from ray.train.lightning import (RayDDPStrategy, RayLightningEnvironment,\n",
        "                                 RayTrainReportCallback, prepare_trainer)\n",
        "from ray.train.torch import TorchTrainer\n",
        "from ray.tune.search.hyperopt import HyperOptSearch\n",
        "from ray.tune.search.optuna import OptunaSearch\n",
        "from ray.tune.schedulers import FIFOScheduler\n",
        "\n",
        "from chemprop import data, featurizers, models, nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vZ-6KLRXIRay"
      },
      "outputs": [],
      "source": [
        "chemprop_dir = Path.cwd().parent\n",
        "input_path = \"/content/Augmented_data.csv\" # path to your data .csv file\n",
        "num_workers = 0 # number of workers for dataloader. 0 means using main process for data loading\n",
        "smiles_column = 'SMILES ' # name of the column containing SMILES strings\n",
        "target_columns = ['Status'] # list of names of the columns containing targets\n",
        "\n",
        "hpopt_save_dir = Path.cwd() / \"hpopt\" # directory to save hyperopt results\n",
        "hpopt_save_dir.mkdir(exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96s9zkWlIRay"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ck4uRcM1IRaz",
        "outputId": "d5267416-4d11-4220-8980-b9866e2d3521",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Unnamed: 0 Catalog Number   \\\n",
              "0              0           S7718   \n",
              "1              1           S2679   \n",
              "2              2           S7511   \n",
              "3              3           S1393   \n",
              "4              4           S7648   \n",
              "...          ...             ...   \n",
              "4681        4724             NaN   \n",
              "4682        4725             NaN   \n",
              "4683        4726             NaN   \n",
              "4684        4727             NaN   \n",
              "4685        4728             NaN   \n",
              "\n",
              "                                                SMILES  Active/Inactive   \\\n",
              "0     CN(C)CCNC(=O)C1=CC=CN2C(=O)C3=C(C=C4C=CC=CC4=C...          Active    \n",
              "1     Cl.CN1CCC(C(O)C1)C2=C3OC(=CC(=O)C3=C(O)C=C2O)C...          Active    \n",
              "2     CC(C)C1=C2C=C(C=CC2=N[N]1C)C3=CC=NC(=N3)NC4CCC...          Active    \n",
              "3     COC1=CC=CC2=C1C(=O)C3=C(O)C4=C(CC(O)(CC4OC5CC(...          Active    \n",
              "4     Cl.Cl.CC(CN(C)C)C1=CC=C(C=C1)C2=C3C(=C(C)C=C2O...          Active    \n",
              "...                                                 ...              ...   \n",
              "4681  [1*]C(=O)C(C)CC.[13*]C1CC(O)CC(=O)O1.[3*]O[3*]...          Active    \n",
              "4682  [15*]C1CCC=C2C=CC(C)C([15*])C21.[1*]C(=O)C(C)C...          Active    \n",
              "4683  [9*]n1c(=O)c([16*])cc2c(C)nc(N)nc21.[3*]O[3*]....          Active    \n",
              "4684  [14*]c1ccc([16*])cn1.[4*]CCO.[15*]C1CCC([15*])...          Active    \n",
              "4685  [9*]n1c(=O)c([16*])cc2c(C)nc(N)nc21.[15*]C1CCC...          Active    \n",
              "\n",
              "        Remark  Status  \n",
              "0     Training       1  \n",
              "1     Training       1  \n",
              "2     Training       1  \n",
              "3     Training       1  \n",
              "4     Training       1  \n",
              "...        ...     ...  \n",
              "4681       NaN       1  \n",
              "4682       NaN       1  \n",
              "4683       NaN       1  \n",
              "4684       NaN       1  \n",
              "4685       NaN       1  \n",
              "\n",
              "[4686 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f2ce289e-62e3-44da-8806-723b55d187f4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Catalog Number</th>\n",
              "      <th>SMILES</th>\n",
              "      <th>Active/Inactive</th>\n",
              "      <th>Remark</th>\n",
              "      <th>Status</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>S7718</td>\n",
              "      <td>CN(C)CCNC(=O)C1=CC=CN2C(=O)C3=C(C=C4C=CC=CC4=C...</td>\n",
              "      <td>Active</td>\n",
              "      <td>Training</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>S2679</td>\n",
              "      <td>Cl.CN1CCC(C(O)C1)C2=C3OC(=CC(=O)C3=C(O)C=C2O)C...</td>\n",
              "      <td>Active</td>\n",
              "      <td>Training</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>S7511</td>\n",
              "      <td>CC(C)C1=C2C=C(C=CC2=N[N]1C)C3=CC=NC(=N3)NC4CCC...</td>\n",
              "      <td>Active</td>\n",
              "      <td>Training</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>S1393</td>\n",
              "      <td>COC1=CC=CC2=C1C(=O)C3=C(O)C4=C(CC(O)(CC4OC5CC(...</td>\n",
              "      <td>Active</td>\n",
              "      <td>Training</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>S7648</td>\n",
              "      <td>Cl.Cl.CC(CN(C)C)C1=CC=C(C=C1)C2=C3C(=C(C)C=C2O...</td>\n",
              "      <td>Active</td>\n",
              "      <td>Training</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4681</th>\n",
              "      <td>4724</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[1*]C(=O)C(C)CC.[13*]C1CC(O)CC(=O)O1.[3*]O[3*]...</td>\n",
              "      <td>Active</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4682</th>\n",
              "      <td>4725</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[15*]C1CCC=C2C=CC(C)C([15*])C21.[1*]C(=O)C(C)C...</td>\n",
              "      <td>Active</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4683</th>\n",
              "      <td>4726</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[9*]n1c(=O)c([16*])cc2c(C)nc(N)nc21.[3*]O[3*]....</td>\n",
              "      <td>Active</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4684</th>\n",
              "      <td>4727</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[14*]c1ccc([16*])cn1.[4*]CCO.[15*]C1CCC([15*])...</td>\n",
              "      <td>Active</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4685</th>\n",
              "      <td>4728</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[9*]n1c(=O)c([16*])cc2c(C)nc(N)nc21.[15*]C1CCC...</td>\n",
              "      <td>Active</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4686 rows × 6 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f2ce289e-62e3-44da-8806-723b55d187f4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f2ce289e-62e3-44da-8806-723b55d187f4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f2ce289e-62e3-44da-8806-723b55d187f4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-29b5c7c3-0305-4eb2-b480-ff981b3483df\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-29b5c7c3-0305-4eb2-b480-ff981b3483df')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-29b5c7c3-0305-4eb2-b480-ff981b3483df button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_a7402b44-0d5f-407e-8c9f-7fe9ae05597b\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_input')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_a7402b44-0d5f-407e-8c9f-7fe9ae05597b button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_input');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_input",
              "summary": "{\n  \"name\": \"df_input\",\n  \"rows\": 4686,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1357,\n        \"min\": 0,\n        \"max\": 4728,\n        \"num_unique_values\": 4686,\n        \"samples\": [\n          3811,\n          4722,\n          805\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Catalog Number \",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1314,\n        \"samples\": [\n          \"Crizotinib (PF-02341066)\",\n          \"S7655\",\n          \"S7494\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SMILES \",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4686,\n        \"samples\": [\n          \"[4*]C(C)C[8*].[9*]n1c([14*])nc2cc([16*])ccc21.[16*]c1c(C)noc1C.[16*]c1ccc([16*])c(Cl)c1.[5*]N1CCOCC1.[8*]CC[8*].[3*]OC\",\n          \"[16*]c1cccc([16*])c1.[5*]N[5*].[15*]C1CC1.[1*]C(C)=O.[16*]c1ccc(I)cc1F.[9*]n1c(=O)c2c([14*])n(C)c(=O)c(C)c2n([9*])c1=O\",\n          \"ONC(=O)C1=CC=CC(=C1)C(=O)NCCC2=CC=CC=C2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Active/Inactive \",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Inactive \",\n          \"Active \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Remark\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Training\",\n          \"Testing\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Status\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "df_input = pd.read_csv(input_path)\n",
        "df_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9yeFpkRUIRaz"
      },
      "outputs": [],
      "source": [
        "smis = df_input.loc[:, smiles_column].values\n",
        "ys = df_input.loc[:, target_columns].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZsp0OooIRaz"
      },
      "source": [
        "## Make data points, splits, and datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NZEnfKcOIRaz",
        "outputId": "d2c6d3c5-6322-4701-f3a3-918d1f1fb431",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[10:57:44] SMILES Parse Error: syntax error while parsing: COC(=O)NC(C)CNC1=NC=CC(=N1)C2=C[N](N=C2C3=C(F)C(=CC(=C3)Cl)N[S;v6](C)(=O)=O)C(C)C\n",
            "[10:57:44] SMILES Parse Error: check for mistakes around position 63:\n",
            "[10:57:44] C(F)C(=CC(=C3)Cl)N[S;v6](C)(=O)=O)C(C)C\n",
            "[10:57:44] ~~~~~~~~~~~~~~~~~~~~^\n",
            "[10:57:44] SMILES Parse Error: Failed parsing SMILES 'COC(=O)NC(C)CNC1=NC=CC(=N1)C2=C[N](N=C2C3=C(F)C(=CC(=C3)Cl)N[S;v6](C)(=O)=O)C(C)C' for input: 'COC(=O)NC(C)CNC1=NC=CC(=N1)C2=C[N](N=C2C3=C(F)C(=CC(=C3)Cl)N[S;v6](C)(=O)=O)C(C)C'\n",
            "[10:57:44] SMILES Parse Error: syntax error while parsing: O.O.O.OC1=CC=CN|2=C1C(=O)O[VH]3|2(O)(=O)OC(=O)C4=NC=CC=C4O3\n",
            "[10:57:44] SMILES Parse Error: check for mistakes around position 16:\n",
            "[10:57:44] O.O.O.OC1=CC=CN|2=C1C(=O)O[VH]3|2(O)(=O)O\n",
            "[10:57:44] ~~~~~~~~~~~~~~~^\n",
            "[10:57:44] SMILES Parse Error: Failed parsing SMILES 'O.O.O.OC1=CC=CN|2=C1C(=O)O[VH]3|2(O)(=O)OC(=O)C4=NC=CC=C4O3' for input: 'O.O.O.OC1=CC=CN|2=C1C(=O)O[VH]3|2(O)(=O)OC(=O)C4=NC=CC=C4O3'\n",
            "[10:57:44] SMILES Parse Error: syntax error while parsing: Cl.CC1=CC(=CC(=C1)C[S;v6](=O)(=O)C2=CC=CC=C2)OCC3=CC=C(CN4CCCC4CO)C=C3\n",
            "[10:57:44] SMILES Parse Error: check for mistakes around position 22:\n",
            "[10:57:44] l.CC1=CC(=CC(=C1)C[S;v6](=O)(=O)C2=CC=CC=\n",
            "[10:57:44] ~~~~~~~~~~~~~~~~~~~~^\n",
            "[10:57:44] SMILES Parse Error: Failed parsing SMILES 'Cl.CC1=CC(=CC(=C1)C[S;v6](=O)(=O)C2=CC=CC=C2)OCC3=CC=C(CN4CCCC4CO)C=C3' for input: 'Cl.CC1=CC(=CC(=C1)C[S;v6](=O)(=O)C2=CC=CC=C2)OCC3=CC=C(CN4CCCC4CO)C=C3'\n",
            "[10:57:44] SMILES Parse Error: syntax error while parsing: N[S;v6](=O)(=O)OCC1CC(CC1O)[N]2C=CC3=C(NC4CCC5=CC=CC=C45)N=CN=C23\n",
            "[10:57:44] SMILES Parse Error: check for mistakes around position 4:\n",
            "[10:57:44] N[S;v6](=O)(=O)OCC1CC(CC1O)[N]2C=CC3=C(NC\n",
            "[10:57:44] ~~~^\n",
            "[10:57:44] SMILES Parse Error: Failed parsing SMILES 'N[S;v6](=O)(=O)OCC1CC(CC1O)[N]2C=CC3=C(NC4CCC5=CC=CC=C45)N=CN=C23' for input: 'N[S;v6](=O)(=O)OCC1CC(CC1O)[N]2C=CC3=C(NC4CCC5=CC=CC=C45)N=CN=C23'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping invalid SMILES: COC(=O)NC(C)CNC1=NC=CC(=N1)C2=C[N](N=C2C3=C(F)C(=CC(=C3)Cl)N[S;v6](C)(=O)=O)C(C)C\n",
            "Skipping invalid SMILES: O.O.O.OC1=CC=CN|2=C1C(=O)O[VH]3|2(O)(=O)OC(=O)C4=NC=CC=C4O3\n",
            "Skipping invalid SMILES: Cl.CC1=CC(=CC(=C1)C[S;v6](=O)(=O)C2=CC=CC=C2)OCC3=CC=C(CN4CCCC4CO)C=C3\n",
            "Skipping invalid SMILES: N[S;v6](=O)(=O)OCC1CC(CC1O)[N]2C=CC3=C(NC4CCC5=CC=CC=C45)N=CN=C23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[10:57:44] SMILES Parse Error: extra open parentheses while parsing: CCC(C)C1NC(=O)C(CC2=CC=C(O)C=C2)NC(=O)C(CC(C)C)NC(=O)C3CCCN3C(=O)C(CSSCC(NC(=O)C(NC(=O)C4CCCN4C(=O)C(CC(O)=O)NC(=O)C(CC5=CC=C(O)C=C5)NC(=O)C(CO)NC1=O)C(C)C)C(=O)NC(CCCNC(N)=N)C(=O)NC(CCCNC(N)=N)C(=O)N\n",
            "[10:57:44] SMILES Parse Error: check for mistakes around position 67:\n",
            "[10:57:44] )NC(=O)C3CCCN3C(=O)C(CSSCC(NC(=O)C(NC(=O)\n",
            "[10:57:44] ~~~~~~~~~~~~~~~~~~~~^\n",
            "[10:57:44] SMILES Parse Error: Failed parsing SMILES 'CCC(C)C1NC(=O)C(CC2=CC=C(O)C=C2)NC(=O)C(CC(C)C)NC(=O)C3CCCN3C(=O)C(CSSCC(NC(=O)C(NC(=O)C4CCCN4C(=O)C(CC(O)=O)NC(=O)C(CC5=CC=C(O)C=C5)NC(=O)C(CO)NC1=O)C(C)C)C(=O)NC(CCCNC(N)=N)C(=O)NC(CCCNC(N)=N)C(=O)N' for input: 'CCC(C)C1NC(=O)C(CC2=CC=C(O)C=C2)NC(=O)C(CC(C)C)NC(=O)C3CCCN3C(=O)C(CSSCC(NC(=O)C(NC(=O)C4CCCN4C(=O)C(CC(O)=O)NC(=O)C(CC5=CC=C(O)C=C5)NC(=O)C(CO)NC1=O)C(C)C)C(=O)NC(CCCNC(N)=N)C(=O)NC(CCCNC(N)=N)C(=O)N'\n",
            "[10:57:44] SMILES Parse Error: syntax error while parsing: CC(C)(C)C[N]1C(=NC2=CC=C(N=C12)C3=C(N=C([NH]3)C(C)(C)C)C4=CC=C(F)C=C4)N.C[S;v6](O)(=O)=O\n",
            "[10:57:44] SMILES Parse Error: check for mistakes around position 76:\n",
            "[10:57:44] C4=CC=C(F)C=C4)N.C[S;v6](O)(=O)=O\n",
            "[10:57:44] ~~~~~~~~~~~~~~~~~~~~^\n",
            "[10:57:44] SMILES Parse Error: Failed parsing SMILES 'CC(C)(C)C[N]1C(=NC2=CC=C(N=C12)C3=C(N=C([NH]3)C(C)(C)C)C4=CC=C(F)C=C4)N.C[S;v6](O)(=O)=O' for input: 'CC(C)(C)C[N]1C(=NC2=CC=C(N=C12)C3=C(N=C([NH]3)C(C)(C)C)C4=CC=C(F)C=C4)N.C[S;v6](O)(=O)=O'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping invalid SMILES: CCC(C)C1NC(=O)C(CC2=CC=C(O)C=C2)NC(=O)C(CC(C)C)NC(=O)C3CCCN3C(=O)C(CSSCC(NC(=O)C(NC(=O)C4CCCN4C(=O)C(CC(O)=O)NC(=O)C(CC5=CC=C(O)C=C5)NC(=O)C(CO)NC1=O)C(C)C)C(=O)NC(CCCNC(N)=N)C(=O)NC(CCCNC(N)=N)C(=O)N\n",
            "Skipping invalid SMILES: CC(C)(C)C[N]1C(=NC2=CC=C(N=C12)C3=C(N=C([NH]3)C(C)(C)C)C4=CC=C(F)C=C4)N.C[S;v6](O)(=O)=O\n"
          ]
        }
      ],
      "source": [
        "all_data = []\n",
        "for smi, y in zip(smis, ys):\n",
        "    try:\n",
        "        datapoint = data.MoleculeDatapoint.from_smi(smi, y)\n",
        "        all_data.append(datapoint)\n",
        "    except RuntimeError:\n",
        "        print(f\"Skipping invalid SMILES: {smi}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Kanb-f-tIRaz",
        "outputId": "d27e7646-fe5d-47b2-8aba-c76ae216afff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chemprop.data.splitting:The return type of make_split_indices has changed in v2.1 - see help(make_split_indices)\n"
          ]
        }
      ],
      "source": [
        "mols = [d.mol for d in all_data]  # RDkit Mol objects are use for structure based splits\n",
        "train_indices, val_indices, test_indices = data.make_split_indices(mols, \"random\", (0.8, 0.1, 0.1))\n",
        "train_data, val_data, test_data = data.split_data_by_indices(\n",
        "    all_data, train_indices, val_indices, test_indices\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0lqRWmboIRa0"
      },
      "outputs": [],
      "source": [
        "from chemprop.featurizers import SimpleMoleculeMolGraphFeaturizer\n",
        "from rdkit.Chem import Descriptors, rdMolDescriptors\n",
        "class CustomFeaturizer(SimpleMoleculeMolGraphFeaturizer):\n",
        "    def featurize(self, mol):\n",
        "        atom_features = []\n",
        "        for atom in mol.GetAtoms():\n",
        "            atom_features.append([\n",
        "                atom.GetAtomicNum(),\n",
        "                atom.GetNumBonds(),\n",
        "                atom.GetFormalCharge(),\n",
        "                atom.GetChiralTag(),\n",
        "                atom.GetTotalNumHs(),\n",
        "                atom.GetHybridization(),\n",
        "                atom.GetIsAromatic(),\n",
        "                atom.GetMass()\n",
        "            ])\n",
        "\n",
        "        bond_features = []\n",
        "        for bond in mol.GetBonds():\n",
        "            bond_features.append([\n",
        "                bond.GetBondType(),\n",
        "                bond.GetIsConjugated(),\n",
        "                bond.IsInRing(),\n",
        "                bond.GetStereo()\n",
        "            ])\n",
        "\n",
        "        mol_features = []\n",
        "        for descriptor_name, descriptor_func in Descriptors.descList:\n",
        "            mol_features.append(descriptor_func(mol))\n",
        "\n",
        "        additional_features = []\n",
        "        for descriptor_name, descriptor_func in rdMolDescriptors.descList:\n",
        "            additional_features.append(descriptor_func(mol))\n",
        "\n",
        "        return atom_features + bond_features + mol_features + additional_features\n",
        "\n",
        "featurizer = CustomFeaturizer()\n",
        "\n",
        "# === Create datasets and loaders ===\n",
        "train_dset = data.MoleculeDataset(train_data[0], featurizer)\n",
        "scaler = train_dset.normalize_targets()\n",
        "val_dset = data.MoleculeDataset(val_data[0], featurizer)\n",
        "val_dset.normalize_targets(scaler)\n",
        "test_dset = data.MoleculeDataset(test_data[0], featurizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-zXBYPeIRa0"
      },
      "source": [
        "# Define helper function to train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mXDvxbutIRa0"
      },
      "outputs": [],
      "source": [
        "def train_model(config, train_dset, val_dset, num_workers, scaler):\n",
        "\n",
        "    # config is a dictionary containing hyperparameters used for the trial\n",
        "    depth = int(config[\"depth\"])\n",
        "    ffn_hidden_dim = int(config[\"ffn_hidden_dim\"])\n",
        "    ffn_num_layers = int(config[\"ffn_num_layers\"])\n",
        "    message_hidden_dim = int(config[\"message_hidden_dim\"])\n",
        "\n",
        "    train_loader = data.build_dataloader(train_dset, num_workers=num_workers, shuffle=True)\n",
        "    val_loader = data.build_dataloader(val_dset, num_workers=num_workers, shuffle=False)\n",
        "\n",
        "    mp = nn.BondMessagePassing(d_h=message_hidden_dim, depth=depth)\n",
        "    agg = nn.MeanAggregation()\n",
        "    output_transform = nn.UnscaleTransform.from_standard_scaler(scaler)\n",
        "    ffn = nn.BinaryClassificationFFN(output_transform=output_transform, input_dim=message_hidden_dim, hidden_dim=ffn_hidden_dim, n_layers=ffn_num_layers)\n",
        "    batch_norm = True\n",
        "    metric_list = [nn.metrics.BinaryF1Score()]\n",
        "    model = models.MPNN(mp, agg, ffn, batch_norm, metric_list)\n",
        "\n",
        "    trainer = pl.Trainer(\n",
        "        accelerator=\"auto\",\n",
        "        devices=1,\n",
        "        max_epochs=20, # number of epochs to train for\n",
        "        # below are needed for Ray and Lightning integration\n",
        "        strategy=RayDDPStrategy(),\n",
        "        callbacks=[RayTrainReportCallback()],\n",
        "        plugins=[RayLightningEnvironment()],\n",
        "    )\n",
        "\n",
        "    trainer = prepare_trainer(trainer)\n",
        "    trainer.fit(model, train_loader, val_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyQcR7sWIRa0"
      },
      "source": [
        "## Define parameter search space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ay1Inml-IRa0"
      },
      "outputs": [],
      "source": [
        "\n",
        "search_space = {\n",
        "    \"depth\": tune.qrandint(lower=2, upper=6, q=1),\n",
        "    \"ffn_hidden_dim\": tune.qrandint(lower=300, upper=2400, q=100),\n",
        "    \"ffn_num_layers\": tune.qrandint(lower=1, upper=3, q=1),\n",
        "    \"message_hidden_dim\": tune.qrandint(lower=300, upper=2400, q=100),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjJsBrBlIRa0",
        "outputId": "2b9b0689-c407-4581-bf07-131b4510a7a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-05 10:58:32,011\tINFO worker.py:1888 -- Started a local Ray instance.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------------------------+\n",
            "| Configuration for experiment     TorchTrainer_2025-05-05_10-58-35   |\n",
            "+---------------------------------------------------------------------+\n",
            "| Search algorithm                 SearchGenerator                    |\n",
            "| Scheduler                        FIFOScheduler                      |\n",
            "| Number of trials                 20                                 |\n",
            "+---------------------------------------------------------------------+\n",
            "\n",
            "View detailed results here: /content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35\n",
            "To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2025-05-05_10-58-28_699870_493/artifacts/2025-05-05_10-58-35/TorchTrainer_2025-05-05_10-58-35/driver_artifacts`\n",
            "\n",
            "Trial status: 1 PENDING\n",
            "Current time: 2025-05-05 10:58:35. Total running time: 0s\n",
            "Logical resource usage: 0/2 CPUs, 0/0 GPUs\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   PENDING                         2                     2000                        2                      500 |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "Trial TorchTrainer_8da927e1 started with configuration:\n",
            "+---------------------------------------------+\n",
            "| Trial TorchTrainer_8da927e1 config          |\n",
            "+---------------------------------------------+\n",
            "| train_loop_config/depth                   2 |\n",
            "| train_loop_config/ffn_hidden_dim       2000 |\n",
            "| train_loop_config/ffn_num_layers          2 |\n",
            "| train_loop_config/message_hidden_dim    500 |\n",
            "+---------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(TorchTrainer pid=2818)\u001b[0m Started distributed worker processes: \n",
            "\u001b[36m(TorchTrainer pid=2818)\u001b[0m - (node_id=85d1826776a8b04eb71395ec08130d8b282f617a724b5c6551ccdd9f, ip=172.28.0.12, pid=2908) world_rank=0, local_rank=0, node_rank=0\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial TorchTrainer_c410a352 started with configuration:\n",
            "+---------------------------------------------+\n",
            "| Trial TorchTrainer_c410a352 config          |\n",
            "+---------------------------------------------+\n",
            "| train_loop_config/depth                   2 |\n",
            "| train_loop_config/ffn_hidden_dim       2200 |\n",
            "| train_loop_config/ffn_num_layers          2 |\n",
            "| train_loop_config/message_hidden_dim    400 |\n",
            "+---------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m GPU available: False, used: False\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 10:59:05. Total running time: 30s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500 |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400 |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000 |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m 2025-05-05 10:59:07.032489: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m E0000 00:00:1746442747.315972    3063 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m E0000 00:00:1746442747.393966    3063 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m 2025-05-05 10:59:07.954640: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(TorchTrainer pid=2907)\u001b[0m Started distributed worker processes: \n",
            "\u001b[36m(TorchTrainer pid=2907)\u001b[0m - (node_id=85d1826776a8b04eb71395ec08130d8b282f617a724b5c6551ccdd9f, ip=172.28.0.12, pid=3058) world_rank=0, local_rank=0, node_rank=0\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \rSanity Checking: |          | 0/? [00:00<?, ?it/s]\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \rSanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]\rSanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m   | Name            | Type                    | Params | Mode \n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m --------------------------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m 0 | message_passing | BondMessagePassing      | 579 K  | train\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m 1 | agg             | MeanAggregation         | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m 2 | bn              | BatchNorm1d             | 1.0 K  | train\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m 3 | predictor       | BinaryClassificationFFN | 5.0 M  | train\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m 4 | X_d_transform   | Identity                | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m 5 | metrics         | ModuleList              | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m --------------------------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m 5.6 M     Trainable params\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m 0         Non-trainable params\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m 5.6 M     Total params\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m 22.346    Total estimated model params size (MB)\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m 26        Modules in train mode\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m 0         Modules in eval mode\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m /usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00,  1.18it/s]\n",
            "Epoch 0:   0%|          | 0/59 [00:00<?, ?it/s] \n",
            "Epoch 0:   2%|▏         | 1/59 [00:01<00:59,  0.97it/s, v_num=0, train_loss_step=0.712]\n",
            "Epoch 0:   3%|▎         | 2/59 [00:01<00:56,  1.01it/s, v_num=0, train_loss_step=0.416]\n",
            "Epoch 0:   5%|▌         | 3/59 [00:02<00:54,  1.02it/s, v_num=0, train_loss_step=0.269]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m GPU available: False, used: False\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m 2025-05-05 10:59:22.095887: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m E0000 00:00:1746442762.146100    3172 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m E0000 00:00:1746442762.161395    3172 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m 2025-05-05 10:59:22.210687: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0:   7%|▋         | 4/59 [00:03<00:52,  1.04it/s, v_num=0, train_loss_step=0.146]\n",
            "Epoch 0:   8%|▊         | 5/59 [00:04<00:50,  1.06it/s, v_num=0, train_loss_step=0.412]\n",
            "Epoch 0:  10%|█         | 6/59 [00:05<00:50,  1.06it/s, v_num=0, train_loss_step=-0.291]\n",
            "Epoch 0:  12%|█▏        | 7/59 [00:06<00:48,  1.06it/s, v_num=0, train_loss_step=0.479] \n",
            "Epoch 0:  14%|█▎        | 8/59 [00:07<00:48,  1.06it/s, v_num=0, train_loss_step=-0.357]\n",
            "Epoch 0:  15%|█▌        | 9/59 [00:08<00:47,  1.05it/s, v_num=0, train_loss_step=-0.676]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m   | Name            | Type                    | Params | Mode \n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m --------------------------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m 0 | message_passing | BondMessagePassing      | 383 K  | train\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m 1 | agg             | MeanAggregation         | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m 2 | bn              | BatchNorm1d             | 800    | train\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m 3 | predictor       | BinaryClassificationFFN | 5.7 M  | train\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m 4 | X_d_transform   | Identity                | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m 5 | metrics         | ModuleList              | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m --------------------------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m 6.1 M     Trainable params\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m 0         Non-trainable params\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m 6.1 M     Total params\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m 24.444    Total estimated model params size (MB)\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m 26        Modules in train mode\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m 0         Modules in eval mode\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m /usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \rSanity Checking: |          | 0/? [00:00<?, ?it/s]\n",
            "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:01<00:01,  0.89it/s]\n",
            "Epoch 0:  17%|█▋        | 10/59 [00:10<00:50,  0.98it/s, v_num=0, train_loss_step=-1.06] \n",
            "Epoch 0:   0%|          | 0/59 [00:00<?, ?it/s] \n",
            "Epoch 0:  19%|█▊        | 11/59 [00:11<00:51,  0.94it/s, v_num=0, train_loss_step=-0.468]\n",
            "Epoch 0:   2%|▏         | 1/59 [00:01<01:25,  0.68it/s, v_num=0, train_loss_step=0.688]\n",
            "Epoch 0:  20%|██        | 12/59 [00:13<00:52,  0.90it/s, v_num=0, train_loss_step=0.031] \n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 10:59:35. Total running time: 1min 0s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500 |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400 |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000 |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 0:  14%|█▎        | 8/59 [00:07<00:47,  1.07it/s, v_num=0, train_loss_step=-0.426]\u001b[32m [repeated 12x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
            "Epoch 0:  39%|███▉      | 23/59 [00:24<00:37,  0.96it/s, v_num=0, train_loss_step=-2.74]\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "Epoch 0:  31%|███       | 18/59 [00:19<00:44,  0.92it/s, v_num=0, train_loss_step=-3.22]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 0:  34%|███▍      | 20/59 [00:25<00:48,  0.80it/s, v_num=0, train_loss_step=0.847]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "Epoch 0:  37%|███▋      | 22/59 [00:31<00:52,  0.71it/s, v_num=0, train_loss_step=4.440] \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:00:05. Total running time: 1min 30s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500 |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400 |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000 |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 0:  58%|█████▊    | 34/59 [00:47<00:34,  0.72it/s, v_num=0, train_loss_step=-3.84]\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "Epoch 0:  59%|█████▉    | 35/59 [00:41<00:28,  0.85it/s, v_num=0, train_loss_step=-25.1]\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
            "Epoch 0:  63%|██████▎   | 37/59 [00:43<00:25,  0.85it/s, v_num=0, train_loss_step=-19.8]\n",
            "Epoch 0:  63%|██████▎   | 37/59 [00:43<00:25,  0.85it/s, v_num=0, train_loss_step=-27.8]\n",
            "Epoch 0:  73%|███████▎  | 43/59 [00:57<00:21,  0.75it/s, v_num=0, train_loss_step=-87.4]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 0:  78%|███████▊  | 46/59 [00:51<00:14,  0.89it/s, v_num=0, train_loss_step=-41.0]\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "Epoch 0:  92%|█████████▏| 54/59 [01:08<00:06,  0.79it/s, v_num=0, train_loss_step=-188.]\n",
            "Epoch 0:  88%|████████▊ | 52/59 [00:56<00:07,  0.92it/s, v_num=0, train_loss_step=62.40]\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "Epoch 0:  93%|█████████▎| 55/59 [01:09<00:05,  0.79it/s, v_num=0, train_loss_step=-450.]\n",
            "Epoch 0:  97%|█████████▋| 57/59 [01:02<00:02,  0.92it/s, v_num=0, train_loss_step=77.50]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 0:  90%|████████▉ | 53/59 [00:57<00:06,  0.93it/s, v_num=0, train_loss_step=-221.]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:04,  1.54it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:00:35. Total running time: 2min 0s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500 |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400 |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000 |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Epoch 0: 100%|██████████| 59/59 [01:03<00:00,  0.93it/s, v_num=0, train_loss_step=-117.]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:04<00:00,  1.80it/s]\u001b[A\n",
            "Epoch 0: 100%|██████████| 59/59 [01:07<00:00,  0.87it/s, v_num=0, train_loss_step=-117., val_loss=-126.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/c410a352/checkpoint_000000)\n",
            "2025-05-05 11:00:38,070\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \rEpoch 0: 100%|██████████| 59/59 [01:08<00:00,  0.86it/s, v_num=0, train_loss_step=-117., val_loss=-126., train_loss_epoch=-54.2]\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \rValidation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \rValidation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Epoch 1:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-117., val_loss=-126., train_loss_epoch=-54.2]\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:05<00:00,  1.28it/s]\u001b[A\u001b[32m [repeated 13x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-05 11:00:39,266\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1:   2%|▏         | 1/59 [00:00<00:51,  1.14it/s, v_num=0, train_loss_step=-567., val_loss=-126., train_loss_epoch=-54.2]\n",
            "Epoch 1:   3%|▎         | 2/59 [00:01<00:47,  1.21it/s, v_num=0, train_loss_step=-86.6, val_loss=-126., train_loss_epoch=-54.2]\n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:05<00:00,  1.40it/s]\u001b[A\n",
            "Epoch 0: 100%|██████████| 59/59 [01:20<00:00,  0.74it/s, v_num=0, train_loss_step=-162., val_loss=-30.4]\n",
            "Epoch 1:   5%|▌         | 3/59 [00:03<01:07,  0.84it/s, v_num=0, train_loss_step=98.10, val_loss=-30.4, train_loss_epoch=-70.9]\n",
            "Epoch 0: 100%|██████████| 59/59 [01:20<00:00,  0.73it/s, v_num=0, train_loss_step=-162., val_loss=-30.4, train_loss_epoch=-70.9]\n",
            "Epoch 1:   5%|▌         | 3/59 [00:03<01:07,  0.83it/s, v_num=0, train_loss_step=-1.38e+3, val_loss=-30.4, train_loss_epoch=-70.9]\n",
            "Epoch 1:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-162., val_loss=-30.4, train_loss_epoch=-70.9]\n",
            "Epoch 1:  10%|█         | 6/59 [00:05<00:52,  1.01it/s, v_num=0, train_loss_step=-849., val_loss=-126., train_loss_epoch=-54.2]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 1:  14%|█▎        | 8/59 [00:09<01:02,  0.82it/s, v_num=0, train_loss_step=-1.82e+3, val_loss=-30.4, train_loss_epoch=-70.9]\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "Epoch 1:  32%|███▏      | 19/59 [00:16<00:34,  1.15it/s, v_num=0, train_loss_step=-5.57e+3, val_loss=-126., train_loss_epoch=-54.2]\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
            "Epoch 1:  41%|████      | 24/59 [00:22<00:32,  1.09it/s, v_num=0, train_loss_step=-9.94e+3, val_loss=-126., train_loss_epoch=-54.2]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:01:05. Total running time: 2min 30s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: c410a352 with val_loss=-125.6384048461914 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2200, 'ffn_num_layers': 2, 'message_hidden_dim': 400}}\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1     val_loss |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500        1           112.272        -70.9362            -162.252          0     -30.3872 |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400        1            93.7465       -54.179             -117.087          0    -125.638  |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000                                                                                        |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 1:  39%|███▉      | 23/59 [00:26<00:41,  0.87it/s, v_num=0, train_loss_step=-8.89e+3, val_loss=-30.4, train_loss_epoch=-70.9]\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "Epoch 1:  64%|██████▍   | 38/59 [00:33<00:18,  1.15it/s, v_num=0, train_loss_step=1.34e+4, val_loss=-126., train_loss_epoch=-54.2]\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
            "Epoch 1:  54%|█████▍    | 32/59 [00:36<00:31,  0.87it/s, v_num=0, train_loss_step=-1.85e+4, val_loss=-30.4, train_loss_epoch=-70.9]\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "Epoch 1:  83%|████████▎ | 49/59 [00:43<00:08,  1.13it/s, v_num=0, train_loss_step=-6.37e+4, val_loss=-126., train_loss_epoch=-54.2]\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "Epoch 1:  92%|█████████▏| 54/59 [00:47<00:04,  1.15it/s, v_num=0, train_loss_step=-5.78e+4, val_loss=-126., train_loss_epoch=-54.2]\n",
            "Epoch 1:  93%|█████████▎| 55/59 [00:47<00:03,  1.15it/s, v_num=0, train_loss_step=-4.06e+4, val_loss=-126., train_loss_epoch=-54.2]\n",
            "Epoch 1:  73%|███████▎  | 43/59 [00:47<00:17,  0.91it/s, v_num=0, train_loss_step=-4.3e+4, val_loss=-30.4, train_loss_epoch=-70.9] \u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "Epoch 1:  95%|█████████▍| 56/59 [00:49<00:02,  1.14it/s, v_num=0, train_loss_step=-9.79e+4, val_loss=-126., train_loss_epoch=-54.2]\n",
            "Epoch 1:  97%|█████████▋| 57/59 [00:50<00:01,  1.13it/s, v_num=0, train_loss_step=-1.12e+5, val_loss=-126., train_loss_epoch=-54.2]\n",
            "Epoch 1:  98%|█████████▊| 58/59 [00:51<00:00,  1.12it/s, v_num=0, train_loss_step=-1.25e+5, val_loss=-126., train_loss_epoch=-54.2]\n",
            "Epoch 1: 100%|██████████| 59/59 [00:52<00:00,  1.13it/s, v_num=0, train_loss_step=-1.55e+5, val_loss=-126., train_loss_epoch=-54.2]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:04,  1.61it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:03,  1.60it/s]\u001b[A\n",
            "Epoch 1:  78%|███████▊  | 46/59 [00:52<00:14,  0.88it/s, v_num=0, train_loss_step=-2.88e+4, val_loss=-30.4, train_loss_epoch=-70.9]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:01<00:03,  1.61it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:02<00:02,  1.60it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:03<00:01,  1.63it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:03<00:01,  1.63it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:04<00:00,  1.63it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:04<00:00,  1.77it/s]\u001b[A\n",
            "Epoch 1: 100%|██████████| 59/59 [00:57<00:00,  1.03it/s, v_num=0, train_loss_step=-1.55e+5, val_loss=-1.14e+5, train_loss_epoch=-54.2]\n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:01:35. Total running time: 3min 0s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: c410a352 with val_loss=-125.6384048461914 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2200, 'ffn_num_layers': 2, 'message_hidden_dim': 400}}\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1     val_loss |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500        1           112.272        -70.9362            -162.252          0     -30.3872 |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400        1            93.7465       -54.179             -117.087          0    -125.638  |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000                                                                                        |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/c410a352/checkpoint_000001)\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: 100%|██████████| 59/59 [00:57<00:00,  1.02it/s, v_num=0, train_loss_step=-1.55e+5, val_loss=-1.14e+5, train_loss_epoch=-2.13e+4]\n",
            "Epoch 2:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-1.55e+5, val_loss=-1.14e+5, train_loss_epoch=-2.13e+4]\n",
            "Epoch 1:  86%|████████▋ | 51/59 [00:57<00:09,  0.88it/s, v_num=0, train_loss_step=-8.55e+4, val_loss=-30.4, train_loss_epoch=-70.9]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 1:  92%|█████████▏| 54/59 [01:00<00:05,  0.89it/s, v_num=0, train_loss_step=-9.46e+4, val_loss=-30.4, train_loss_epoch=-70.9]\n",
            "Epoch 1:  93%|█████████▎| 55/59 [01:01<00:04,  0.89it/s, v_num=0, train_loss_step=-3.25e+4, val_loss=-30.4, train_loss_epoch=-70.9]\n",
            "Epoch 1:  95%|█████████▍| 56/59 [01:02<00:03,  0.89it/s, v_num=0, train_loss_step=-1.08e+5, val_loss=-30.4, train_loss_epoch=-70.9]\n",
            "Epoch 2:  10%|█         | 6/59 [00:04<00:42,  1.23it/s, v_num=0, train_loss_step=-1.46e+5, val_loss=-1.14e+5, train_loss_epoch=-2.13e+4]\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "Epoch 1:  97%|█████████▋| 57/59 [01:04<00:02,  0.88it/s, v_num=0, train_loss_step=-1.65e+5, val_loss=-30.4, train_loss_epoch=-70.9]\n",
            "Epoch 1:  98%|█████████▊| 58/59 [01:06<00:01,  0.88it/s, v_num=0, train_loss_step=-1.63e+5, val_loss=-30.4, train_loss_epoch=-70.9]\n",
            "Epoch 1: 100%|██████████| 59/59 [01:06<00:00,  0.88it/s, v_num=0, train_loss_step=-1.32e+5, val_loss=-30.4, train_loss_epoch=-70.9]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:04,  1.55it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:04,  1.44it/s]\u001b[A\n",
            "Epoch 2:  19%|█▊        | 11/59 [00:10<00:46,  1.03it/s, v_num=0, train_loss_step=-2.58e+5, val_loss=-1.14e+5, train_loss_epoch=-2.13e+4]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:02<00:03,  1.39it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:02<00:02,  1.37it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:03<00:02,  1.37it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:04<00:01,  1.36it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:05<00:00,  1.35it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:05<00:00,  1.48it/s]\u001b[A\n",
            "Epoch 1: 100%|██████████| 59/59 [01:12<00:00,  0.81it/s, v_num=0, train_loss_step=-1.32e+5, val_loss=-1e+5, train_loss_epoch=-70.9]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/8da927e1/checkpoint_000001)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: 100%|██████████| 59/59 [01:12<00:00,  0.81it/s, v_num=0, train_loss_step=-1.32e+5, val_loss=-1e+5, train_loss_epoch=-2.64e+4]\n",
            "Epoch 2:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-1.32e+5, val_loss=-1e+5, train_loss_epoch=-2.64e+4]\n",
            "Epoch 2:  32%|███▏      | 19/59 [00:16<00:35,  1.13it/s, v_num=0, train_loss_step=-2.63e+5, val_loss=-1.14e+5, train_loss_epoch=-2.13e+4]\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "Epoch 2:  42%|████▏     | 25/59 [00:22<00:30,  1.10it/s, v_num=0, train_loss_step=-3.7e+5, val_loss=-1.14e+5, train_loss_epoch=-2.13e+4] \u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "Epoch 2:  53%|█████▎    | 31/59 [00:27<00:25,  1.11it/s, v_num=0, train_loss_step=-7.96e+5, val_loss=-1.14e+5, train_loss_epoch=-2.13e+4]\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:02:05. Total running time: 3min 30s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: c410a352 with val_loss=-114000.2734375 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2200, 'ffn_num_layers': 2, 'message_hidden_dim': 400}}\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1     val_loss |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500        2            185.596       -26386.8             -132260          0      -100495 |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400        2            151.753       -21269.3             -154898          0      -114000 |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000                                                                                        |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 2:  64%|██████▍   | 38/59 [00:33<00:18,  1.14it/s, v_num=0, train_loss_step=-7.76e+5, val_loss=-1.14e+5, train_loss_epoch=-2.13e+4]\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "Epoch 2:  73%|███████▎  | 43/59 [00:38<00:14,  1.11it/s, v_num=0, train_loss_step=-1.34e+6, val_loss=-1.14e+5, train_loss_epoch=-2.13e+4]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "Epoch 2:  81%|████████▏ | 48/59 [00:43<00:10,  1.09it/s, v_num=0, train_loss_step=-4.16e+5, val_loss=-1.14e+5, train_loss_epoch=-2.13e+4]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "Epoch 2:  92%|█████████▏| 54/59 [00:48<00:04,  1.11it/s, v_num=0, train_loss_step=-2.18e+6, val_loss=-1.14e+5, train_loss_epoch=-2.13e+4]\n",
            "Epoch 2:  93%|█████████▎| 55/59 [00:49<00:03,  1.12it/s, v_num=0, train_loss_step=-1.51e+6, val_loss=-1.14e+5, train_loss_epoch=-2.13e+4]\n",
            "Epoch 2:  49%|████▉     | 29/59 [00:32<00:33,  0.89it/s, v_num=0, train_loss_step=-9.03e+5, val_loss=-1e+5, train_loss_epoch=-2.64e+4]\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "Epoch 2:  95%|█████████▍| 56/59 [00:50<00:02,  1.11it/s, v_num=0, train_loss_step=-1.75e+6, val_loss=-1.14e+5, train_loss_epoch=-2.13e+4]\n",
            "Epoch 2:  97%|█████████▋| 57/59 [00:51<00:01,  1.10it/s, v_num=0, train_loss_step=-2.04e+6, val_loss=-1.14e+5, train_loss_epoch=-2.13e+4]\n",
            "Epoch 2:  98%|█████████▊| 58/59 [00:53<00:00,  1.09it/s, v_num=0, train_loss_step=-1.98e+6, val_loss=-1.14e+5, train_loss_epoch=-2.13e+4]\n",
            "Epoch 2: 100%|██████████| 59/59 [00:53<00:00,  1.09it/s, v_num=0, train_loss_step=-2.79e+6, val_loss=-1.14e+5, train_loss_epoch=-2.13e+4]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:  56%|█████▌    | 33/59 [00:38<00:30,  0.86it/s, v_num=0, train_loss_step=-8.98e+5, val_loss=-1e+5, train_loss_epoch=-2.64e+4]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:04,  1.49it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:03,  1.57it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:01<00:03,  1.60it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:02<00:02,  1.60it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:03<00:01,  1.62it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:03<00:01,  1.62it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:04<00:00,  1.61it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:04<00:00,  1.76it/s]\u001b[A\n",
            "Epoch 2: 100%|██████████| 59/59 [00:58<00:00,  1.01it/s, v_num=0, train_loss_step=-2.79e+6, val_loss=-1.18e+6, train_loss_epoch=-2.13e+4]\n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:02:35. Total running time: 4min 0s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: c410a352 with val_loss=-114000.2734375 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2200, 'ffn_num_layers': 2, 'message_hidden_dim': 400}}\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1     val_loss |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500        2            185.596       -26386.8             -132260          0      -100495 |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400        2            151.753       -21269.3             -154898          0      -114000 |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000                                                                                        |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-05 11:02:36,036\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/c410a352/checkpoint_000002)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: 100%|██████████| 59/59 [00:59<00:00,  1.00it/s, v_num=0, train_loss_step=-2.79e+6, val_loss=-1.18e+6, train_loss_epoch=-6.81e+5]\n",
            "Epoch 3:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-2.79e+6, val_loss=-1.18e+6, train_loss_epoch=-6.81e+5]\n",
            "Epoch 2:  64%|██████▍   | 38/59 [00:43<00:23,  0.88it/s, v_num=0, train_loss_step=-8.54e+5, val_loss=-1e+5, train_loss_epoch=-2.64e+4]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 3:  12%|█▏        | 7/59 [00:05<00:41,  1.25it/s, v_num=0, train_loss_step=-1.77e+6, val_loss=-1.18e+6, train_loss_epoch=-6.81e+5]\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "Epoch 2:  81%|████████▏ | 48/59 [00:54<00:12,  0.88it/s, v_num=0, train_loss_step=-6e+5, val_loss=-1e+5, train_loss_epoch=-2.64e+4]   \u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "Epoch 3:  32%|███▏      | 19/59 [00:16<00:34,  1.14it/s, v_num=0, train_loss_step=-4.53e+6, val_loss=-1.18e+6, train_loss_epoch=-6.81e+5]\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
            "Epoch 2:  92%|█████████▏| 54/59 [01:00<00:05,  0.89it/s, v_num=0, train_loss_step=-2.38e+6, val_loss=-1e+5, train_loss_epoch=-2.64e+4]\n",
            "Epoch 2:  93%|█████████▎| 55/59 [01:01<00:04,  0.89it/s, v_num=0, train_loss_step=-1.53e+6, val_loss=-1e+5, train_loss_epoch=-2.64e+4]\n",
            "Epoch 2:  95%|█████████▍| 56/59 [01:02<00:03,  0.90it/s, v_num=0, train_loss_step=-2.04e+6, val_loss=-1e+5, train_loss_epoch=-2.64e+4]\n",
            "Epoch 2:  97%|█████████▋| 57/59 [01:03<00:02,  0.90it/s, v_num=0, train_loss_step=-2.2e+6, val_loss=-1e+5, train_loss_epoch=-2.64e+4] \n",
            "Epoch 2:  98%|█████████▊| 58/59 [01:04<00:01,  0.90it/s, v_num=0, train_loss_step=-2.51e+6, val_loss=-1e+5, train_loss_epoch=-2.64e+4]\n",
            "Epoch 3:  42%|████▏     | 25/59 [00:21<00:29,  1.14it/s, v_num=0, train_loss_step=-2.88e+6, val_loss=-1.18e+6, train_loss_epoch=-6.81e+5]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 2: 100%|██████████| 59/59 [01:05<00:00,  0.90it/s, v_num=0, train_loss_step=-2.81e+6, val_loss=-1e+5, train_loss_epoch=-2.64e+4]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:01<00:07,  0.93it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:02<00:07,  0.82it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:03<00:05,  0.89it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:04<00:04,  0.97it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:04<00:02,  1.03it/s]\u001b[A\n",
            "Epoch 3:  51%|█████     | 30/59 [00:26<00:26,  1.11it/s, v_num=0, train_loss_step=-3.03e+6, val_loss=-1.18e+6, train_loss_epoch=-6.81e+5]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:05<00:01,  1.07it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:06<00:00,  1.10it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:06<00:00,  1.21it/s]\u001b[A\n",
            "Epoch 2: 100%|██████████| 59/59 [01:12<00:00,  0.82it/s, v_num=0, train_loss_step=-2.81e+6, val_loss=-2.32e+6, train_loss_epoch=-2.64e+4]\n",
            "Epoch 2: 100%|██████████| 59/59 [01:12<00:00,  0.81it/s, v_num=0, train_loss_step=-2.81e+6, val_loss=-2.32e+6, train_loss_epoch=-7.77e+5]\n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:03:05. Total running time: 4min 30s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-2317398.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1       val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500        3            258.666        -777472        -2.80854e+06          0   -2.3174e+06  |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400        3            211.627        -681244        -2.78747e+06          0   -1.18102e+06 |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000                                                                                          |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/8da927e1/checkpoint_000002)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-2.81e+6, val_loss=-2.32e+6, train_loss_epoch=-7.77e+5]\n",
            "Epoch 3:  63%|██████▎   | 37/59 [00:32<00:19,  1.14it/s, v_num=0, train_loss_step=-4.08e+6, val_loss=-1.18e+6, train_loss_epoch=-6.81e+5]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "Epoch 3:  73%|███████▎  | 43/59 [00:38<00:14,  1.13it/s, v_num=0, train_loss_step=-7.01e+6, val_loss=-1.18e+6, train_loss_epoch=-6.81e+5]\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "Epoch 3:  20%|██        | 12/59 [00:13<00:52,  0.89it/s, v_num=0, train_loss_step=-4.34e+6, val_loss=-2.32e+6, train_loss_epoch=-7.77e+5]\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "Epoch 3:  92%|█████████▏| 54/59 [00:47<00:04,  1.14it/s, v_num=0, train_loss_step=-7.31e+6, val_loss=-1.18e+6, train_loss_epoch=-6.81e+5]\n",
            "Epoch 3:  93%|█████████▎| 55/59 [00:48<00:03,  1.14it/s, v_num=0, train_loss_step=-6.09e+6, val_loss=-1.18e+6, train_loss_epoch=-6.81e+5]\n",
            "Epoch 3:  31%|███       | 18/59 [00:19<00:43,  0.94it/s, v_num=0, train_loss_step=-1.99e+6, val_loss=-2.32e+6, train_loss_epoch=-7.77e+5]\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "Epoch 3:  95%|█████████▍| 56/59 [00:48<00:02,  1.14it/s, v_num=0, train_loss_step=-7.95e+6, val_loss=-1.18e+6, train_loss_epoch=-6.81e+5]\n",
            "Epoch 3:  97%|█████████▋| 57/59 [00:49<00:01,  1.15it/s, v_num=0, train_loss_step=-6.63e+6, val_loss=-1.18e+6, train_loss_epoch=-6.81e+5]\n",
            "Epoch 3:  98%|█████████▊| 58/59 [00:50<00:00,  1.15it/s, v_num=0, train_loss_step=-8.88e+6, val_loss=-1.18e+6, train_loss_epoch=-6.81e+5]\n",
            "Epoch 3: 100%|██████████| 59/59 [00:51<00:00,  1.15it/s, v_num=0, train_loss_step=-4.54e+6, val_loss=-1.18e+6, train_loss_epoch=-6.81e+5]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:06,  1.10it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:05,  1.02it/s]\u001b[A\n",
            "Epoch 3:  37%|███▋      | 22/59 [00:24<00:41,  0.89it/s, v_num=0, train_loss_step=-1.63e+6, val_loss=-2.32e+6, train_loss_epoch=-7.77e+5]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:02<00:04,  1.05it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:03<00:03,  1.09it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:04<00:02,  1.17it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:04<00:01,  1.22it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:05<00:00,  1.27it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:05<00:00,  1.40it/s]\u001b[A\n",
            "Epoch 3: 100%|██████████| 59/59 [00:57<00:00,  1.03it/s, v_num=0, train_loss_step=-4.54e+6, val_loss=-5.35e+6, train_loss_epoch=-6.81e+5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/c410a352/checkpoint_000003)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: 100%|██████████| 59/59 [00:57<00:00,  1.02it/s, v_num=0, train_loss_step=-4.54e+6, val_loss=-5.35e+6, train_loss_epoch=-3.92e+6]\n",
            "Epoch 4:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-4.54e+6, val_loss=-5.35e+6, train_loss_epoch=-3.92e+6]\n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:03:35. Total running time: 5min 0s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: c410a352 with val_loss=-5348606.5 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2200, 'ffn_num_layers': 2, 'message_hidden_dim': 400}}\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)          train_loss     train_loss_step     val/f1       val_loss |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500        3            258.666   -777472                  -2.80854e+06          0   -2.3174e+06  |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400        4            269.84         -3.91772e+06        -4.54402e+06          0   -5.34861e+06 |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000                                                                                               |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 3:  46%|████▌     | 27/59 [00:29<00:35,  0.90it/s, v_num=0, train_loss_step=-6.79e+6, val_loss=-2.32e+6, train_loss_epoch=-7.77e+5]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 4:  15%|█▌        | 9/59 [00:06<00:38,  1.30it/s, v_num=0, train_loss_step=-1.25e+7, val_loss=-5.35e+6, train_loss_epoch=-3.92e+6]\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
            "Epoch 4:  24%|██▎       | 14/59 [00:12<00:40,  1.11it/s, v_num=0, train_loss_step=-6.82e+6, val_loss=-5.35e+6, train_loss_epoch=-3.92e+6]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "Epoch 4:  36%|███▌      | 21/59 [00:17<00:32,  1.18it/s, v_num=0, train_loss_step=-7.05e+6, val_loss=-5.35e+6, train_loss_epoch=-3.92e+6]\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
            "Epoch 3:  80%|███████▉  | 47/59 [00:51<00:13,  0.91it/s, v_num=0, train_loss_step=-5.87e+6, val_loss=-2.32e+6, train_loss_epoch=-7.77e+5]\n",
            "Epoch 3:  80%|███████▉  | 47/59 [00:51<00:13,  0.91it/s, v_num=0, train_loss_step=-6.18e+6, val_loss=-2.32e+6, train_loss_epoch=-7.77e+5]\n",
            "Epoch 4:  47%|████▋     | 28/59 [00:24<00:26,  1.16it/s, v_num=0, train_loss_step=-1.03e+7, val_loss=-5.35e+6, train_loss_epoch=-3.92e+6]\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "Epoch 4:  58%|█████▊    | 34/59 [00:29<00:21,  1.14it/s, v_num=0, train_loss_step=-1.62e+7, val_loss=-5.35e+6, train_loss_epoch=-3.92e+6]\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "Epoch 3:  92%|█████████▏| 54/59 [00:59<00:05,  0.91it/s, v_num=0, train_loss_step=-7.63e+6, val_loss=-2.32e+6, train_loss_epoch=-7.77e+5]\n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:04:05. Total running time: 5min 30s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: c410a352 with val_loss=-5348606.5 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2200, 'ffn_num_layers': 2, 'message_hidden_dim': 400}}\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)          train_loss     train_loss_step     val/f1       val_loss |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500        3            258.666   -777472                  -2.80854e+06          0   -2.3174e+06  |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400        4            269.84         -3.91772e+06        -4.54402e+06          0   -5.34861e+06 |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000                                                                                               |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 3:  93%|█████████▎| 55/59 [01:00<00:04,  0.91it/s, v_num=0, train_loss_step=-7.14e+6, val_loss=-2.32e+6, train_loss_epoch=-7.77e+5]\n",
            "Epoch 3:  95%|█████████▍| 56/59 [01:01<00:03,  0.91it/s, v_num=0, train_loss_step=-9.61e+6, val_loss=-2.32e+6, train_loss_epoch=-7.77e+5]\n",
            "Epoch 3:  97%|█████████▋| 57/59 [01:02<00:02,  0.91it/s, v_num=0, train_loss_step=-8.14e+6, val_loss=-2.32e+6, train_loss_epoch=-7.77e+5]\n",
            "Epoch 3:  98%|█████████▊| 58/59 [01:03<00:01,  0.91it/s, v_num=0, train_loss_step=-1.01e+7, val_loss=-2.32e+6, train_loss_epoch=-7.77e+5]\n",
            "Epoch 4:  68%|██████▊   | 40/59 [00:34<00:16,  1.16it/s, v_num=0, train_loss_step=-1.14e+7, val_loss=-5.35e+6, train_loss_epoch=-3.92e+6]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 3: 100%|██████████| 59/59 [01:04<00:00,  0.92it/s, v_num=0, train_loss_step=-5.98e+6, val_loss=-2.32e+6, train_loss_epoch=-7.77e+5]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:04,  1.63it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:04,  1.34it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:02<00:04,  1.06it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:04<00:04,  0.95it/s]\u001b[A\n",
            "Epoch 4:  78%|███████▊  | 46/59 [00:40<00:11,  1.13it/s, v_num=0, train_loss_step=-1.8e+7, val_loss=-5.35e+6, train_loss_epoch=-3.92e+6] \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:05<00:03,  0.93it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:06<00:02,  0.93it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:07<00:01,  0.98it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:07<00:00,  1.08it/s]\u001b[A\n",
            "Epoch 3: 100%|██████████| 59/59 [01:11<00:00,  0.82it/s, v_num=0, train_loss_step=-5.98e+6, val_loss=-2.24e+6, train_loss_epoch=-7.77e+5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-05 11:04:18,204\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/8da927e1/checkpoint_000003)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: 100%|██████████| 59/59 [01:12<00:00,  0.82it/s, v_num=0, train_loss_step=-5.98e+6, val_loss=-2.24e+6, train_loss_epoch=-4.68e+6]\n",
            "Epoch 4:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-5.98e+6, val_loss=-2.24e+6, train_loss_epoch=-4.68e+6]\n",
            "Epoch 4:  88%|████████▊ | 52/59 [00:45<00:06,  1.14it/s, v_num=0, train_loss_step=-2.05e+7, val_loss=-5.35e+6, train_loss_epoch=-3.92e+6]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 4:  92%|█████████▏| 54/59 [00:47<00:04,  1.15it/s, v_num=0, train_loss_step=-2.22e+7, val_loss=-5.35e+6, train_loss_epoch=-3.92e+6]\n",
            "Epoch 4:  93%|█████████▎| 55/59 [00:47<00:03,  1.15it/s, v_num=0, train_loss_step=-1.89e+7, val_loss=-5.35e+6, train_loss_epoch=-3.92e+6]\n",
            "Epoch 4:  95%|█████████▍| 56/59 [00:48<00:02,  1.15it/s, v_num=0, train_loss_step=-2.13e+7, val_loss=-5.35e+6, train_loss_epoch=-3.92e+6]\n",
            "Epoch 4:  97%|█████████▋| 57/59 [00:49<00:01,  1.15it/s, v_num=0, train_loss_step=-2.02e+7, val_loss=-5.35e+6, train_loss_epoch=-3.92e+6]\n",
            "Epoch 4:  98%|█████████▊| 58/59 [00:50<00:00,  1.15it/s, v_num=0, train_loss_step=-8.28e+6, val_loss=-5.35e+6, train_loss_epoch=-3.92e+6]\n",
            "Epoch 4: 100%|██████████| 59/59 [00:50<00:00,  1.16it/s, v_num=0, train_loss_step=-2.77e+7, val_loss=-5.35e+6, train_loss_epoch=-3.92e+6]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4:  12%|█▏        | 7/59 [00:06<00:49,  1.06it/s, v_num=0, train_loss_step=-5.59e+6, val_loss=-2.24e+6, train_loss_epoch=-4.68e+6]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:03,  2.09it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:03,  1.65it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:02<00:03,  1.30it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:03<00:03,  1.15it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:04<00:02,  1.10it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:05<00:01,  1.07it/s]\u001b[A\n",
            "Epoch 4:  19%|█▊        | 11/59 [00:11<00:51,  0.94it/s, v_num=0, train_loss_step=-1.37e+7, val_loss=-2.24e+6, train_loss_epoch=-4.68e+6]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:06<00:00,  1.10it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:06<00:00,  1.22it/s]\u001b[A\n",
            "Epoch 4: 100%|██████████| 59/59 [00:57<00:00,  1.03it/s, v_num=0, train_loss_step=-2.77e+7, val_loss=-2.02e+7, train_loss_epoch=-3.92e+6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-05 11:04:32,655\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/c410a352/checkpoint_000004)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: 100%|██████████| 59/59 [00:58<00:00,  1.02it/s, v_num=0, train_loss_step=-2.77e+7, val_loss=-2.02e+7, train_loss_epoch=-1.29e+7]\n",
            "Epoch 5:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-2.77e+7, val_loss=-2.02e+7, train_loss_epoch=-1.29e+7]\n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:04:35. Total running time: 6min 0s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: c410a352 with val_loss=-20204666.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2200, 'ffn_num_layers': 2, 'message_hidden_dim': 400}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1       val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500        4            331.14    -4.67571e+06        -5.97531e+06          0   -2.23803e+06 |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400        5            328.164   -1.29309e+07        -2.76708e+07          0   -2.02047e+07 |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000                                                                                          |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 5:   7%|▋         | 4/59 [00:03<00:42,  1.30it/s, v_num=0, train_loss_step=-2.29e+7, val_loss=-2.02e+7, train_loss_epoch=-1.29e+7]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "Epoch 5:  19%|█▊        | 11/59 [00:08<00:36,  1.30it/s, v_num=0, train_loss_step=-2.77e+7, val_loss=-2.02e+7, train_loss_epoch=-1.29e+7]\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
            "Epoch 4:  44%|████▍     | 26/59 [00:28<00:36,  0.91it/s, v_num=0, train_loss_step=-6.82e+6, val_loss=-2.24e+6, train_loss_epoch=-4.68e+6]\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "Epoch 5:  39%|███▉      | 23/59 [00:19<00:30,  1.18it/s, v_num=0, train_loss_step=-2.23e+7, val_loss=-2.02e+7, train_loss_epoch=-1.29e+7]\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
            "Epoch 4:  63%|██████▎   | 37/59 [00:39<00:23,  0.93it/s, v_num=0, train_loss_step=-4.9e+6, val_loss=-2.24e+6, train_loss_epoch=-4.68e+6] \u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "Epoch 4:  69%|██████▉   | 41/59 [00:44<00:19,  0.91it/s, v_num=0, train_loss_step=-2.25e+7, val_loss=-2.24e+6, train_loss_epoch=-4.68e+6]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:05:05. Total running time: 6min 30s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: c410a352 with val_loss=-20204666.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2200, 'ffn_num_layers': 2, 'message_hidden_dim': 400}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1       val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500        4            331.14    -4.67571e+06        -5.97531e+06          0   -2.23803e+06 |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400        5            328.164   -1.29309e+07        -2.76708e+07          0   -2.02047e+07 |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000                                                                                          |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 5:  71%|███████   | 42/59 [00:35<00:14,  1.17it/s, v_num=0, train_loss_step=-4.12e+7, val_loss=-2.02e+7, train_loss_epoch=-1.29e+7]\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
            "Epoch 4:  86%|████████▋ | 51/59 [00:55<00:08,  0.92it/s, v_num=0, train_loss_step=-2.12e+7, val_loss=-2.24e+6, train_loss_epoch=-4.68e+6]\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "Epoch 4:  92%|█████████▏| 54/59 [00:59<00:05,  0.91it/s, v_num=0, train_loss_step=-2.35e+7, val_loss=-2.24e+6, train_loss_epoch=-4.68e+6]\n",
            "Epoch 4:  93%|█████████▎| 55/59 [01:00<00:04,  0.91it/s, v_num=0, train_loss_step=-2.27e+7, val_loss=-2.24e+6, train_loss_epoch=-4.68e+6]\n",
            "Epoch 5:  90%|████████▉ | 53/59 [00:46<00:05,  1.15it/s, v_num=0, train_loss_step=-4.39e+7, val_loss=-2.02e+7, train_loss_epoch=-1.29e+7]\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "Epoch 4:  95%|█████████▍| 56/59 [01:01<00:03,  0.92it/s, v_num=0, train_loss_step=-2.56e+7, val_loss=-2.24e+6, train_loss_epoch=-4.68e+6]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:04,  1.54it/s]\u001b[A\n",
            "Epoch 4: 100%|██████████| 59/59 [01:03<00:00,  0.93it/s, v_num=0, train_loss_step=-3.23e+7, val_loss=-2.24e+6, train_loss_epoch=-4.68e+6]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:04,  1.47it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:05<00:00,  1.17it/s]\u001b[A\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "Epoch 5: 100%|██████████| 59/59 [00:50<00:00,  1.17it/s, v_num=0, train_loss_step=-3.83e+7, val_loss=-2.02e+7, train_loss_epoch=-1.29e+7]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:06<00:00,  1.24it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Epoch 4: 100%|██████████| 59/59 [01:10<00:00,  0.84it/s, v_num=0, train_loss_step=-3.23e+7, val_loss=-2.67e+7, train_loss_epoch=-4.68e+6]\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Epoch 4: 100%|██████████| 59/59 [01:11<00:00,  0.83it/s, v_num=0, train_loss_step=-3.23e+7, val_loss=-2.67e+7, train_loss_epoch=-1.46e+7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/8da927e1/checkpoint_000004)\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/c410a352/checkpoint_000005)\n",
            "2025-05-05 11:05:29,963\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-3.23e+7, val_loss=-2.67e+7, train_loss_epoch=-1.46e+7]\n",
            "Epoch 6:   2%|▏         | 1/59 [00:01<01:03,  0.91it/s, v_num=0, train_loss_step=-4.69e+7, val_loss=-3.79e+7, train_loss_epoch=-3.08e+7]\n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:05<00:00,  1.34it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:05<00:00,  1.44it/s]\u001b[A\n",
            "Epoch 5: 100%|██████████| 59/59 [00:56<00:00,  1.05it/s, v_num=0, train_loss_step=-3.83e+7, val_loss=-3.79e+7, train_loss_epoch=-1.29e+7]\n",
            "Epoch 5: 100%|██████████| 59/59 [00:56<00:00,  1.04it/s, v_num=0, train_loss_step=-3.83e+7, val_loss=-3.79e+7, train_loss_epoch=-3.08e+7]\n",
            "Epoch 6:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-3.83e+7, val_loss=-3.79e+7, train_loss_epoch=-3.08e+7]\n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:05:36. Total running time: 7min 0s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: c410a352 with val_loss=-37879492.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2200, 'ffn_num_layers': 2, 'message_hidden_dim': 400}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1       val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500        5            402.545   -1.45693e+07        -3.22772e+07          0   -2.66766e+07 |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400        6            385.424   -3.07603e+07        -3.83142e+07          0   -3.78795e+07 |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000                                                                                          |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 6:  14%|█▎        | 8/59 [00:06<00:41,  1.23it/s, v_num=0, train_loss_step=-3.73e+7, val_loss=-3.79e+7, train_loss_epoch=-3.08e+7]\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
            "Epoch 6:  22%|██▏       | 13/59 [00:10<00:36,  1.26it/s, v_num=0, train_loss_step=-5.38e+7, val_loss=-3.79e+7, train_loss_epoch=-3.08e+7]\n",
            "Epoch 6:  22%|██▏       | 13/59 [00:10<00:36,  1.26it/s, v_num=0, train_loss_step=-5.51e+7, val_loss=-3.79e+7, train_loss_epoch=-3.08e+7]\n",
            "Epoch 5:  20%|██        | 12/59 [00:12<00:49,  0.95it/s, v_num=0, train_loss_step=-2.78e+7, val_loss=-2.67e+7, train_loss_epoch=-1.46e+7]\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "Epoch 5:  27%|██▋       | 16/59 [00:17<00:48,  0.89it/s, v_num=0, train_loss_step=-3.32e+7, val_loss=-2.67e+7, train_loss_epoch=-1.46e+7]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "Epoch 5:  32%|███▏      | 19/59 [00:20<00:43,  0.91it/s, v_num=0, train_loss_step=-2.77e+7, val_loss=-2.67e+7, train_loss_epoch=-1.46e+7]\n",
            "Epoch 5:  32%|███▏      | 19/59 [00:20<00:43,  0.91it/s, v_num=0, train_loss_step=-1.91e+7, val_loss=-2.67e+7, train_loss_epoch=-1.46e+7]\n",
            "Epoch 6:  46%|████▌     | 27/59 [00:22<00:27,  1.18it/s, v_num=0, train_loss_step=-4.07e+7, val_loss=-3.79e+7, train_loss_epoch=-3.08e+7]\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "Epoch 5:  44%|████▍     | 26/59 [00:28<00:36,  0.91it/s, v_num=0, train_loss_step=-3.15e+7, val_loss=-2.67e+7, train_loss_epoch=-1.46e+7]\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "Epoch 6:  64%|██████▍   | 38/59 [00:33<00:18,  1.14it/s, v_num=0, train_loss_step=-6.96e+7, val_loss=-3.79e+7, train_loss_epoch=-3.08e+7]\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:06:06. Total running time: 7min 30s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: c410a352 with val_loss=-37879492.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2200, 'ffn_num_layers': 2, 'message_hidden_dim': 400}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1       val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500        5            402.545   -1.45693e+07        -3.22772e+07          0   -2.66766e+07 |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400        6            385.424   -3.07603e+07        -3.83142e+07          0   -3.78795e+07 |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000                                                                                          |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 6:  76%|███████▋  | 45/59 [00:38<00:12,  1.16it/s, v_num=0, train_loss_step=-5.36e+7, val_loss=-3.79e+7, train_loss_epoch=-3.08e+7]\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "Epoch 6:  85%|████████▍ | 50/59 [00:43<00:07,  1.14it/s, v_num=0, train_loss_step=-7.9e+7, val_loss=-3.79e+7, train_loss_epoch=-3.08e+7] \u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "Epoch 6:  92%|█████████▏| 54/59 [00:47<00:04,  1.13it/s, v_num=0, train_loss_step=-7.48e+7, val_loss=-3.79e+7, train_loss_epoch=-3.08e+7]\n",
            "Epoch 6:  93%|█████████▎| 55/59 [00:48<00:03,  1.13it/s, v_num=0, train_loss_step=-6.89e+7, val_loss=-3.79e+7, train_loss_epoch=-3.08e+7]\n",
            "Epoch 5:  76%|███████▋  | 45/59 [00:49<00:15,  0.91it/s, v_num=0, train_loss_step=-3.77e+7, val_loss=-2.67e+7, train_loss_epoch=-1.46e+7]\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "Epoch 6:  95%|█████████▍| 56/59 [00:49<00:02,  1.14it/s, v_num=0, train_loss_step=-7.93e+7, val_loss=-3.79e+7, train_loss_epoch=-3.08e+7]\n",
            "Epoch 6:  97%|█████████▋| 57/59 [00:49<00:01,  1.14it/s, v_num=0, train_loss_step=-9.13e+7, val_loss=-3.79e+7, train_loss_epoch=-3.08e+7]\n",
            "Epoch 6:  98%|█████████▊| 58/59 [00:50<00:00,  1.14it/s, v_num=0, train_loss_step=-7.09e+7, val_loss=-3.79e+7, train_loss_epoch=-3.08e+7]\n",
            "Epoch 6: 100%|██████████| 59/59 [00:51<00:00,  1.15it/s, v_num=0, train_loss_step=-7.89e+7, val_loss=-3.79e+7, train_loss_epoch=-3.08e+7]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:03,  2.01it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:03,  1.81it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:01<00:02,  1.74it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:02<00:02,  1.69it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:02<00:01,  1.71it/s]\u001b[A\n",
            "Epoch 5:  85%|████████▍ | 50/59 [00:54<00:09,  0.92it/s, v_num=0, train_loss_step=-5.52e+7, val_loss=-2.67e+7, train_loss_epoch=-1.46e+7]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:03<00:01,  1.68it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:04<00:00,  1.68it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:04<00:00,  1.81it/s]\u001b[A\n",
            "Epoch 6: 100%|██████████| 59/59 [00:55<00:00,  1.06it/s, v_num=0, train_loss_step=-7.89e+7, val_loss=-7.42e+7, train_loss_epoch=-3.08e+7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/c410a352/checkpoint_000006)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: 100%|██████████| 59/59 [00:56<00:00,  1.05it/s, v_num=0, train_loss_step=-7.89e+7, val_loss=-7.42e+7, train_loss_epoch=-5.4e+7] \n",
            "Epoch 7:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-7.89e+7, val_loss=-7.42e+7, train_loss_epoch=-5.4e+7]\n",
            "Epoch 5:  92%|█████████▏| 54/59 [00:58<00:05,  0.92it/s, v_num=0, train_loss_step=-3.24e+7, val_loss=-2.67e+7, train_loss_epoch=-1.46e+7]\n",
            "Epoch 7:   3%|▎         | 2/59 [00:02<01:13,  0.77it/s, v_num=0, train_loss_step=-8.42e+7, val_loss=-7.42e+7, train_loss_epoch=-5.4e+7]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 5:  93%|█████████▎| 55/59 [01:00<00:04,  0.91it/s, v_num=0, train_loss_step=-5.8e+7, val_loss=-2.67e+7, train_loss_epoch=-1.46e+7] \n",
            "Epoch 5:  95%|█████████▍| 56/59 [01:01<00:03,  0.91it/s, v_num=0, train_loss_step=-4.56e+7, val_loss=-2.67e+7, train_loss_epoch=-1.46e+7]\n",
            "Epoch 5:  97%|█████████▋| 57/59 [01:02<00:02,  0.91it/s, v_num=0, train_loss_step=-4.65e+7, val_loss=-2.67e+7, train_loss_epoch=-1.46e+7]\n",
            "Epoch 5:  98%|█████████▊| 58/59 [01:03<00:01,  0.91it/s, v_num=0, train_loss_step=-4.89e+7, val_loss=-2.67e+7, train_loss_epoch=-1.46e+7]\n",
            "Epoch 5: 100%|██████████| 59/59 [01:04<00:00,  0.92it/s, v_num=0, train_loss_step=-5.09e+7, val_loss=-2.67e+7, train_loss_epoch=-1.46e+7]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:04,  1.53it/s]\u001b[A\n",
            "Epoch 7:  12%|█▏        | 7/59 [00:07<00:53,  0.98it/s, v_num=0, train_loss_step=-9.09e+7, val_loss=-7.42e+7, train_loss_epoch=-5.4e+7]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:04,  1.44it/s]\u001b[A\n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:06:36. Total running time: 8min 0s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: c410a352 with val_loss=-74246352.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2200, 'ffn_num_layers': 2, 'message_hidden_dim': 400}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1       val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500        5            402.545   -1.45693e+07        -3.22772e+07          0   -2.66766e+07 |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400        7            442.153   -5.39702e+07        -7.88899e+07          0   -7.42464e+07 |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000                                                                                          |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:02<00:03,  1.38it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:02<00:02,  1.36it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:03<00:02,  1.37it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:04<00:01,  1.35it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:05<00:00,  1.33it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:05<00:00,  1.46it/s]\u001b[A\n",
            "Epoch 5: 100%|██████████| 59/59 [01:09<00:00,  0.85it/s, v_num=0, train_loss_step=-5.09e+7, val_loss=-4.95e+7, train_loss_epoch=-1.46e+7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/8da927e1/checkpoint_000005)\n",
            "2025-05-05 11:06:40,397\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \rEpoch 5: 100%|██████████| 59/59 [01:10<00:00,  0.84it/s, v_num=0, train_loss_step=-5.09e+7, val_loss=-4.95e+7, train_loss_epoch=-3.47e+7]\n",
            "Epoch 7:  24%|██▎       | 14/59 [00:12<00:40,  1.12it/s, v_num=0, train_loss_step=-9.1e+7, val_loss=-7.42e+7, train_loss_epoch=-5.4e+7] \u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 6:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-5.09e+7, val_loss=-4.95e+7, train_loss_epoch=-3.47e+7]\n",
            "Epoch 7:  34%|███▍      | 20/59 [00:18<00:36,  1.06it/s, v_num=0, train_loss_step=-9.11e+7, val_loss=-7.42e+7, train_loss_epoch=-5.4e+7]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "Epoch 6:  15%|█▌        | 9/59 [00:10<00:58,  0.85it/s, v_num=0, train_loss_step=-6.22e+7, val_loss=-4.95e+7, train_loss_epoch=-3.47e+7]\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "Epoch 6:  25%|██▌       | 15/59 [00:16<00:47,  0.92it/s, v_num=0, train_loss_step=-5.28e+7, val_loss=-4.95e+7, train_loss_epoch=-3.47e+7]\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
            "Epoch 7:  64%|██████▍   | 38/59 [00:34<00:19,  1.09it/s, v_num=0, train_loss_step=-9.49e+7, val_loss=-7.42e+7, train_loss_epoch=-5.4e+7]\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "Epoch 6:  34%|███▍      | 20/59 [00:23<00:45,  0.87it/s, v_num=0, train_loss_step=-7.42e+7, val_loss=-4.95e+7, train_loss_epoch=-3.47e+7]\n",
            "Epoch 6:  34%|███▍      | 20/59 [00:23<00:45,  0.87it/s, v_num=0, train_loss_step=-4.87e+7, val_loss=-4.95e+7, train_loss_epoch=-3.47e+7]\n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:07:06. Total running time: 8min 30s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: c410a352 with val_loss=-74246352.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2200, 'ffn_num_layers': 2, 'message_hidden_dim': 400}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1       val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500        6            473.216   -3.46628e+07        -5.09357e+07          0   -4.94642e+07 |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400        7            442.153   -5.39702e+07        -7.88899e+07          0   -7.42464e+07 |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000                                                                                          |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 7:  76%|███████▋  | 45/59 [00:40<00:12,  1.12it/s, v_num=0, train_loss_step=-5.39e+7, val_loss=-7.42e+7, train_loss_epoch=-5.4e+7]\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "Epoch 6:  49%|████▉     | 29/59 [00:31<00:32,  0.91it/s, v_num=0, train_loss_step=-7.4e+7, val_loss=-4.95e+7, train_loss_epoch=-3.47e+7] \u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "Epoch 7:  92%|█████████▏| 54/59 [00:49<00:04,  1.10it/s, v_num=0, train_loss_step=-9.09e+7, val_loss=-7.42e+7, train_loss_epoch=-5.4e+7]\n",
            "Epoch 7:  93%|█████████▎| 55/59 [00:49<00:03,  1.10it/s, v_num=0, train_loss_step=-1.16e+8, val_loss=-7.42e+7, train_loss_epoch=-5.4e+7]\n",
            "Epoch 7:  95%|█████████▍| 56/59 [00:50<00:02,  1.10it/s, v_num=0, train_loss_step=-1.19e+8, val_loss=-7.42e+7, train_loss_epoch=-5.4e+7]\n",
            "Epoch 6:  54%|█████▍    | 32/59 [00:36<00:30,  0.88it/s, v_num=0, train_loss_step=-6.46e+7, val_loss=-4.95e+7, train_loss_epoch=-3.47e+7]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 7:  97%|█████████▋| 57/59 [00:51<00:01,  1.11it/s, v_num=0, train_loss_step=-1.07e+8, val_loss=-7.42e+7, train_loss_epoch=-5.4e+7]\n",
            "Epoch 7:  98%|█████████▊| 58/59 [00:52<00:00,  1.11it/s, v_num=0, train_loss_step=-1.25e+8, val_loss=-7.42e+7, train_loss_epoch=-5.4e+7]\n",
            "Epoch 7: 100%|██████████| 59/59 [00:52<00:00,  1.12it/s, v_num=0, train_loss_step=-7.81e+7, val_loss=-7.42e+7, train_loss_epoch=-5.4e+7]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:03,  2.13it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:03,  1.83it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:01<00:02,  1.72it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:02<00:02,  1.70it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:02<00:01,  1.69it/s]\u001b[A\n",
            "Epoch 6:  63%|██████▎   | 37/59 [00:41<00:24,  0.89it/s, v_num=0, train_loss_step=-7.02e+7, val_loss=-4.95e+7, train_loss_epoch=-3.47e+7]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:03<00:01,  1.67it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:04<00:00,  1.66it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:04<00:00,  1.81it/s]\u001b[A\n",
            "Epoch 7: 100%|██████████| 59/59 [00:57<00:00,  1.03it/s, v_num=0, train_loss_step=-7.81e+7, val_loss=-1.12e+8, train_loss_epoch=-5.4e+7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/c410a352/checkpoint_000007)\n",
            "2025-05-05 11:07:24,999\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: 100%|██████████| 59/59 [00:57<00:00,  1.02it/s, v_num=0, train_loss_step=-7.81e+7, val_loss=-1.12e+8, train_loss_epoch=-8.59e+7]\n",
            "Epoch 8:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-7.81e+7, val_loss=-1.12e+8, train_loss_epoch=-8.59e+7]\n",
            "Epoch 8:   5%|▌         | 3/59 [00:02<00:51,  1.08it/s, v_num=0, train_loss_step=-1.03e+8, val_loss=-1.12e+8, train_loss_epoch=-8.59e+7]\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "Epoch 8:  14%|█▎        | 8/59 [00:08<00:52,  0.97it/s, v_num=0, train_loss_step=-1.32e+8, val_loss=-1.12e+8, train_loss_epoch=-8.59e+7]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:07:36. Total running time: 9min 0s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: c410a352 with val_loss=-111807984.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2200, 'ffn_num_layers': 2, 'message_hidden_dim': 400}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1       val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500        6            473.216   -3.46628e+07        -5.09357e+07          0   -4.94642e+07 |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400        8            500.355   -8.585e+07          -7.81283e+07          0   -1.11808e+08 |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000                                                                                          |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 8:  25%|██▌       | 15/59 [00:13<00:39,  1.11it/s, v_num=0, train_loss_step=-1.18e+8, val_loss=-1.12e+8, train_loss_epoch=-8.59e+7]\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
            "Epoch 6:  92%|█████████▏| 54/59 [00:59<00:05,  0.91it/s, v_num=0, train_loss_step=-8.79e+7, val_loss=-4.95e+7, train_loss_epoch=-3.47e+7]\n",
            "Epoch 6:  93%|█████████▎| 55/59 [01:00<00:04,  0.91it/s, v_num=0, train_loss_step=-8.13e+7, val_loss=-4.95e+7, train_loss_epoch=-3.47e+7]\n",
            "Epoch 6:  95%|█████████▍| 56/59 [01:01<00:03,  0.91it/s, v_num=0, train_loss_step=-8.83e+7, val_loss=-4.95e+7, train_loss_epoch=-3.47e+7]\n",
            "Epoch 6:  97%|█████████▋| 57/59 [01:03<00:02,  0.90it/s, v_num=0, train_loss_step=-1.02e+8, val_loss=-4.95e+7, train_loss_epoch=-3.47e+7]\n",
            "Epoch 8:  34%|███▍      | 20/59 [00:18<00:35,  1.10it/s, v_num=0, train_loss_step=-8.73e+7, val_loss=-1.12e+8, train_loss_epoch=-8.59e+7]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 6:  98%|█████████▊| 58/59 [01:04<00:01,  0.89it/s, v_num=0, train_loss_step=-8.35e+7, val_loss=-4.95e+7, train_loss_epoch=-3.47e+7]\n",
            "Epoch 6: 100%|██████████| 59/59 [01:05<00:00,  0.90it/s, v_num=0, train_loss_step=-9e+7, val_loss=-4.95e+7, train_loss_epoch=-3.47e+7]   \n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:05,  1.36it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:04,  1.34it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:02<00:03,  1.30it/s]\u001b[A\n",
            "Epoch 8:  44%|████▍     | 26/59 [00:24<00:30,  1.08it/s, v_num=0, train_loss_step=-1.39e+8, val_loss=-1.12e+8, train_loss_epoch=-8.59e+7]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:03<00:03,  1.30it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:03<00:02,  1.32it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:04<00:01,  1.31it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:05<00:00,  1.31it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:05<00:00,  1.43it/s]\u001b[A\n",
            "Epoch 6: 100%|██████████| 59/59 [01:11<00:00,  0.83it/s, v_num=0, train_loss_step=-9e+7, val_loss=-7.46e+7, train_loss_epoch=-3.47e+7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/8da927e1/checkpoint_000006)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: 100%|██████████| 59/59 [01:11<00:00,  0.82it/s, v_num=0, train_loss_step=-9e+7, val_loss=-7.46e+7, train_loss_epoch=-6.21e+7]\n",
            "Epoch 7:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-9e+7, val_loss=-7.46e+7, train_loss_epoch=-6.21e+7]\n",
            "Epoch 7:   3%|▎         | 2/59 [00:01<00:54,  1.05it/s, v_num=0, train_loss_step=-9.22e+7, val_loss=-7.46e+7, train_loss_epoch=-6.21e+7]\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "Epoch 8:  64%|██████▍   | 38/59 [00:34<00:19,  1.09it/s, v_num=0, train_loss_step=-1.06e+8, val_loss=-1.12e+8, train_loss_epoch=-8.59e+7]\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "Epoch 7:  19%|█▊        | 11/59 [00:12<00:53,  0.89it/s, v_num=0, train_loss_step=-1.05e+8, val_loss=-7.46e+7, train_loss_epoch=-6.21e+7]\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:08:06. Total running time: 9min 30s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: c410a352 with val_loss=-111807984.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2200, 'ffn_num_layers': 2, 'message_hidden_dim': 400}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1       val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500        7            545.381   -6.20807e+07        -8.99745e+07          0   -7.45657e+07 |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400        8            500.355   -8.585e+07          -7.81283e+07          0   -1.11808e+08 |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000                                                                                          |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 8:  86%|████████▋ | 51/59 [00:45<00:07,  1.13it/s, v_num=0, train_loss_step=-1.38e+8, val_loss=-1.12e+8, train_loss_epoch=-8.59e+7]\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
            "Epoch 8:  92%|█████████▏| 54/59 [00:48<00:04,  1.12it/s, v_num=0, train_loss_step=-8.99e+7, val_loss=-1.12e+8, train_loss_epoch=-8.59e+7]\n",
            "Epoch 8:  93%|█████████▎| 55/59 [00:49<00:03,  1.11it/s, v_num=0, train_loss_step=-1.47e+8, val_loss=-1.12e+8, train_loss_epoch=-8.59e+7]\n",
            "Epoch 8:  95%|█████████▍| 56/59 [00:50<00:02,  1.10it/s, v_num=0, train_loss_step=-1.43e+8, val_loss=-1.12e+8, train_loss_epoch=-8.59e+7]\n",
            "Epoch 7:  34%|███▍      | 20/59 [00:22<00:43,  0.89it/s, v_num=0, train_loss_step=-9.5e+7, val_loss=-7.46e+7, train_loss_epoch=-6.21e+7] \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 8:  97%|█████████▋| 57/59 [00:51<00:01,  1.10it/s, v_num=0, train_loss_step=-1.49e+8, val_loss=-1.12e+8, train_loss_epoch=-8.59e+7]\n",
            "Epoch 8:  98%|█████████▊| 58/59 [00:52<00:00,  1.10it/s, v_num=0, train_loss_step=-1.13e+8, val_loss=-1.12e+8, train_loss_epoch=-8.59e+7]\n",
            "Epoch 8: 100%|██████████| 59/59 [00:52<00:00,  1.11it/s, v_num=0, train_loss_step=-1.61e+8, val_loss=-1.12e+8, train_loss_epoch=-8.59e+7]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:03,  2.04it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:03,  1.80it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:01<00:02,  1.74it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:02<00:02,  1.70it/s]\u001b[A\n",
            "Epoch 7:  44%|████▍     | 26/59 [00:28<00:36,  0.91it/s, v_num=0, train_loss_step=-4e+7, val_loss=-7.46e+7, train_loss_epoch=-6.21e+7]  \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:02<00:01,  1.71it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:03<00:01,  1.68it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:04<00:00,  1.68it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:04<00:00,  1.83it/s]\u001b[A\n",
            "Epoch 8: 100%|██████████| 59/59 [00:57<00:00,  1.03it/s, v_num=0, train_loss_step=-1.61e+8, val_loss=-7.49e+7, train_loss_epoch=-8.59e+7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-05 11:08:23,284\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/c410a352/checkpoint_000008)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: 100%|██████████| 59/59 [00:57<00:00,  1.02it/s, v_num=0, train_loss_step=-1.61e+8, val_loss=-7.49e+7, train_loss_epoch=-1.21e+8]\n",
            "Epoch 9:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-1.61e+8, val_loss=-7.49e+7, train_loss_epoch=-1.21e+8]\n",
            "Epoch 9:   7%|▋         | 4/59 [00:03<00:42,  1.29it/s, v_num=0, train_loss_step=-1.23e+8, val_loss=-7.49e+7, train_loss_epoch=-1.21e+8]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "Epoch 7:  59%|█████▉    | 35/59 [00:39<00:26,  0.89it/s, v_num=0, train_loss_step=-9.25e+7, val_loss=-7.46e+7, train_loss_epoch=-6.21e+7]\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:08:36. Total running time: 10min 0s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: c410a352 with val_loss=-74906560.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2200, 'ffn_num_layers': 2, 'message_hidden_dim': 400}}\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step      val/f1       val_loss |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500        7            545.381   -6.20807e+07        -8.99745e+07   0           -7.45657e+07 |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400        9            558.637   -1.21343e+08        -1.61388e+08   0.0190476   -7.49066e+07 |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000                                                                                           |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 7:  69%|██████▉   | 41/59 [00:44<00:19,  0.91it/s, v_num=0, train_loss_step=-9.75e+7, val_loss=-7.46e+7, train_loss_epoch=-6.21e+7]\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
            "Epoch 9:  37%|███▋      | 22/59 [00:19<00:32,  1.14it/s, v_num=0, train_loss_step=-1.69e+8, val_loss=-7.49e+7, train_loss_epoch=-1.21e+8]\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "Epoch 9:  46%|████▌     | 27/59 [00:24<00:29,  1.10it/s, v_num=0, train_loss_step=-1.04e+8, val_loss=-7.49e+7, train_loss_epoch=-1.21e+8]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "Epoch 9:  49%|████▉     | 29/59 [00:26<00:27,  1.11it/s, v_num=0, train_loss_step=-1.66e+8, val_loss=-7.49e+7, train_loss_epoch=-1.21e+8]\n",
            "Epoch 9:  49%|████▉     | 29/59 [00:26<00:27,  1.11it/s, v_num=0, train_loss_step=-1.48e+8, val_loss=-7.49e+7, train_loss_epoch=-1.21e+8]\n",
            "Epoch 7:  92%|█████████▏| 54/59 [00:59<00:05,  0.91it/s, v_num=0, train_loss_step=-1.08e+8, val_loss=-7.46e+7, train_loss_epoch=-6.21e+7]\n",
            "Epoch 7:  93%|█████████▎| 55/59 [01:00<00:04,  0.91it/s, v_num=0, train_loss_step=-1.33e+8, val_loss=-7.46e+7, train_loss_epoch=-6.21e+7]\n",
            "Epoch 9:  58%|█████▊    | 34/59 [00:30<00:22,  1.13it/s, v_num=0, train_loss_step=-1.5e+8, val_loss=-7.49e+7, train_loss_epoch=-1.21e+8] \u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "Epoch 7:  95%|█████████▍| 56/59 [01:01<00:03,  0.92it/s, v_num=0, train_loss_step=-1.32e+8, val_loss=-7.46e+7, train_loss_epoch=-6.21e+7]\n",
            "Epoch 7:  97%|█████████▋| 57/59 [01:02<00:02,  0.92it/s, v_num=0, train_loss_step=-1.28e+8, val_loss=-7.46e+7, train_loss_epoch=-6.21e+7]\n",
            "Epoch 7:  98%|█████████▊| 58/59 [01:03<00:01,  0.92it/s, v_num=0, train_loss_step=-1.4e+8, val_loss=-7.46e+7, train_loss_epoch=-6.21e+7] \n",
            "Epoch 7: 100%|██████████| 59/59 [01:03<00:00,  0.93it/s, v_num=0, train_loss_step=-1.03e+8, val_loss=-7.46e+7, train_loss_epoch=-6.21e+7]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:05,  1.18it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:02<00:06,  0.89it/s]\u001b[A\n",
            "Epoch 9:  66%|██████▌   | 39/59 [00:34<00:17,  1.15it/s, v_num=0, train_loss_step=-1.53e+8, val_loss=-7.49e+7, train_loss_epoch=-1.21e+8]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:03<00:06,  0.82it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:04<00:04,  0.81it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:05<00:03,  0.87it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:06<00:02,  0.92it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:07<00:01,  0.96it/s]\u001b[A\n",
            "Epoch 9:  76%|███████▋  | 45/59 [00:40<00:12,  1.12it/s, v_num=0, train_loss_step=-1.98e+8, val_loss=-7.49e+7, train_loss_epoch=-1.21e+8]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:07<00:00,  1.07it/s]\u001b[A\n",
            "Epoch 7: 100%|██████████| 59/59 [01:11<00:00,  0.83it/s, v_num=0, train_loss_step=-1.03e+8, val_loss=-1.32e+8, train_loss_epoch=-6.21e+7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-05 11:09:04,652\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/8da927e1/checkpoint_000007)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: 100%|██████████| 59/59 [01:11<00:00,  0.82it/s, v_num=0, train_loss_step=-1.03e+8, val_loss=-1.32e+8, train_loss_epoch=-9.84e+7]\n",
            "Epoch 8:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-1.03e+8, val_loss=-1.32e+8, train_loss_epoch=-9.84e+7]\n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:09:06. Total running time: 10min 30s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-131581568.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step      val/f1       val_loss |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500        8            617.404   -9.83748e+07        -1.0306e+08    0.128       -1.31582e+08 |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400        9            558.637   -1.21343e+08        -1.61388e+08   0.0190476   -7.49066e+07 |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000                                                                                           |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 8:   7%|▋         | 4/59 [00:03<00:53,  1.02it/s, v_num=0, train_loss_step=-1.15e+8, val_loss=-1.32e+8, train_loss_epoch=-9.84e+7]\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "Epoch 9:  92%|█████████▏| 54/59 [00:47<00:04,  1.14it/s, v_num=0, train_loss_step=-2.07e+8, val_loss=-7.49e+7, train_loss_epoch=-1.21e+8]\n",
            "Epoch 9:  93%|█████████▎| 55/59 [00:48<00:03,  1.15it/s, v_num=0, train_loss_step=-1.79e+8, val_loss=-7.49e+7, train_loss_epoch=-1.21e+8]\n",
            "Epoch 9:  95%|█████████▍| 56/59 [00:49<00:02,  1.14it/s, v_num=0, train_loss_step=-1.39e+8, val_loss=-7.49e+7, train_loss_epoch=-1.21e+8]\n",
            "Epoch 9:  97%|█████████▋| 57/59 [00:50<00:01,  1.12it/s, v_num=0, train_loss_step=-1.64e+8, val_loss=-7.49e+7, train_loss_epoch=-1.21e+8]\n",
            "Epoch 8:  14%|█▎        | 8/59 [00:08<00:54,  0.94it/s, v_num=0, train_loss_step=-1.52e+8, val_loss=-1.32e+8, train_loss_epoch=-9.84e+7]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 9:  98%|█████████▊| 58/59 [00:51<00:00,  1.12it/s, v_num=0, train_loss_step=-1.96e+8, val_loss=-7.49e+7, train_loss_epoch=-1.21e+8]\n",
            "Epoch 9: 100%|██████████| 59/59 [00:52<00:00,  1.12it/s, v_num=0, train_loss_step=-1.54e+8, val_loss=-7.49e+7, train_loss_epoch=-1.21e+8]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:04,  1.52it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:03,  1.57it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:01<00:03,  1.58it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:02<00:02,  1.59it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:03<00:01,  1.60it/s]\u001b[A\n",
            "Epoch 8:  20%|██        | 12/59 [00:13<00:53,  0.88it/s, v_num=0, train_loss_step=-1.35e+8, val_loss=-1.32e+8, train_loss_epoch=-9.84e+7]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:03<00:01,  1.61it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:04<00:00,  1.61it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:04<00:00,  1.76it/s]\u001b[A\n",
            "Epoch 9: 100%|██████████| 59/59 [00:57<00:00,  1.03it/s, v_num=0, train_loss_step=-1.54e+8, val_loss=-1.63e+8, train_loss_epoch=-1.21e+8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/c410a352/checkpoint_000009)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: 100%|██████████| 59/59 [00:57<00:00,  1.02it/s, v_num=0, train_loss_step=-1.54e+8, val_loss=-1.63e+8, train_loss_epoch=-1.59e+8]\n",
            "Epoch 10:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-1.54e+8, val_loss=-1.63e+8, train_loss_epoch=-1.59e+8]\n",
            "Epoch 10:   7%|▋         | 4/59 [00:03<00:42,  1.30it/s, v_num=0, train_loss_step=-1.93e+8, val_loss=-1.63e+8, train_loss_epoch=-1.59e+8]\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "Epoch 10:  15%|█▌        | 9/59 [00:08<00:45,  1.09it/s, v_num=0, train_loss_step=-1.26e+8, val_loss=-1.63e+8, train_loss_epoch=-1.59e+8]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "Epoch 8:  46%|████▌     | 27/59 [00:30<00:35,  0.89it/s, v_num=0, train_loss_step=-1.22e+8, val_loss=-1.32e+8, train_loss_epoch=-9.84e+7]\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:09:36. Total running time: 11min 1s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: c410a352 with val_loss=-162616704.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2200, 'ffn_num_layers': 2, 'message_hidden_dim': 400}}\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step      val/f1       val_loss |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500        8            617.404   -9.83748e+07        -1.0306e+08    0.128       -1.31582e+08 |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400       10            616.806   -1.59364e+08        -1.54004e+08   0.0190476   -1.62617e+08 |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000                                                                                           |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 8:  56%|█████▌    | 33/59 [00:35<00:28,  0.92it/s, v_num=0, train_loss_step=-1.25e+8, val_loss=-1.32e+8, train_loss_epoch=-9.84e+7]\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
            "Epoch 8:  63%|██████▎   | 37/59 [00:41<00:24,  0.89it/s, v_num=0, train_loss_step=-1.25e+8, val_loss=-1.32e+8, train_loss_epoch=-9.84e+7]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "Epoch 10:  58%|█████▊    | 34/59 [00:30<00:22,  1.13it/s, v_num=0, train_loss_step=-1.79e+8, val_loss=-1.63e+8, train_loss_epoch=-1.59e+8]\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
            "Epoch 8:  81%|████████▏ | 48/59 [00:52<00:11,  0.92it/s, v_num=0, train_loss_step=-1.72e+8, val_loss=-1.32e+8, train_loss_epoch=-9.84e+7]\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "Epoch 10:  76%|███████▋  | 45/59 [00:40<00:12,  1.11it/s, v_num=0, train_loss_step=-1.67e+8, val_loss=-1.63e+8, train_loss_epoch=-1.59e+8]\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "Epoch 8:  92%|█████████▏| 54/59 [00:59<00:05,  0.91it/s, v_num=0, train_loss_step=-1.01e+8, val_loss=-1.32e+8, train_loss_epoch=-9.84e+7]\n",
            "Epoch 8:  93%|█████████▎| 55/59 [01:00<00:04,  0.91it/s, v_num=0, train_loss_step=-1.54e+8, val_loss=-1.32e+8, train_loss_epoch=-9.84e+7]\n",
            "Epoch 8:  95%|█████████▍| 56/59 [01:01<00:03,  0.91it/s, v_num=0, train_loss_step=-1.65e+8, val_loss=-1.32e+8, train_loss_epoch=-9.84e+7]\n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:10:06. Total running time: 11min 31s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: c410a352 with val_loss=-162616704.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2200, 'ffn_num_layers': 2, 'message_hidden_dim': 400}}\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step      val/f1       val_loss |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500        8            617.404   -9.83748e+07        -1.0306e+08    0.128       -1.31582e+08 |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400       10            616.806   -1.59364e+08        -1.54004e+08   0.0190476   -1.62617e+08 |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000                                                                                           |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 8:  97%|█████████▋| 57/59 [01:02<00:02,  0.91it/s, v_num=0, train_loss_step=-1.68e+8, val_loss=-1.32e+8, train_loss_epoch=-9.84e+7]\n",
            "Epoch 10:  88%|████████▊ | 52/59 [00:45<00:06,  1.14it/s, v_num=0, train_loss_step=-1.69e+8, val_loss=-1.63e+8, train_loss_epoch=-1.59e+8]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "Epoch 8:  98%|█████████▊| 58/59 [01:03<00:01,  0.92it/s, v_num=0, train_loss_step=-1.37e+8, val_loss=-1.32e+8, train_loss_epoch=-9.84e+7]\n",
            "Epoch 8: 100%|██████████| 59/59 [01:03<00:00,  0.93it/s, v_num=0, train_loss_step=-1.81e+8, val_loss=-1.32e+8, train_loss_epoch=-9.84e+7]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:04,  1.64it/s]\u001b[A\n",
            "Epoch 10:  93%|█████████▎| 55/59 [00:48<00:03,  1.14it/s, v_num=0, train_loss_step=-2.25e+8, val_loss=-1.63e+8, train_loss_epoch=-1.59e+8]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:04,  1.48it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:02<00:03,  1.35it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:03<00:03,  1.33it/s]\u001b[A\n",
            "Epoch 10:  90%|████████▉ | 53/59 [00:46<00:05,  1.14it/s, v_num=0, train_loss_step=-2.13e+8, val_loss=-1.63e+8, train_loss_epoch=-1.59e+8]\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:04<00:02,  1.16it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:05<00:01,  1.06it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:06,  1.10it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Epoch 10: 100%|██████████| 59/59 [00:51<00:00,  1.14it/s, v_num=0, train_loss_step=-2.38e+8, val_loss=-1.63e+8, train_loss_epoch=-1.59e+8]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:07<00:00,  1.09it/s]\u001b[A\n",
            "Epoch 8: 100%|██████████| 59/59 [01:11<00:00,  0.83it/s, v_num=0, train_loss_step=-1.81e+8, val_loss=-1.14e+8, train_loss_epoch=-9.84e+7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-05 11:10:16,729\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/8da927e1/checkpoint_000008)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \rEpoch 8: 100%|██████████| 59/59 [01:11<00:00,  0.82it/s, v_num=0, train_loss_step=-1.81e+8, val_loss=-1.14e+8, train_loss_epoch=-1.39e+8]\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Epoch 9:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-1.81e+8, val_loss=-1.14e+8, train_loss_epoch=-1.39e+8]\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Epoch 9:   2%|▏         | 1/59 [00:00<00:56,  1.03it/s, v_num=0, train_loss_step=-1.64e+8, val_loss=-1.14e+8, train_loss_epoch=-1.39e+8]\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Epoch 9:   3%|▎         | 2/59 [00:01<00:54,  1.05it/s, v_num=0, train_loss_step=-1.04e+8, val_loss=-1.14e+8, train_loss_epoch=-1.39e+8]\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-05 11:10:20,007\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \rEpoch 9:   5%|▌         | 3/59 [00:02<00:53,  1.05it/s, v_num=0, train_loss_step=-1.04e+8, val_loss=-1.14e+8, train_loss_epoch=-1.39e+8]\rEpoch 9:   5%|▌         | 3/59 [00:02<00:53,  1.05it/s, v_num=0, train_loss_step=-1.73e+8, val_loss=-1.14e+8, train_loss_epoch=-1.39e+8]\n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:05<00:00,  1.29it/s]\u001b[A\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 9:   7%|▋         | 4/59 [00:03<00:54,  1.01it/s, v_num=0, train_loss_step=-1.39e+8, val_loss=-1.14e+8, train_loss_epoch=-1.39e+8]\n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:05<00:00,  1.42it/s]\u001b[A\n",
            "Epoch 10: 100%|██████████| 59/59 [00:57<00:00,  1.02it/s, v_num=0, train_loss_step=-2.38e+8, val_loss=-1.57e+8, train_loss_epoch=-1.59e+8]\n",
            "Epoch 10: 100%|██████████| 59/59 [00:58<00:00,  1.01it/s, v_num=0, train_loss_step=-2.38e+8, val_loss=-1.57e+8, train_loss_epoch=-1.97e+8]\n",
            "Epoch 11:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-2.38e+8, val_loss=-1.57e+8, train_loss_epoch=-1.97e+8]\n",
            "Epoch 11:   7%|▋         | 4/59 [00:03<00:44,  1.24it/s, v_num=0, train_loss_step=-2.41e+8, val_loss=-1.57e+8, train_loss_epoch=-1.97e+8]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 11:  17%|█▋        | 10/59 [00:09<00:44,  1.11it/s, v_num=0, train_loss_step=-2.85e+8, val_loss=-1.57e+8, train_loss_epoch=-1.97e+8]\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "Epoch 11:  19%|█▊        | 11/59 [00:10<00:44,  1.07it/s, v_num=0, train_loss_step=-2.85e+8, val_loss=-1.57e+8, train_loss_epoch=-1.97e+8]\n",
            "Epoch 11:  19%|█▊        | 11/59 [00:10<00:44,  1.07it/s, v_num=0, train_loss_step=-2.01e+8, val_loss=-1.57e+8, train_loss_epoch=-1.97e+8]\n",
            "Epoch 9:  27%|██▋       | 16/59 [00:17<00:46,  0.93it/s, v_num=0, train_loss_step=-1.63e+8, val_loss=-1.14e+8, train_loss_epoch=-1.39e+8]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:10:36. Total running time: 12min 1s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: c410a352 with val_loss=-157094368.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2200, 'ffn_num_layers': 2, 'message_hidden_dim': 400}}\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step      val/f1       val_loss |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500        9            689.463   -1.38631e+08        -1.80824e+08   0.0377358   -1.13529e+08 |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400       11            675.294   -1.96999e+08        -2.38372e+08   0           -1.57094e+08 |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000                                                                                           |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 11:  37%|███▋      | 22/59 [00:19<00:32,  1.13it/s, v_num=0, train_loss_step=-2.31e+8, val_loss=-1.57e+8, train_loss_epoch=-1.97e+8]\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "Epoch 9:  44%|████▍     | 26/59 [00:28<00:35,  0.92it/s, v_num=0, train_loss_step=-1.93e+8, val_loss=-1.14e+8, train_loss_epoch=-1.39e+8]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "Epoch 9:  53%|█████▎    | 31/59 [00:33<00:30,  0.93it/s, v_num=0, train_loss_step=-1.91e+8, val_loss=-1.14e+8, train_loss_epoch=-1.39e+8]\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "Epoch 9:  63%|██████▎   | 37/59 [00:39<00:23,  0.95it/s, v_num=0, train_loss_step=-2.01e+8, val_loss=-1.14e+8, train_loss_epoch=-1.39e+8]\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
            "Epoch 11:  75%|███████▍  | 44/59 [00:40<00:13,  1.07it/s, v_num=0, train_loss_step=-1.57e+8, val_loss=-1.57e+8, train_loss_epoch=-1.97e+8]\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "Epoch 9:  78%|███████▊  | 46/59 [00:49<00:13,  0.93it/s, v_num=0, train_loss_step=-2.36e+8, val_loss=-1.14e+8, train_loss_epoch=-1.39e+8]\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:11:06. Total running time: 12min 31s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: c410a352 with val_loss=-157094368.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2200, 'ffn_num_layers': 2, 'message_hidden_dim': 400}}\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step      val/f1       val_loss |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500        9            689.463   -1.38631e+08        -1.80824e+08   0.0377358   -1.13529e+08 |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400       11            675.294   -1.96999e+08        -2.38372e+08   0           -1.57094e+08 |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000                                                                                           |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 9:  81%|████████▏ | 48/59 [00:51<00:11,  0.94it/s, v_num=0, train_loss_step=-2.09e+8, val_loss=-1.14e+8, train_loss_epoch=-1.39e+8]\n",
            "Epoch 9:  81%|████████▏ | 48/59 [00:51<00:11,  0.94it/s, v_num=0, train_loss_step=-1.88e+8, val_loss=-1.14e+8, train_loss_epoch=-1.39e+8]\n",
            "Epoch 11:  92%|█████████▏| 54/59 [00:49<00:04,  1.10it/s, v_num=0, train_loss_step=-2.56e+8, val_loss=-1.57e+8, train_loss_epoch=-1.97e+8]\n",
            "Epoch 11:  93%|█████████▎| 55/59 [00:49<00:03,  1.10it/s, v_num=0, train_loss_step=-2.84e+8, val_loss=-1.57e+8, train_loss_epoch=-1.97e+8]\n",
            "Epoch 11:  95%|█████████▍| 56/59 [00:50<00:02,  1.10it/s, v_num=0, train_loss_step=-2.58e+8, val_loss=-1.57e+8, train_loss_epoch=-1.97e+8]\n",
            "Epoch 11:  97%|█████████▋| 57/59 [00:51<00:01,  1.10it/s, v_num=0, train_loss_step=-2.92e+8, val_loss=-1.57e+8, train_loss_epoch=-1.97e+8]\n",
            "Epoch 9:  86%|████████▋ | 51/59 [00:54<00:08,  0.94it/s, v_num=0, train_loss_step=-1.62e+8, val_loss=-1.14e+8, train_loss_epoch=-1.39e+8]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 11:  98%|█████████▊| 58/59 [00:53<00:00,  1.09it/s, v_num=0, train_loss_step=-2.72e+8, val_loss=-1.57e+8, train_loss_epoch=-1.97e+8]\n",
            "Epoch 11: 100%|██████████| 59/59 [00:54<00:00,  1.09it/s, v_num=0, train_loss_step=-2.97e+8, val_loss=-1.57e+8, train_loss_epoch=-1.97e+8]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:05,  1.29it/s]\u001b[A\n",
            "Epoch 9:  92%|█████████▏| 54/59 [00:58<00:05,  0.92it/s, v_num=0, train_loss_step=-2.39e+8, val_loss=-1.14e+8, train_loss_epoch=-1.39e+8]\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:05,  1.17it/s]\u001b[A\n",
            "Epoch 9:  93%|█████████▎| 55/59 [01:00<00:04,  0.92it/s, v_num=0, train_loss_step=-1.94e+8, val_loss=-1.14e+8, train_loss_epoch=-1.39e+8]\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:02<00:04,  1.25it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:03<00:03,  1.32it/s]\u001b[A\n",
            "Epoch 9:  90%|████████▉ | 53/59 [00:57<00:06,  0.93it/s, v_num=0, train_loss_step=-2.02e+8, val_loss=-1.14e+8, train_loss_epoch=-1.39e+8]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "Epoch 9:  95%|█████████▍| 56/59 [01:01<00:03,  0.92it/s, v_num=0, train_loss_step=-1.66e+8, val_loss=-1.14e+8, train_loss_epoch=-1.39e+8]\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:03<00:02,  1.37it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:04<00:01,  1.41it/s]\u001b[A\n",
            "Epoch 9:  97%|█████████▋| 57/59 [01:02<00:02,  0.92it/s, v_num=0, train_loss_step=-1.85e+8, val_loss=-1.14e+8, train_loss_epoch=-1.39e+8]\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:04<00:00,  1.43it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:05<00:00,  1.57it/s]\u001b[A\n",
            "Epoch 11: 100%|██████████| 59/59 [00:59<00:00,  0.99it/s, v_num=0, train_loss_step=-2.97e+8, val_loss=-1.95e+8, train_loss_epoch=-1.97e+8]\n",
            "Epoch 9:  98%|█████████▊| 58/59 [01:02<00:01,  0.92it/s, v_num=0, train_loss_step=-2.07e+8, val_loss=-1.14e+8, train_loss_epoch=-1.39e+8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/c410a352/checkpoint_000011)\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11: 100%|██████████| 59/59 [01:00<00:00,  0.98it/s, v_num=0, train_loss_step=-2.97e+8, val_loss=-1.95e+8, train_loss_epoch=-2.41e+8]\n",
            "Epoch 9: 100%|██████████| 59/59 [01:03<00:00,  0.93it/s, v_num=0, train_loss_step=-1.9e+8, val_loss=-1.14e+8, train_loss_epoch=-1.39e+8] \n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 12:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-2.97e+8, val_loss=-1.95e+8, train_loss_epoch=-2.41e+8]\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:04,  1.62it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:04,  1.46it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:02<00:03,  1.40it/s]\u001b[A\n",
            "Epoch 12:   3%|▎         | 2/59 [00:01<00:44,  1.27it/s, v_num=0, train_loss_step=-2.85e+8, val_loss=-1.95e+8, train_loss_epoch=-2.41e+8]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:02<00:02,  1.39it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:03<00:02,  1.39it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:04<00:01,  1.38it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:05<00:00,  1.37it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:05<00:00,  1.49it/s]\u001b[A\n",
            "Epoch 9: 100%|██████████| 59/59 [01:09<00:00,  0.85it/s, v_num=0, train_loss_step=-1.9e+8, val_loss=-1.84e+8, train_loss_epoch=-1.39e+8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/8da927e1/checkpoint_000009)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: 100%|██████████| 59/59 [01:09<00:00,  0.85it/s, v_num=0, train_loss_step=-1.9e+8, val_loss=-1.84e+8, train_loss_epoch=-1.81e+8]\n",
            "Epoch 10:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-1.9e+8, val_loss=-1.84e+8, train_loss_epoch=-1.81e+8]\n",
            "Epoch 10:   2%|▏         | 1/59 [00:01<01:34,  0.61it/s, v_num=0, train_loss_step=-1.84e+8, val_loss=-1.84e+8, train_loss_epoch=-1.81e+8]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 12:  15%|█▌        | 9/59 [00:08<00:46,  1.08it/s, v_num=0, train_loss_step=-2.63e+8, val_loss=-1.95e+8, train_loss_epoch=-2.41e+8]\n",
            "Epoch 12:  15%|█▌        | 9/59 [00:08<00:46,  1.08it/s, v_num=0, train_loss_step=-2.99e+8, val_loss=-1.95e+8, train_loss_epoch=-2.41e+8]\n",
            "Epoch 12:  24%|██▎       | 14/59 [00:13<00:43,  1.04it/s, v_num=0, train_loss_step=-2.6e+8, val_loss=-1.95e+8, train_loss_epoch=-2.41e+8] \u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:11:36. Total running time: 13min 1s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: c410a352 with val_loss=-195252192.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2200, 'ffn_num_layers': 2, 'message_hidden_dim': 400}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1       val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500       10            759.269   -1.81376e+08        -1.90483e+08   0.295302   -1.83775e+08 |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400       12            735.62    -2.40508e+08        -2.96841e+08   0.230216   -1.95252e+08 |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000                                                                                          |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 10:  19%|█▊        | 11/59 [00:12<00:53,  0.90it/s, v_num=0, train_loss_step=-2.23e+8, val_loss=-1.84e+8, train_loss_epoch=-1.81e+8]\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "Epoch 10:  25%|██▌       | 15/59 [00:17<00:50,  0.87it/s, v_num=0, train_loss_step=-2.47e+8, val_loss=-1.84e+8, train_loss_epoch=-1.81e+8]\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "Epoch 12:  51%|█████     | 30/59 [00:28<00:27,  1.05it/s, v_num=0, train_loss_step=-3.02e+8, val_loss=-1.95e+8, train_loss_epoch=-2.41e+8]\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "Epoch 10:  42%|████▏     | 25/59 [00:27<00:37,  0.91it/s, v_num=0, train_loss_step=-2.65e+8, val_loss=-1.84e+8, train_loss_epoch=-1.81e+8]\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "Epoch 12:  69%|██████▉   | 41/59 [00:39<00:17,  1.04it/s, v_num=0, train_loss_step=-3.33e+8, val_loss=-1.95e+8, train_loss_epoch=-2.41e+8]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "Epoch 10:  59%|█████▉    | 35/59 [00:38<00:26,  0.91it/s, v_num=0, train_loss_step=-2.09e+8, val_loss=-1.84e+8, train_loss_epoch=-1.81e+8]\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:12:06. Total running time: 13min 31s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: c410a352 with val_loss=-195252192.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2200, 'ffn_num_layers': 2, 'message_hidden_dim': 400}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1       val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500       10            759.269   -1.81376e+08        -1.90483e+08   0.295302   -1.83775e+08 |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400       12            735.62    -2.40508e+08        -2.96841e+08   0.230216   -1.95252e+08 |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000                                                                                          |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 10:  69%|██████▉   | 41/59 [00:44<00:19,  0.93it/s, v_num=0, train_loss_step=-2.34e+8, val_loss=-1.84e+8, train_loss_epoch=-1.81e+8]\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
            "Epoch 12:  92%|█████████▏| 54/59 [00:50<00:04,  1.06it/s, v_num=0, train_loss_step=-3.03e+8, val_loss=-1.95e+8, train_loss_epoch=-2.41e+8]\n",
            "Epoch 12:  93%|█████████▎| 55/59 [00:52<00:03,  1.06it/s, v_num=0, train_loss_step=-2.68e+8, val_loss=-1.95e+8, train_loss_epoch=-2.41e+8]\n",
            "Epoch 12:  95%|█████████▍| 56/59 [00:53<00:02,  1.05it/s, v_num=0, train_loss_step=-2.65e+8, val_loss=-1.95e+8, train_loss_epoch=-2.41e+8]\n",
            "Epoch 12:  97%|█████████▋| 57/59 [00:54<00:01,  1.04it/s, v_num=0, train_loss_step=-3.02e+8, val_loss=-1.95e+8, train_loss_epoch=-2.41e+8]\n",
            "Epoch 12:  98%|█████████▊| 58/59 [00:56<00:00,  1.03it/s, v_num=0, train_loss_step=-2.74e+8, val_loss=-1.95e+8, train_loss_epoch=-2.41e+8]\n",
            "Epoch 10:  75%|███████▍  | 44/59 [00:48<00:16,  0.91it/s, v_num=0, train_loss_step=-2.24e+8, val_loss=-1.84e+8, train_loss_epoch=-1.81e+8]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "Epoch 12: 100%|██████████| 59/59 [00:56<00:00,  1.04it/s, v_num=0, train_loss_step=-3.56e+8, val_loss=-1.95e+8, train_loss_epoch=-2.41e+8]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:03,  2.10it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:03,  1.82it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:01<00:02,  1.71it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:02<00:02,  1.69it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:02<00:01,  1.69it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:03<00:01,  1.69it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:04<00:00,  1.67it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:04<00:00,  1.83it/s]\u001b[A\n",
            "Epoch 12: 100%|██████████| 59/59 [01:01<00:00,  0.97it/s, v_num=0, train_loss_step=-3.56e+8, val_loss=-2.65e+8, train_loss_epoch=-2.41e+8]\n",
            "Epoch 10:  85%|████████▍ | 50/59 [00:54<00:09,  0.92it/s, v_num=0, train_loss_step=-2.01e+8, val_loss=-1.84e+8, train_loss_epoch=-1.81e+8]\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/c410a352/checkpoint_000012)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12: 100%|██████████| 59/59 [01:01<00:00,  0.96it/s, v_num=0, train_loss_step=-3.56e+8, val_loss=-2.65e+8, train_loss_epoch=-2.76e+8]\n",
            "Epoch 13:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-3.56e+8, val_loss=-2.65e+8, train_loss_epoch=-2.76e+8]\n",
            "Epoch 10:  92%|█████████▏| 54/59 [00:58<00:05,  0.92it/s, v_num=0, train_loss_step=-2.35e+8, val_loss=-1.84e+8, train_loss_epoch=-1.81e+8]\n",
            "Epoch 10:  93%|█████████▎| 55/59 [00:59<00:04,  0.92it/s, v_num=0, train_loss_step=-2.67e+8, val_loss=-1.84e+8, train_loss_epoch=-1.81e+8]\n",
            "Epoch 13:   8%|▊         | 5/59 [00:04<00:45,  1.19it/s, v_num=0, train_loss_step=-3.32e+8, val_loss=-2.65e+8, train_loss_epoch=-2.76e+8]\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "Epoch 10:  95%|█████████▍| 56/59 [01:01<00:03,  0.92it/s, v_num=0, train_loss_step=-2.35e+8, val_loss=-1.84e+8, train_loss_epoch=-1.81e+8]\n",
            "Epoch 10:  97%|█████████▋| 57/59 [01:02<00:02,  0.91it/s, v_num=0, train_loss_step=-2.24e+8, val_loss=-1.84e+8, train_loss_epoch=-1.81e+8]\n",
            "Epoch 10:  98%|█████████▊| 58/59 [01:04<00:01,  0.90it/s, v_num=0, train_loss_step=-2.17e+8, val_loss=-1.84e+8, train_loss_epoch=-1.81e+8]\n",
            "Epoch 10: 100%|██████████| 59/59 [01:05<00:00,  0.91it/s, v_num=0, train_loss_step=-2.74e+8, val_loss=-1.84e+8, train_loss_epoch=-1.81e+8]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 13:  14%|█▎        | 8/59 [00:08<00:54,  0.94it/s, v_num=0, train_loss_step=-3.15e+8, val_loss=-2.65e+8, train_loss_epoch=-2.76e+8]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:04,  1.61it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:04,  1.48it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:02<00:03,  1.43it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:02<00:02,  1.41it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:03<00:02,  1.41it/s]\u001b[A\n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:12:36. Total running time: 14min 1s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: c410a352 with val_loss=-264852560.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2200, 'ffn_num_layers': 2, 'message_hidden_dim': 400}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1       val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500       10            759.269   -1.81376e+08        -1.90483e+08   0.295302   -1.83775e+08 |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400       13            797.622   -2.76127e+08        -3.5612e+08    0.119658   -2.64853e+08 |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000                                                                                          |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:04<00:01,  1.39it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:05<00:00,  1.39it/s]\u001b[A\n",
            "Epoch 13:  24%|██▎       | 14/59 [00:13<00:44,  1.02it/s, v_num=0, train_loss_step=-3.04e+8, val_loss=-2.65e+8, train_loss_epoch=-2.76e+8]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:05<00:00,  1.52it/s]\u001b[A\n",
            "Epoch 10: 100%|██████████| 59/59 [01:10<00:00,  0.84it/s, v_num=0, train_loss_step=-2.74e+8, val_loss=-2.09e+8, train_loss_epoch=-1.81e+8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/8da927e1/checkpoint_000010)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: 100%|██████████| 59/59 [01:11<00:00,  0.83it/s, v_num=0, train_loss_step=-2.74e+8, val_loss=-2.09e+8, train_loss_epoch=-2.25e+8]\n",
            "Epoch 11:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-2.74e+8, val_loss=-2.09e+8, train_loss_epoch=-2.25e+8]\n",
            "Epoch 11:   7%|▋         | 4/59 [00:04<00:56,  0.97it/s, v_num=0, train_loss_step=-2.71e+8, val_loss=-2.09e+8, train_loss_epoch=-2.25e+8]\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "Epoch 13:  42%|████▏     | 25/59 [00:25<00:34,  0.99it/s, v_num=0, train_loss_step=-3.26e+8, val_loss=-2.65e+8, train_loss_epoch=-2.76e+8]\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "Epoch 11:  20%|██        | 12/59 [00:13<00:53,  0.88it/s, v_num=0, train_loss_step=-2.55e+8, val_loss=-2.09e+8, train_loss_epoch=-2.25e+8]\n",
            "Epoch 11:  20%|██        | 12/59 [00:13<00:53,  0.88it/s, v_num=0, train_loss_step=-2.57e+8, val_loss=-2.09e+8, train_loss_epoch=-2.25e+8]\n",
            "Epoch 11:  22%|██▏       | 13/59 [00:14<00:51,  0.89it/s, v_num=0, train_loss_step=-2.82e+8, val_loss=-2.09e+8, train_loss_epoch=-2.25e+8]\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "Epoch 11:  31%|███       | 18/59 [00:19<00:45,  0.91it/s, v_num=0, train_loss_step=-2.8e+8, val_loss=-2.09e+8, train_loss_epoch=-2.25e+8] \u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "Epoch 11:  37%|███▋      | 22/59 [00:25<00:42,  0.87it/s, v_num=0, train_loss_step=-2.81e+8, val_loss=-2.09e+8, train_loss_epoch=-2.25e+8]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:13:06. Total running time: 14min 31s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: c410a352 with val_loss=-264852560.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2200, 'ffn_num_layers': 2, 'message_hidden_dim': 400}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1       val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500       11            830.776   -2.25227e+08        -2.73661e+08   0.171875   -2.09339e+08 |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400       13            797.622   -2.76127e+08        -3.5612e+08    0.119658   -2.64853e+08 |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000                                                                                          |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 13:  81%|████████▏ | 48/59 [00:46<00:10,  1.04it/s, v_num=0, train_loss_step=-3.54e+8, val_loss=-2.65e+8, train_loss_epoch=-2.76e+8]\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "Epoch 13:  90%|████████▉ | 53/59 [00:51<00:05,  1.04it/s, v_num=0, train_loss_step=-3.49e+8, val_loss=-2.65e+8, train_loss_epoch=-2.76e+8]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "Epoch 13:  92%|█████████▏| 54/59 [00:52<00:04,  1.03it/s, v_num=0, train_loss_step=-3.27e+8, val_loss=-2.65e+8, train_loss_epoch=-2.76e+8]\n",
            "Epoch 13:  93%|█████████▎| 55/59 [00:53<00:03,  1.02it/s, v_num=0, train_loss_step=-3.21e+8, val_loss=-2.65e+8, train_loss_epoch=-2.76e+8]\n",
            "Epoch 13:  95%|█████████▍| 56/59 [00:54<00:02,  1.02it/s, v_num=0, train_loss_step=-3.6e+8, val_loss=-2.65e+8, train_loss_epoch=-2.76e+8] \n",
            "Epoch 11:  58%|█████▊    | 34/59 [00:39<00:28,  0.87it/s, v_num=0, train_loss_step=-2.87e+8, val_loss=-2.09e+8, train_loss_epoch=-2.25e+8]\n",
            "Epoch 11:  58%|█████▊    | 34/59 [00:39<00:28,  0.87it/s, v_num=0, train_loss_step=-3.12e+8, val_loss=-2.09e+8, train_loss_epoch=-2.25e+8]\n",
            "Epoch 13:  97%|█████████▋| 57/59 [00:55<00:01,  1.02it/s, v_num=0, train_loss_step=-2.36e+8, val_loss=-2.65e+8, train_loss_epoch=-2.76e+8]\n",
            "Epoch 13:  98%|█████████▊| 58/59 [00:56<00:00,  1.02it/s, v_num=0, train_loss_step=-3.19e+8, val_loss=-2.65e+8, train_loss_epoch=-2.76e+8]\n",
            "Epoch 11:  59%|█████▉    | 35/59 [00:40<00:27,  0.87it/s, v_num=0, train_loss_step=-2.12e+8, val_loss=-2.09e+8, train_loss_epoch=-2.25e+8]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "Epoch 13: 100%|██████████| 59/59 [00:57<00:00,  1.03it/s, v_num=0, train_loss_step=-3.67e+8, val_loss=-2.65e+8, train_loss_epoch=-2.76e+8]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:03,  1.95it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:03,  1.86it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:01<00:02,  1.76it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:02<00:02,  1.73it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:02<00:01,  1.73it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:03<00:01,  1.69it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:04<00:00,  1.69it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:04<00:00,  1.85it/s]\u001b[A\n",
            "Epoch 13: 100%|██████████| 59/59 [01:01<00:00,  0.96it/s, v_num=0, train_loss_step=-3.67e+8, val_loss=-2.96e+8, train_loss_epoch=-2.76e+8]\n",
            "Epoch 11:  69%|██████▉   | 41/59 [00:46<00:20,  0.89it/s, v_num=0, train_loss_step=-2.77e+8, val_loss=-2.09e+8, train_loss_epoch=-2.25e+8]\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/c410a352/checkpoint_000013)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13: 100%|██████████| 59/59 [01:02<00:00,  0.95it/s, v_num=0, train_loss_step=-3.67e+8, val_loss=-2.96e+8, train_loss_epoch=-3.2e+8] \n",
            "Epoch 14:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-3.67e+8, val_loss=-2.96e+8, train_loss_epoch=-3.2e+8]\n",
            "Epoch 11:  76%|███████▋  | 45/59 [00:51<00:15,  0.88it/s, v_num=0, train_loss_step=-3.12e+8, val_loss=-2.09e+8, train_loss_epoch=-2.25e+8]\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "Epoch 14:  17%|█▋        | 10/59 [00:10<00:50,  0.97it/s, v_num=0, train_loss_step=-2.62e+8, val_loss=-2.96e+8, train_loss_epoch=-3.2e+8]\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:13:36. Total running time: 15min 1s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: c410a352 with val_loss=-296162112.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2200, 'ffn_num_layers': 2, 'message_hidden_dim': 400}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1       val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500       11            830.776   -2.25227e+08        -2.73661e+08   0.171875   -2.09339e+08 |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400       14            859.997   -3.20092e+08        -3.66793e+08   0.234375   -2.96162e+08 |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000                                                                                          |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 11:  92%|█████████▏| 54/59 [01:00<00:05,  0.89it/s, v_num=0, train_loss_step=-3.09e+8, val_loss=-2.09e+8, train_loss_epoch=-2.25e+8]\n",
            "Epoch 11:  93%|█████████▎| 55/59 [01:01<00:04,  0.89it/s, v_num=0, train_loss_step=-3.11e+8, val_loss=-2.09e+8, train_loss_epoch=-2.25e+8]\n",
            "Epoch 14:  29%|██▉       | 17/59 [00:16<00:39,  1.06it/s, v_num=0, train_loss_step=-3.56e+8, val_loss=-2.96e+8, train_loss_epoch=-3.2e+8]\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "Epoch 11:  95%|█████████▍| 56/59 [01:02<00:03,  0.89it/s, v_num=0, train_loss_step=-2.84e+8, val_loss=-2.09e+8, train_loss_epoch=-2.25e+8]\n",
            "Epoch 11:  97%|█████████▋| 57/59 [01:03<00:02,  0.89it/s, v_num=0, train_loss_step=-3.35e+8, val_loss=-2.09e+8, train_loss_epoch=-2.25e+8]\n",
            "Epoch 11:  98%|█████████▊| 58/59 [01:05<00:01,  0.88it/s, v_num=0, train_loss_step=-3.01e+8, val_loss=-2.09e+8, train_loss_epoch=-2.25e+8]\n",
            "Epoch 11: 100%|██████████| 59/59 [01:06<00:00,  0.89it/s, v_num=0, train_loss_step=-3.38e+8, val_loss=-2.09e+8, train_loss_epoch=-2.25e+8]\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:06,  1.10it/s]\u001b[A\n",
            "Epoch 14:  36%|███▌      | 21/59 [00:21<00:38,  0.98it/s, v_num=0, train_loss_step=-3.67e+8, val_loss=-2.96e+8, train_loss_epoch=-3.2e+8]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:02<00:06,  0.95it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:02<00:04,  1.05it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:03<00:03,  1.11it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:04<00:02,  1.15it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:05<00:01,  1.17it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:05<00:00,  1.20it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:06<00:00,  1.30it/s]\u001b[A\n",
            "Epoch 11: 100%|██████████| 59/59 [01:12<00:00,  0.81it/s, v_num=0, train_loss_step=-3.38e+8, val_loss=-2.35e+8, train_loss_epoch=-2.25e+8]\n",
            "Epoch 14:  46%|████▌     | 27/59 [00:26<00:31,  1.02it/s, v_num=0, train_loss_step=-3.71e+8, val_loss=-2.96e+8, train_loss_epoch=-3.2e+8]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 11: 100%|██████████| 59/59 [01:13<00:00,  0.81it/s, v_num=0, train_loss_step=-3.38e+8, val_loss=-2.35e+8, train_loss_epoch=-2.76e+8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/8da927e1/checkpoint_000011)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-3.38e+8, val_loss=-2.35e+8, train_loss_epoch=-2.76e+8]\n",
            "Epoch 12:   8%|▊         | 5/59 [00:04<00:50,  1.07it/s, v_num=0, train_loss_step=-2.23e+8, val_loss=-2.35e+8, train_loss_epoch=-2.76e+8]\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "Epoch 12:  15%|█▌        | 9/59 [00:10<00:58,  0.85it/s, v_num=0, train_loss_step=-3.3e+8, val_loss=-2.35e+8, train_loss_epoch=-2.76e+8] \u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:14:06. Total running time: 15min 31s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: c410a352 with val_loss=-296162112.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2200, 'ffn_num_layers': 2, 'message_hidden_dim': 400}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1       val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500       12            904.395   -2.75838e+08        -3.37647e+08   0.5        -2.34733e+08 |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400       14            859.997   -3.20092e+08        -3.66793e+08   0.234375   -2.96162e+08 |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000                                                                                          |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 14:  76%|███████▋  | 45/59 [00:43<00:13,  1.04it/s, v_num=0, train_loss_step=-3.98e+8, val_loss=-2.96e+8, train_loss_epoch=-3.2e+8]\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
            "Epoch 14:  86%|████████▋ | 51/59 [00:48<00:07,  1.05it/s, v_num=0, train_loss_step=-3.93e+8, val_loss=-2.96e+8, train_loss_epoch=-3.2e+8]\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "Epoch 14:  92%|█████████▏| 54/59 [00:52<00:04,  1.03it/s, v_num=0, train_loss_step=-3.87e+8, val_loss=-2.96e+8, train_loss_epoch=-3.2e+8]\n",
            "Epoch 14:  93%|█████████▎| 55/59 [00:53<00:03,  1.03it/s, v_num=0, train_loss_step=-3.85e+8, val_loss=-2.96e+8, train_loss_epoch=-3.2e+8]\n",
            "Epoch 12:  41%|████      | 24/59 [00:27<00:39,  0.88it/s, v_num=0, train_loss_step=-3.49e+8, val_loss=-2.35e+8, train_loss_epoch=-2.76e+8]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 14:  95%|█████████▍| 56/59 [00:54<00:02,  1.03it/s, v_num=0, train_loss_step=-4.3e+8, val_loss=-2.96e+8, train_loss_epoch=-3.2e+8] \n",
            "Epoch 14:  97%|█████████▋| 57/59 [00:55<00:01,  1.04it/s, v_num=0, train_loss_step=-3.72e+8, val_loss=-2.96e+8, train_loss_epoch=-3.2e+8]\n",
            "Epoch 14:  98%|█████████▊| 58/59 [00:55<00:00,  1.04it/s, v_num=0, train_loss_step=-3.91e+8, val_loss=-2.96e+8, train_loss_epoch=-3.2e+8]\n",
            "Epoch 14: 100%|██████████| 59/59 [00:56<00:00,  1.05it/s, v_num=0, train_loss_step=-1.89e+8, val_loss=-2.96e+8, train_loss_epoch=-3.2e+8]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:03,  2.11it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:03,  1.84it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:01<00:02,  1.75it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:02<00:02,  1.72it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:02<00:01,  1.72it/s]\u001b[A\n",
            "Epoch 12:  49%|████▉     | 29/59 [00:32<00:33,  0.91it/s, v_num=0, train_loss_step=-3.33e+8, val_loss=-2.35e+8, train_loss_epoch=-2.76e+8]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:03<00:01,  1.71it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:04<00:00,  1.70it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:04<00:00,  1.86it/s]\u001b[A\n",
            "Epoch 14: 100%|██████████| 59/59 [01:00<00:00,  0.97it/s, v_num=0, train_loss_step=-1.89e+8, val_loss=-3.49e+8, train_loss_epoch=-3.2e+8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-05 11:14:26,358\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/c410a352/checkpoint_000014)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14: 100%|██████████| 59/59 [01:01<00:00,  0.96it/s, v_num=0, train_loss_step=-1.89e+8, val_loss=-3.49e+8, train_loss_epoch=-3.53e+8]\n",
            "Epoch 15:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-1.89e+8, val_loss=-3.49e+8, train_loss_epoch=-3.53e+8]\n",
            "Epoch 15:   5%|▌         | 3/59 [00:03<01:12,  0.77it/s, v_num=0, train_loss_step=-4.27e+8, val_loss=-3.49e+8, train_loss_epoch=-3.53e+8]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 15:  15%|█▌        | 9/59 [00:09<00:51,  0.97it/s, v_num=0, train_loss_step=-3.81e+8, val_loss=-3.49e+8, train_loss_epoch=-3.53e+8]\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:14:36. Total running time: 16min 1s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: c410a352 with val_loss=-349210400.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2200, 'ffn_num_layers': 2, 'message_hidden_dim': 400}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1       val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500       12            904.395   -2.75838e+08        -3.37647e+08   0.5        -2.34733e+08 |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400       15            921.466   -3.52502e+08        -1.88902e+08   0.346154   -3.4921e+08  |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000                                                                                          |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 12:  75%|███████▍  | 44/59 [00:49<00:16,  0.90it/s, v_num=0, train_loss_step=-2.92e+8, val_loss=-2.35e+8, train_loss_epoch=-2.76e+8]\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "Epoch 15:  34%|███▍      | 20/59 [00:19<00:38,  1.00it/s, v_num=0, train_loss_step=-3.1e+8, val_loss=-3.49e+8, train_loss_epoch=-3.53e+8] \u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "Epoch 15:  44%|████▍     | 26/59 [00:25<00:31,  1.04it/s, v_num=0, train_loss_step=-4.06e+8, val_loss=-3.49e+8, train_loss_epoch=-3.53e+8]\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "Epoch 12:  92%|█████████▏| 54/59 [01:01<00:05,  0.89it/s, v_num=0, train_loss_step=-3.27e+8, val_loss=-2.35e+8, train_loss_epoch=-2.76e+8]\n",
            "Epoch 12:  93%|█████████▎| 55/59 [01:01<00:04,  0.89it/s, v_num=0, train_loss_step=-3.09e+8, val_loss=-2.35e+8, train_loss_epoch=-2.76e+8]\n",
            "Epoch 12:  95%|█████████▍| 56/59 [01:02<00:03,  0.89it/s, v_num=0, train_loss_step=-3.09e+8, val_loss=-2.35e+8, train_loss_epoch=-2.76e+8]\n",
            "Epoch 12:  97%|█████████▋| 57/59 [01:03<00:02,  0.89it/s, v_num=0, train_loss_step=-3.52e+8, val_loss=-2.35e+8, train_loss_epoch=-2.76e+8]\n",
            "Epoch 12:  98%|█████████▊| 58/59 [01:04<00:01,  0.89it/s, v_num=0, train_loss_step=-3.28e+8, val_loss=-2.35e+8, train_loss_epoch=-2.76e+8]\n",
            "Epoch 15:  54%|█████▍    | 32/59 [00:29<00:25,  1.07it/s, v_num=0, train_loss_step=-3.46e+8, val_loss=-3.49e+8, train_loss_epoch=-3.53e+8]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 15:  56%|█████▌    | 33/59 [00:30<00:24,  1.07it/s, v_num=0, train_loss_step=-3.46e+8, val_loss=-3.49e+8, train_loss_epoch=-3.53e+8]\n",
            "Epoch 15:  56%|█████▌    | 33/59 [00:30<00:24,  1.07it/s, v_num=0, train_loss_step=-4.23e+8, val_loss=-3.49e+8, train_loss_epoch=-3.53e+8]\n",
            "Epoch 12: 100%|██████████| 59/59 [01:05<00:00,  0.90it/s, v_num=0, train_loss_step=-3.84e+8, val_loss=-2.35e+8, train_loss_epoch=-2.76e+8]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:01<00:07,  0.99it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:02<00:07,  0.82it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:03<00:06,  0.81it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:04<00:04,  0.85it/s]\u001b[A\n",
            "Epoch 15:  61%|██████    | 36/59 [00:34<00:22,  1.03it/s, v_num=0, train_loss_step=-3.68e+8, val_loss=-3.49e+8, train_loss_epoch=-3.53e+8]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:05<00:03,  0.92it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:06<00:02,  0.96it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:06<00:00,  1.00it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:07<00:00,  1.11it/s]\u001b[A\n",
            "Epoch 12: 100%|██████████| 59/59 [01:13<00:00,  0.81it/s, v_num=0, train_loss_step=-3.84e+8, val_loss=-3.03e+8, train_loss_epoch=-2.76e+8]\n",
            "Epoch 12: 100%|██████████| 59/59 [01:13<00:00,  0.80it/s, v_num=0, train_loss_step=-3.84e+8, val_loss=-3.03e+8, train_loss_epoch=-3.17e+8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/8da927e1/checkpoint_000012)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \rEpoch 12:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-3.84e+8, val_loss=-3.03e+8, train_loss_epoch=-3.17e+8]         \rEpoch 13:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-3.84e+8, val_loss=-3.03e+8, train_loss_epoch=-3.17e+8]\n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:15:06. Total running time: 16min 31s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: c410a352 with val_loss=-349210400.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2200, 'ffn_num_layers': 2, 'message_hidden_dim': 400}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1       val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500       13            978.277   -3.17298e+08        -3.8392e+08    0.296774   -3.03124e+08 |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400       15            921.466   -3.52502e+08        -1.88902e+08   0.346154   -3.4921e+08  |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000                                                                                          |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 13:   3%|▎         | 2/59 [00:02<00:58,  0.98it/s, v_num=0, train_loss_step=-3.7e+8, val_loss=-3.03e+8, train_loss_epoch=-3.17e+8] \u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "Epoch 13:  12%|█▏        | 7/59 [00:07<00:53,  0.97it/s, v_num=0, train_loss_step=-3.87e+8, val_loss=-3.03e+8, train_loss_epoch=-3.17e+8]\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "Epoch 15:  92%|█████████▏| 54/59 [00:51<00:04,  1.04it/s, v_num=0, train_loss_step=-3.94e+8, val_loss=-3.49e+8, train_loss_epoch=-3.53e+8]\n",
            "Epoch 13:  17%|█▋        | 10/59 [00:11<00:58,  0.83it/s, v_num=0, train_loss_step=-3.87e+8, val_loss=-3.03e+8, train_loss_epoch=-3.17e+8]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 15:  93%|█████████▎| 55/59 [00:52<00:03,  1.05it/s, v_num=0, train_loss_step=-3.83e+8, val_loss=-3.49e+8, train_loss_epoch=-3.53e+8]\n",
            "Epoch 15:  95%|█████████▍| 56/59 [00:53<00:02,  1.05it/s, v_num=0, train_loss_step=-4.36e+8, val_loss=-3.49e+8, train_loss_epoch=-3.53e+8]\n",
            "Epoch 15:  97%|█████████▋| 57/59 [00:54<00:01,  1.05it/s, v_num=0, train_loss_step=-4.1e+8, val_loss=-3.49e+8, train_loss_epoch=-3.53e+8] \n",
            "Epoch 15:  98%|█████████▊| 58/59 [00:55<00:00,  1.05it/s, v_num=0, train_loss_step=-4.26e+8, val_loss=-3.49e+8, train_loss_epoch=-3.53e+8]\n",
            "Epoch 15: 100%|██████████| 59/59 [00:55<00:00,  1.06it/s, v_num=0, train_loss_step=-4.26e+8, val_loss=-3.49e+8, train_loss_epoch=-3.53e+8]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:03,  2.07it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:03,  1.86it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:01<00:02,  1.77it/s]\u001b[A\n",
            "Epoch 13:  25%|██▌       | 15/59 [00:17<00:49,  0.88it/s, v_num=0, train_loss_step=-3.33e+8, val_loss=-3.03e+8, train_loss_epoch=-3.17e+8]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:02<00:02,  1.73it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:02<00:01,  1.74it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:03<00:01,  1.71it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:04<00:00,  1.69it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:04<00:00,  1.84it/s]\u001b[A\n",
            "Epoch 15: 100%|██████████| 59/59 [00:59<00:00,  0.98it/s, v_num=0, train_loss_step=-4.26e+8, val_loss=-3.64e+8, train_loss_epoch=-3.53e+8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/c410a352/checkpoint_000015)\n",
            "2025-05-05 11:15:27,206\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15: 100%|██████████| 59/59 [01:00<00:00,  0.98it/s, v_num=0, train_loss_step=-4.26e+8, val_loss=-3.64e+8, train_loss_epoch=-3.84e+8]\n",
            "Epoch 16:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-4.26e+8, val_loss=-3.64e+8, train_loss_epoch=-3.84e+8]\n",
            "Epoch 16:   3%|▎         | 2/59 [00:02<01:12,  0.79it/s, v_num=0, train_loss_step=-4.59e+8, val_loss=-3.64e+8, train_loss_epoch=-3.84e+8]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 16:  12%|█▏        | 7/59 [00:07<00:56,  0.92it/s, v_num=0, train_loss_step=-4.28e+8, val_loss=-3.64e+8, train_loss_epoch=-3.84e+8]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:15:36. Total running time: 17min 1s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: c410a352 with val_loss=-363901408.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2200, 'ffn_num_layers': 2, 'message_hidden_dim': 400}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1       val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500       13            978.277   -3.17298e+08        -3.8392e+08    0.296774   -3.03124e+08 |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400       16            982.287   -3.84162e+08        -4.26383e+08   0.325843   -3.63901e+08 |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000                                                                                          |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 16:  24%|██▎       | 14/59 [00:13<00:42,  1.06it/s, v_num=0, train_loss_step=-4.25e+8, val_loss=-3.64e+8, train_loss_epoch=-3.84e+8]\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
            "Epoch 16:  32%|███▏      | 19/59 [00:18<00:39,  1.01it/s, v_num=0, train_loss_step=-3.35e+8, val_loss=-3.64e+8, train_loss_epoch=-3.84e+8]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "Epoch 16:  42%|████▏     | 25/59 [00:24<00:32,  1.03it/s, v_num=0, train_loss_step=-4.47e+8, val_loss=-3.64e+8, train_loss_epoch=-3.84e+8]\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "Epoch 13:  76%|███████▋  | 45/59 [00:51<00:15,  0.88it/s, v_num=0, train_loss_step=-3.18e+8, val_loss=-3.03e+8, train_loss_epoch=-3.17e+8]\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "Epoch 16:  61%|██████    | 36/59 [00:34<00:22,  1.04it/s, v_num=0, train_loss_step=-4.11e+8, val_loss=-3.64e+8, train_loss_epoch=-3.84e+8]\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:16:06. Total running time: 17min 31s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: c410a352 with val_loss=-363901408.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2200, 'ffn_num_layers': 2, 'message_hidden_dim': 400}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1       val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500       13            978.277   -3.17298e+08        -3.8392e+08    0.296774   -3.03124e+08 |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400       16            982.287   -3.84162e+08        -4.26383e+08   0.325843   -3.63901e+08 |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000                                                                                          |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 16:  71%|███████   | 42/59 [00:39<00:16,  1.06it/s, v_num=0, train_loss_step=-4.53e+8, val_loss=-3.64e+8, train_loss_epoch=-3.84e+8]\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "Epoch 13:  92%|█████████▏| 54/59 [01:02<00:05,  0.87it/s, v_num=0, train_loss_step=-3.65e+8, val_loss=-3.03e+8, train_loss_epoch=-3.17e+8]\n",
            "Epoch 13:  93%|█████████▎| 55/59 [01:03<00:04,  0.87it/s, v_num=0, train_loss_step=-3.55e+8, val_loss=-3.03e+8, train_loss_epoch=-3.17e+8]\n",
            "Epoch 13:  95%|█████████▍| 56/59 [01:04<00:03,  0.87it/s, v_num=0, train_loss_step=-3.92e+8, val_loss=-3.03e+8, train_loss_epoch=-3.17e+8]\n",
            "Epoch 13:  97%|█████████▋| 57/59 [01:05<00:02,  0.88it/s, v_num=0, train_loss_step=-2.64e+8, val_loss=-3.03e+8, train_loss_epoch=-3.17e+8]\n",
            "Epoch 13:  98%|█████████▊| 58/59 [01:06<00:01,  0.88it/s, v_num=0, train_loss_step=-3.53e+8, val_loss=-3.03e+8, train_loss_epoch=-3.17e+8]\n",
            "Epoch 13: 100%|██████████| 59/59 [01:06<00:00,  0.89it/s, v_num=0, train_loss_step=-4.11e+8, val_loss=-3.03e+8, train_loss_epoch=-3.17e+8]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 16:  81%|████████▏ | 48/59 [00:44<00:10,  1.08it/s, v_num=0, train_loss_step=-4.41e+8, val_loss=-3.64e+8, train_loss_epoch=-3.84e+8]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:04,  1.40it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:05,  1.00it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:03<00:05,  0.90it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:04<00:04,  0.87it/s]\u001b[A\n",
            "Epoch 16:  90%|████████▉ | 53/59 [00:50<00:05,  1.05it/s, v_num=0, train_loss_step=-3.63e+8, val_loss=-3.64e+8, train_loss_epoch=-3.84e+8]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:05<00:03,  0.87it/s]\u001b[A\n",
            "Epoch 16:  92%|█████████▏| 54/59 [00:51<00:04,  1.05it/s, v_num=0, train_loss_step=-2.97e+8, val_loss=-3.64e+8, train_loss_epoch=-3.84e+8]\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:06<00:02,  0.92it/s]\u001b[A\n",
            "Epoch 16:  93%|█████████▎| 55/59 [00:52<00:03,  1.05it/s, v_num=0, train_loss_step=-2.88e+8, val_loss=-3.64e+8, train_loss_epoch=-3.84e+8]\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:07<00:01,  0.96it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:07<00:00,  1.06it/s]\u001b[A\n",
            "Epoch 13: 100%|██████████| 59/59 [01:14<00:00,  0.79it/s, v_num=0, train_loss_step=-4.11e+8, val_loss=-3.52e+8, train_loss_epoch=-3.17e+8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-05 11:16:20,813\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/8da927e1/checkpoint_000013)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \rEpoch 13: 100%|██████████| 59/59 [01:14<00:00,  0.79it/s, v_num=0, train_loss_step=-4.11e+8, val_loss=-3.52e+8, train_loss_epoch=-3.63e+8]\n",
            "Epoch 16:  95%|█████████▍| 56/59 [00:53<00:02,  1.05it/s, v_num=0, train_loss_step=-4.76e+8, val_loss=-3.64e+8, train_loss_epoch=-3.84e+8]\n",
            "Epoch 14:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-4.11e+8, val_loss=-3.52e+8, train_loss_epoch=-3.63e+8]\n",
            "Epoch 16:  97%|█████████▋| 57/59 [00:54<00:01,  1.05it/s, v_num=0, train_loss_step=-3.87e+8, val_loss=-3.64e+8, train_loss_epoch=-3.84e+8]\n",
            "Epoch 16:  98%|█████████▊| 58/59 [00:54<00:00,  1.06it/s, v_num=0, train_loss_step=-4.11e+8, val_loss=-3.64e+8, train_loss_epoch=-3.84e+8]\n",
            "Epoch 16: 100%|██████████| 59/59 [00:55<00:00,  1.07it/s, v_num=0, train_loss_step=-4.33e+8, val_loss=-3.64e+8, train_loss_epoch=-3.84e+8]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Epoch 14:   3%|▎         | 2/59 [00:01<00:56,  1.01it/s, v_num=0, train_loss_step=-4.12e+8, val_loss=-3.52e+8, train_loss_epoch=-3.63e+8]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:01<00:02,  1.72it/s]\u001b[A\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:04<00:00,  1.81it/s]\u001b[A\n",
            "Epoch 16: 100%|██████████| 59/59 [00:59<00:00,  0.98it/s, v_num=0, train_loss_step=-4.33e+8, val_loss=-3.97e+8, train_loss_epoch=-3.84e+8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/c410a352/checkpoint_000016)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16: 100%|██████████| 59/59 [01:00<00:00,  0.98it/s, v_num=0, train_loss_step=-4.33e+8, val_loss=-3.97e+8, train_loss_epoch=-4.14e+8]\n",
            "Epoch 17:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-4.33e+8, val_loss=-3.97e+8, train_loss_epoch=-4.14e+8]\n",
            "Epoch 17:   2%|▏         | 1/59 [00:01<01:16,  0.76it/s, v_num=0, train_loss_step=-3.91e+8, val_loss=-3.97e+8, train_loss_epoch=-4.14e+8]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:04<00:00,  1.67it/s]\u001b[A\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "Epoch 17:  10%|█         | 6/59 [00:06<00:59,  0.88it/s, v_num=0, train_loss_step=-4.35e+8, val_loss=-3.97e+8, train_loss_epoch=-4.14e+8]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:16:36. Total running time: 18min 1s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: c410a352 with val_loss=-397465952.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2200, 'ffn_num_layers': 2, 'message_hidden_dim': 400}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1       val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500       14            1053.37   -3.63412e+08        -4.10734e+08   0.285714   -3.51864e+08 |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400       17            1043.08   -4.14458e+08        -4.32687e+08   0.37037    -3.97466e+08 |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000                                                                                          |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 14:  29%|██▉       | 17/59 [00:19<00:47,  0.89it/s, v_num=0, train_loss_step=-4.05e+8, val_loss=-3.52e+8, train_loss_epoch=-3.63e+8]\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "Epoch 17:  31%|███       | 18/59 [00:17<00:40,  1.02it/s, v_num=0, train_loss_step=-4.42e+8, val_loss=-3.97e+8, train_loss_epoch=-4.14e+8]\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "Epoch 14:  44%|████▍     | 26/59 [00:30<00:38,  0.86it/s, v_num=0, train_loss_step=-4.02e+8, val_loss=-3.52e+8, train_loss_epoch=-3.63e+8]\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "Epoch 17:  51%|█████     | 30/59 [00:28<00:27,  1.06it/s, v_num=0, train_loss_step=-4.91e+8, val_loss=-3.97e+8, train_loss_epoch=-4.14e+8]\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "Epoch 17:  59%|█████▉    | 35/59 [00:33<00:23,  1.03it/s, v_num=0, train_loss_step=-3.09e+8, val_loss=-3.97e+8, train_loss_epoch=-4.14e+8]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:17:06. Total running time: 18min 31s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: c410a352 with val_loss=-397465952.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2200, 'ffn_num_layers': 2, 'message_hidden_dim': 400}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1       val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500       14            1053.37   -3.63412e+08        -4.10734e+08   0.285714   -3.51864e+08 |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400       17            1043.08   -4.14458e+08        -4.32687e+08   0.37037    -3.97466e+08 |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000                                                                                          |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 17:  69%|██████▉   | 41/59 [00:39<00:17,  1.05it/s, v_num=0, train_loss_step=-4.4e+8, val_loss=-3.97e+8, train_loss_epoch=-4.14e+8] \u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "Epoch 14:  78%|███████▊  | 46/59 [00:51<00:14,  0.89it/s, v_num=0, train_loss_step=-3.64e+8, val_loss=-3.52e+8, train_loss_epoch=-3.63e+8]\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "Epoch 17:  88%|████████▊ | 52/59 [00:49<00:06,  1.04it/s, v_num=0, train_loss_step=-3.52e+8, val_loss=-3.97e+8, train_loss_epoch=-4.14e+8]\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "Epoch 17:  92%|█████████▏| 54/59 [00:51<00:04,  1.05it/s, v_num=0, train_loss_step=-3.67e+8, val_loss=-3.97e+8, train_loss_epoch=-4.14e+8]\n",
            "Epoch 17:  93%|█████████▎| 55/59 [00:52<00:03,  1.05it/s, v_num=0, train_loss_step=-4.8e+8, val_loss=-3.97e+8, train_loss_epoch=-4.14e+8] \n",
            "Epoch 17:  95%|█████████▍| 56/59 [00:53<00:02,  1.06it/s, v_num=0, train_loss_step=-4.88e+8, val_loss=-3.97e+8, train_loss_epoch=-4.14e+8]\n",
            "Epoch 17:  97%|█████████▋| 57/59 [00:53<00:01,  1.06it/s, v_num=0, train_loss_step=-4.72e+8, val_loss=-3.97e+8, train_loss_epoch=-4.14e+8]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 14:  90%|████████▉ | 53/59 [01:00<00:06,  0.87it/s, v_num=0, train_loss_step=-4.5e+8, val_loss=-3.52e+8, train_loss_epoch=-3.63e+8] \u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:03,  2.13it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:03,  1.86it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:01<00:02,  1.75it/s]\u001b[A\n",
            "Epoch 14:  95%|█████████▍| 56/59 [01:03<00:03,  0.88it/s, v_num=0, train_loss_step=-4.72e+8, val_loss=-3.52e+8, train_loss_epoch=-3.63e+8]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:02<00:02,  1.72it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:02<00:01,  1.70it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:03<00:01,  1.69it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:04<00:00,  1.67it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:04<00:00,  1.83it/s]\u001b[A\n",
            "Epoch 17: 100%|██████████| 59/59 [00:59<00:00,  0.99it/s, v_num=0, train_loss_step=-4.72e+8, val_loss=-3.65e+8, train_loss_epoch=-4.14e+8]\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/c410a352/checkpoint_000017)\n",
            "2025-05-05 11:17:28,582\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \rEpoch 17: 100%|██████████| 59/59 [01:00<00:00,  0.98it/s, v_num=0, train_loss_step=-4.72e+8, val_loss=-3.65e+8, train_loss_epoch=-4.43e+8]\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \rValidation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \rValidation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:05,  1.06it/s]\u001b[A\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "Epoch 18:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-4.72e+8, val_loss=-3.65e+8, train_loss_epoch=-4.43e+8]\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Epoch 14: 100%|██████████| 59/59 [01:06<00:00,  0.89it/s, v_num=0, train_loss_step=-2.6e+8, val_loss=-3.52e+8, train_loss_epoch=-3.63e+8] \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "Epoch 18:   2%|▏         | 1/59 [00:01<01:28,  0.66it/s, v_num=0, train_loss_step=-4.83e+8, val_loss=-3.65e+8, train_loss_epoch=-4.43e+8]\n",
            "Epoch 18:   3%|▎         | 2/59 [00:02<01:15,  0.76it/s, v_num=0, train_loss_step=-5.28e+8, val_loss=-3.65e+8, train_loss_epoch=-4.43e+8]\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Epoch 18:   5%|▌         | 3/59 [00:03<01:13,  0.76it/s, v_num=0, train_loss_step=-4.64e+8, val_loss=-3.65e+8, train_loss_epoch=-4.43e+8]\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Epoch 18:   7%|▋         | 4/59 [00:04<01:04,  0.85it/s, v_num=0, train_loss_step=-5.12e+8, val_loss=-3.65e+8, train_loss_epoch=-4.43e+8]\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:07<00:01,  0.94it/s]\u001b[A\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 18:   8%|▊         | 5/59 [00:05<00:59,  0.91it/s, v_num=0, train_loss_step=-4.78e+8, val_loss=-3.65e+8, train_loss_epoch=-4.43e+8]\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:07<00:00,  1.04it/s]\u001b[A\n",
            "Epoch 14: 100%|██████████| 59/59 [01:13<00:00,  0.80it/s, v_num=0, train_loss_step=-2.6e+8, val_loss=-3.94e+8, train_loss_epoch=-3.63e+8]\n",
            "Epoch 14: 100%|██████████| 59/59 [01:14<00:00,  0.79it/s, v_num=0, train_loss_step=-2.6e+8, val_loss=-3.94e+8, train_loss_epoch=-4e+8]   \n",
            "Epoch 18:  10%|█         | 6/59 [00:06<00:56,  0.93it/s, v_num=0, train_loss_step=-4.67e+8, val_loss=-3.65e+8, train_loss_epoch=-4.43e+8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/8da927e1/checkpoint_000014)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-2.6e+8, val_loss=-3.94e+8, train_loss_epoch=-4e+8]\n",
            "Epoch 18:  12%|█▏        | 7/59 [00:07<00:53,  0.96it/s, v_num=0, train_loss_step=-4.28e+8, val_loss=-3.65e+8, train_loss_epoch=-4.43e+8]\n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:17:37. Total running time: 19min 1s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-393851296.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1       val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500       15            1128.09   -4.0039e+08         -2.60359e+08   0.402235   -3.93851e+08 |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400       18            1103.58   -4.43248e+08        -4.72427e+08   0.391304   -3.64524e+08 |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000                                                                                          |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 15:  10%|█         | 6/59 [00:05<00:50,  1.05it/s, v_num=0, train_loss_step=-4.69e+8, val_loss=-3.94e+8, train_loss_epoch=-4e+8]\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "Epoch 18:  31%|███       | 18/59 [00:17<00:39,  1.03it/s, v_num=0, train_loss_step=-5.13e+8, val_loss=-3.65e+8, train_loss_epoch=-4.43e+8]\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "Epoch 18:  41%|████      | 24/59 [00:22<00:33,  1.05it/s, v_num=0, train_loss_step=-4.64e+8, val_loss=-3.65e+8, train_loss_epoch=-4.43e+8]\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "Epoch 15:  34%|███▍      | 20/59 [00:21<00:41,  0.93it/s, v_num=0, train_loss_step=-3.73e+8, val_loss=-3.94e+8, train_loss_epoch=-4e+8]\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "Epoch 18:  59%|█████▉    | 35/59 [00:33<00:23,  1.04it/s, v_num=0, train_loss_step=-4.55e+8, val_loss=-3.65e+8, train_loss_epoch=-4.43e+8]\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:18:07. Total running time: 19min 31s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-393851296.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1       val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500       15            1128.09   -4.0039e+08         -2.60359e+08   0.402235   -3.93851e+08 |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400       18            1103.58   -4.43248e+08        -4.72427e+08   0.391304   -3.64524e+08 |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000                                                                                          |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 15:  49%|████▉     | 29/59 [00:32<00:33,  0.90it/s, v_num=0, train_loss_step=-3.58e+8, val_loss=-3.94e+8, train_loss_epoch=-4e+8]\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "Epoch 15:  58%|█████▊    | 34/59 [00:37<00:27,  0.91it/s, v_num=0, train_loss_step=-4.64e+8, val_loss=-3.94e+8, train_loss_epoch=-4e+8]\n",
            "Epoch 15:  58%|█████▊    | 34/59 [00:37<00:27,  0.91it/s, v_num=0, train_loss_step=-3.07e+8, val_loss=-3.94e+8, train_loss_epoch=-4e+8]\n",
            "Epoch 18:  80%|███████▉  | 47/59 [00:43<00:11,  1.09it/s, v_num=0, train_loss_step=-4.64e+8, val_loss=-3.65e+8, train_loss_epoch=-4.43e+8]\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "Epoch 15:  63%|██████▎   | 37/59 [00:42<00:25,  0.87it/s, v_num=0, train_loss_step=-4.22e+8, val_loss=-3.94e+8, train_loss_epoch=-4e+8]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 18:  92%|█████████▏| 54/59 [00:50<00:04,  1.06it/s, v_num=0, train_loss_step=-4.4e+8, val_loss=-3.65e+8, train_loss_epoch=-4.43e+8] \n",
            "Epoch 18:  93%|█████████▎| 55/59 [00:51<00:03,  1.06it/s, v_num=0, train_loss_step=-4.9e+8, val_loss=-3.65e+8, train_loss_epoch=-4.43e+8]\n",
            "Epoch 18:  95%|█████████▍| 56/59 [00:52<00:02,  1.07it/s, v_num=0, train_loss_step=-5.23e+8, val_loss=-3.65e+8, train_loss_epoch=-4.43e+8]\n",
            "Epoch 18:  97%|█████████▋| 57/59 [00:53<00:01,  1.07it/s, v_num=0, train_loss_step=-3.84e+8, val_loss=-3.65e+8, train_loss_epoch=-4.43e+8]\n",
            "Epoch 18:  98%|█████████▊| 58/59 [00:54<00:00,  1.07it/s, v_num=0, train_loss_step=-4.7e+8, val_loss=-3.65e+8, train_loss_epoch=-4.43e+8] \n",
            "Epoch 15:  71%|███████   | 42/59 [00:47<00:19,  0.89it/s, v_num=0, train_loss_step=-4.81e+8, val_loss=-3.94e+8, train_loss_epoch=-4e+8]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 18: 100%|██████████| 59/59 [00:54<00:00,  1.08it/s, v_num=0, train_loss_step=-3.56e+8, val_loss=-3.65e+8, train_loss_epoch=-4.43e+8]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:03,  2.11it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:03,  1.81it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:01<00:02,  1.75it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:02<00:02,  1.71it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:02<00:01,  1.71it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:03<00:01,  1.69it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:04<00:00,  1.70it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:04<00:00,  1.80it/s]\u001b[A\n",
            "Epoch 18: 100%|██████████| 59/59 [00:59<00:00,  1.00it/s, v_num=0, train_loss_step=-3.56e+8, val_loss=-4.62e+8, train_loss_epoch=-4.43e+8]\n",
            "Epoch 15:  80%|███████▉  | 47/59 [00:52<00:13,  0.90it/s, v_num=0, train_loss_step=-4.7e+8, val_loss=-3.94e+8, train_loss_epoch=-4e+8] \u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-05 11:18:29,292\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/c410a352/checkpoint_000018)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18: 100%|██████████| 59/59 [01:00<00:00,  0.98it/s, v_num=0, train_loss_step=-3.56e+8, val_loss=-4.62e+8, train_loss_epoch=-4.63e+8]\n",
            "Epoch 19:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-3.56e+8, val_loss=-4.62e+8, train_loss_epoch=-4.63e+8]\n",
            "Epoch 15:  85%|████████▍ | 50/59 [00:56<00:10,  0.88it/s, v_num=0, train_loss_step=-5e+8, val_loss=-3.94e+8, train_loss_epoch=-4e+8]\n",
            "Epoch 15:  85%|████████▍ | 50/59 [00:56<00:10,  0.88it/s, v_num=0, train_loss_step=-4.83e+8, val_loss=-3.94e+8, train_loss_epoch=-4e+8]\n",
            "Epoch 19:   5%|▌         | 3/59 [00:03<01:12,  0.78it/s, v_num=0, train_loss_step=-5.03e+8, val_loss=-4.62e+8, train_loss_epoch=-4.63e+8]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 15:  92%|█████████▏| 54/59 [01:01<00:05,  0.88it/s, v_num=0, train_loss_step=-4.31e+8, val_loss=-3.94e+8, train_loss_epoch=-4e+8]\n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:18:37. Total running time: 20min 1s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: c410a352 with val_loss=-462312288.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2200, 'ffn_num_layers': 2, 'message_hidden_dim': 400}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1       val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500       15            1128.09   -4.0039e+08         -2.60359e+08   0.402235   -3.93851e+08 |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400       19            1164.22   -4.63387e+08        -3.55798e+08   0.233577   -4.62312e+08 |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000                                                                                          |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 15:  93%|█████████▎| 55/59 [01:02<00:04,  0.89it/s, v_num=0, train_loss_step=-4.27e+8, val_loss=-3.94e+8, train_loss_epoch=-4e+8]\n",
            "Epoch 15:  95%|█████████▍| 56/59 [01:02<00:03,  0.89it/s, v_num=0, train_loss_step=-4.86e+8, val_loss=-3.94e+8, train_loss_epoch=-4e+8]\n",
            "Epoch 19:  15%|█▌        | 9/59 [00:08<00:48,  1.04it/s, v_num=0, train_loss_step=-5.35e+8, val_loss=-4.62e+8, train_loss_epoch=-4.63e+8]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "Epoch 15:  97%|█████████▋| 57/59 [01:03<00:02,  0.89it/s, v_num=0, train_loss_step=-4.54e+8, val_loss=-3.94e+8, train_loss_epoch=-4e+8]\n",
            "Epoch 15:  98%|█████████▊| 58/59 [01:05<00:01,  0.89it/s, v_num=0, train_loss_step=-4.81e+8, val_loss=-3.94e+8, train_loss_epoch=-4e+8]\n",
            "Epoch 15: 100%|██████████| 59/59 [01:05<00:00,  0.90it/s, v_num=0, train_loss_step=-4.74e+8, val_loss=-3.94e+8, train_loss_epoch=-4e+8]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:04,  1.60it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:04,  1.49it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:02<00:03,  1.29it/s]\u001b[A\n",
            "Epoch 19:  25%|██▌       | 15/59 [00:13<00:39,  1.11it/s, v_num=0, train_loss_step=-4.54e+8, val_loss=-4.62e+8, train_loss_epoch=-4.63e+8]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:03<00:03,  1.07it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:05<00:03,  1.00it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:06<00:02,  0.96it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:07<00:01,  0.96it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:07<00:00,  1.07it/s]\u001b[A\n",
            "Epoch 15: 100%|██████████| 59/59 [01:13<00:00,  0.81it/s, v_num=0, train_loss_step=-4.74e+8, val_loss=-4.2e+8, train_loss_epoch=-4e+8] \n",
            "Epoch 19:  32%|███▏      | 19/59 [00:18<00:39,  1.01it/s, v_num=0, train_loss_step=-5.53e+8, val_loss=-4.62e+8, train_loss_epoch=-4.63e+8]\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/8da927e1/checkpoint_000015)\n",
            "2025-05-05 11:18:49,600\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15: 100%|██████████| 59/59 [01:13<00:00,  0.80it/s, v_num=0, train_loss_step=-4.74e+8, val_loss=-4.2e+8, train_loss_epoch=-4.37e+8]\n",
            "Epoch 16:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-4.74e+8, val_loss=-4.2e+8, train_loss_epoch=-4.37e+8]\n",
            "Epoch 19:  44%|████▍     | 26/59 [00:24<00:31,  1.06it/s, v_num=0, train_loss_step=-5.31e+8, val_loss=-4.62e+8, train_loss_epoch=-4.63e+8]\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "Epoch 19:  54%|█████▍    | 32/59 [00:29<00:25,  1.07it/s, v_num=0, train_loss_step=-4.55e+8, val_loss=-4.62e+8, train_loss_epoch=-4.63e+8]\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "Epoch 16:  24%|██▎       | 14/59 [00:15<00:49,  0.90it/s, v_num=0, train_loss_step=-4.68e+8, val_loss=-4.2e+8, train_loss_epoch=-4.37e+8]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "Trial status: 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:19:07. Total running time: 20min 31s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: c410a352 with val_loss=-462312288.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2200, 'ffn_num_layers': 2, 'message_hidden_dim': 400}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status       ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1       val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                         2                     2000                        2                      500       16            1202.09   -4.37316e+08        -4.73885e+08   0.360656   -4.20417e+08 |\n",
            "| TorchTrainer_c410a352   RUNNING                         2                     2200                        2                      400       19            1164.22   -4.63387e+08        -3.55798e+08   0.233577   -4.62312e+08 |\n",
            "| TorchTrainer_33384398   PENDING                         2                      600                        2                     2000                                                                                          |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 19:  75%|███████▍  | 44/59 [00:41<00:14,  1.07it/s, v_num=0, train_loss_step=-4.27e+8, val_loss=-4.62e+8, train_loss_epoch=-4.63e+8]\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
            "Epoch 19:  83%|████████▎ | 49/59 [00:46<00:09,  1.06it/s, v_num=0, train_loss_step=-5.21e+8, val_loss=-4.62e+8, train_loss_epoch=-4.63e+8]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "Epoch 19:  92%|█████████▏| 54/59 [00:51<00:04,  1.05it/s, v_num=0, train_loss_step=-4.96e+8, val_loss=-4.62e+8, train_loss_epoch=-4.63e+8]\n",
            "Epoch 19:  93%|█████████▎| 55/59 [00:52<00:03,  1.06it/s, v_num=0, train_loss_step=-5.13e+8, val_loss=-4.62e+8, train_loss_epoch=-4.63e+8]\n",
            "Epoch 16:  47%|████▋     | 28/59 [00:31<00:34,  0.90it/s, v_num=0, train_loss_step=-4.58e+8, val_loss=-4.2e+8, train_loss_epoch=-4.37e+8]\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "Epoch 19:  95%|█████████▍| 56/59 [00:52<00:02,  1.06it/s, v_num=0, train_loss_step=-5.17e+8, val_loss=-4.62e+8, train_loss_epoch=-4.63e+8]\n",
            "Epoch 19:  97%|█████████▋| 57/59 [00:53<00:01,  1.06it/s, v_num=0, train_loss_step=-5.44e+8, val_loss=-4.62e+8, train_loss_epoch=-4.63e+8]\n",
            "Epoch 19:  98%|█████████▊| 58/59 [00:54<00:00,  1.07it/s, v_num=0, train_loss_step=-5.23e+8, val_loss=-4.62e+8, train_loss_epoch=-4.63e+8]\n",
            "Epoch 19: 100%|██████████| 59/59 [00:54<00:00,  1.07it/s, v_num=0, train_loss_step=-5.17e+8, val_loss=-4.62e+8, train_loss_epoch=-4.63e+8]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:03,  2.11it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:03,  1.85it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:01<00:02,  1.74it/s]\u001b[A\n",
            "Epoch 16:  58%|█████▊    | 34/59 [00:37<00:27,  0.92it/s, v_num=0, train_loss_step=-5.19e+8, val_loss=-4.2e+8, train_loss_epoch=-4.37e+8]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:02<00:02,  1.71it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:02<00:01,  1.71it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:03<00:01,  1.69it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:04<00:00,  1.59it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:04<00:00,  1.66it/s]\u001b[A\n",
            "Epoch 19: 100%|██████████| 59/59 [00:59<00:00,  0.99it/s, v_num=0, train_loss_step=-5.17e+8, val_loss=-4.67e+8, train_loss_epoch=-4.63e+8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-05 11:19:30,550\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/c410a352/checkpoint_000019)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m \rEpoch 19: 100%|██████████| 59/59 [01:00<00:00,  0.97it/s, v_num=0, train_loss_step=-5.17e+8, val_loss=-4.67e+8, train_loss_epoch=-4.88e+8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=3058)\u001b[0m `Trainer.fit` stopped: `max_epochs=20` reached.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19: 100%|██████████| 59/59 [01:01<00:00,  0.97it/s, v_num=0, train_loss_step=-5.17e+8, val_loss=-4.67e+8, train_loss_epoch=-4.88e+8]\n",
            "Epoch 16:  64%|██████▍   | 38/59 [00:42<00:23,  0.90it/s, v_num=0, train_loss_step=-4.42e+8, val_loss=-4.2e+8, train_loss_epoch=-4.37e+8]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\n",
            "Trial TorchTrainer_c410a352 completed after 20 iterations at 2025-05-05 11:19:32. Total running time: 20min 56s\n",
            "+----------------------------------------------------------+\n",
            "| Trial TorchTrainer_c410a352 result                       |\n",
            "+----------------------------------------------------------+\n",
            "| checkpoint_dir_name                    checkpoint_000019 |\n",
            "| time_this_iter_s                                61.24575 |\n",
            "| time_total_s                                  1225.46825 |\n",
            "| training_iteration                                    20 |\n",
            "| epoch                                                 19 |\n",
            "| step                                                1180 |\n",
            "| train_loss                                   -487560160. |\n",
            "| train_loss_epoch                             -487560160. |\n",
            "| train_loss_step                              -517013760. |\n",
            "| val/f1                                           0.26667 |\n",
            "| val_loss                                     -467179616. |\n",
            "+----------------------------------------------------------+\n",
            "\n",
            "Trial status: 1 RUNNING | 1 TERMINATED | 1 PENDING\n",
            "Current time: 2025-05-05 11:19:37. Total running time: 21min 1s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: c410a352 with val_loss=-467179616.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2200, 'ffn_num_layers': 2, 'message_hidden_dim': 400}}\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1       val_loss |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                           2                     2000                        2                      500       16            1202.09   -4.37316e+08        -4.73885e+08   0.360656   -4.20417e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20            1225.47   -4.8756e+08         -5.17014e+08   0.266667   -4.6718e+08  |\n",
            "| TorchTrainer_33384398   PENDING                           2                      600                        2                     2000                                                                                          |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 16:  69%|██████▉   | 41/59 [00:47<00:20,  0.87it/s, v_num=0, train_loss_step=-4.74e+8, val_loss=-4.2e+8, train_loss_epoch=-4.37e+8]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "Epoch 16:  80%|███████▉  | 47/59 [00:53<00:13,  0.89it/s, v_num=0, train_loss_step=-4.07e+8, val_loss=-4.2e+8, train_loss_epoch=-4.37e+8]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 16:  86%|████████▋ | 51/59 [00:58<00:09,  0.87it/s, v_num=0, train_loss_step=-5.25e+8, val_loss=-4.2e+8, train_loss_epoch=-4.37e+8]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "Epoch 16:  92%|█████████▏| 54/59 [01:01<00:05,  0.88it/s, v_num=0, train_loss_step=-3.51e+8, val_loss=-4.2e+8, train_loss_epoch=-4.37e+8]\n",
            "Epoch 16:  93%|█████████▎| 55/59 [01:02<00:04,  0.88it/s, v_num=0, train_loss_step=-4.08e+8, val_loss=-4.2e+8, train_loss_epoch=-4.37e+8]\n",
            "Epoch 16:  95%|█████████▍| 56/59 [01:03<00:03,  0.88it/s, v_num=0, train_loss_step=-5.33e+8, val_loss=-4.2e+8, train_loss_epoch=-4.37e+8]\n",
            "\n",
            "Trial TorchTrainer_33384398 started with configuration:\n",
            "+---------------------------------------------+\n",
            "| Trial TorchTrainer_33384398 config          |\n",
            "+---------------------------------------------+\n",
            "| train_loop_config/depth                   2 |\n",
            "| train_loop_config/ffn_hidden_dim        600 |\n",
            "| train_loop_config/ffn_num_layers          2 |\n",
            "| train_loop_config/message_hidden_dim   2000 |\n",
            "+---------------------------------------------+\n",
            "Epoch 16:  97%|█████████▋| 57/59 [01:04<00:02,  0.89it/s, v_num=0, train_loss_step=-4.38e+8, val_loss=-4.2e+8, train_loss_epoch=-4.37e+8]\n",
            "Epoch 16:  90%|████████▉ | 53/59 [01:00<00:06,  0.87it/s, v_num=0, train_loss_step=-4.03e+8, val_loss=-4.2e+8, train_loss_epoch=-4.37e+8]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "Epoch 16:  98%|█████████▊| 58/59 [01:05<00:01,  0.89it/s, v_num=0, train_loss_step=-4.55e+8, val_loss=-4.2e+8, train_loss_epoch=-4.37e+8]\n",
            "Epoch 16: 100%|██████████| 59/59 [01:05<00:00,  0.90it/s, v_num=0, train_loss_step=-5.1e+8, val_loss=-4.2e+8, train_loss_epoch=-4.37e+8] \n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:04,  1.72it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:03,  1.56it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:02<00:03,  1.49it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:02<00:02,  1.40it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:04<00:02,  1.24it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:05<00:01,  1.11it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:06<00:00,  1.07it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:06<00:00,  1.15it/s]\u001b[A\n",
            "Epoch 16: 100%|██████████| 59/59 [01:12<00:00,  0.81it/s, v_num=0, train_loss_step=-5.1e+8, val_loss=-4.59e+8, train_loss_epoch=-4.37e+8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/8da927e1/checkpoint_000016)\n",
            "2025-05-05 11:20:03,451\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16: 100%|██████████| 59/59 [01:13<00:00,  0.80it/s, v_num=0, train_loss_step=-5.1e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\n",
            "Epoch 17:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-5.1e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(TorchTrainer pid=8176)\u001b[0m Started distributed worker processes: \n",
            "\u001b[36m(TorchTrainer pid=8176)\u001b[0m - (node_id=85d1826776a8b04eb71395ec08130d8b282f617a724b5c6551ccdd9f, ip=172.28.0.12, pid=8293) world_rank=0, local_rank=0, node_rank=0\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17:   2%|▏         | 1/59 [00:00<00:51,  1.12it/s, v_num=0, train_loss_step=-4.51e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\n",
            "Epoch 17:   3%|▎         | 2/59 [00:01<00:50,  1.12it/s, v_num=0, train_loss_step=-3.94e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\n",
            "Epoch 17:   5%|▌         | 3/59 [00:02<00:50,  1.11it/s, v_num=0, train_loss_step=-5.46e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\n",
            "\n",
            "Trial status: 2 RUNNING | 1 TERMINATED | 1 PENDING\n",
            "Current time: 2025-05-05 11:20:07. Total running time: 21min 31s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: c410a352 with val_loss=-467179616.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2200, 'ffn_num_layers': 2, 'message_hidden_dim': 400}}\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1       val_loss |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                           2                     2000                        2                      500       17            1275.91   -4.70688e+08        -5.09762e+08   0.358382   -4.58879e+08 |\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000                                                                                          |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20            1225.47   -4.8756e+08         -5.17014e+08   0.266667   -4.6718e+08  |\n",
            "| TorchTrainer_b3fd7458   PENDING                           4                     2300                        1                      300                                                                                          |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 17:   7%|▋         | 4/59 [00:03<00:49,  1.12it/s, v_num=0, train_loss_step=-4.03e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\n",
            "Epoch 17:   8%|▊         | 5/59 [00:04<00:48,  1.11it/s, v_num=0, train_loss_step=-5.16e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\n",
            "Epoch 17:  10%|█         | 6/59 [00:05<00:48,  1.09it/s, v_num=0, train_loss_step=-5.01e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\n",
            "Epoch 17:  12%|█▏        | 7/59 [00:06<00:48,  1.08it/s, v_num=0, train_loss_step=-4.8e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8] \n",
            "Epoch 17:  14%|█▎        | 8/59 [00:07<00:47,  1.07it/s, v_num=0, train_loss_step=-5.41e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\n",
            "Epoch 17:  15%|█▌        | 9/59 [00:08<00:46,  1.07it/s, v_num=0, train_loss_step=-4.91e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m GPU available: False, used: False\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \rEpoch 17:  17%|█▋        | 10/59 [00:09<00:45,  1.07it/s, v_num=0, train_loss_step=-4.91e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\rEpoch 17:  17%|█▋        | 10/59 [00:09<00:45,  1.07it/s, v_num=0, train_loss_step=-5.3e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m 2025-05-05 11:20:13.061395: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m E0000 00:00:1746444013.115176    8406 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m E0000 00:00:1746444013.129929    8406 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m 2025-05-05 11:20:13.178608: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17:  19%|█▊        | 11/59 [00:10<00:47,  1.00it/s, v_num=0, train_loss_step=-5.24e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\n",
            "Epoch 17:  20%|██        | 12/59 [00:12<00:49,  0.94it/s, v_num=0, train_loss_step=-5.07e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\n",
            "Epoch 17:  22%|██▏       | 13/59 [00:14<00:50,  0.91it/s, v_num=0, train_loss_step=-4.75e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\n",
            "Epoch 17:  24%|██▎       | 14/59 [00:15<00:49,  0.91it/s, v_num=0, train_loss_step=-5.01e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\n",
            "Epoch 17:  25%|██▌       | 15/59 [00:16<00:47,  0.92it/s, v_num=0, train_loss_step=-4.74e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\n",
            "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n",
            "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m   | Name            | Type                    | Params | Mode \n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m --------------------------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m 0 | message_passing | BondMessagePassing      | 8.3 M  | train\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m 1 | agg             | MeanAggregation         | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m 2 | bn              | BatchNorm1d             | 4.0 K  | train\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m 3 | predictor       | BinaryClassificationFFN | 1.6 M  | train\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m 4 | X_d_transform   | Identity                | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m 5 | metrics         | ModuleList              | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m --------------------------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m 9.9 M     Trainable params\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m 0         Non-trainable params\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m 9.9 M     Total params\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m 39.535    Total estimated model params size (MB)\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m 26        Modules in train mode\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m 0         Modules in eval mode\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m /usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17:  27%|██▋       | 16/59 [00:17<00:46,  0.93it/s, v_num=0, train_loss_step=-3.9e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8] \n",
            "Epoch 17:  29%|██▉       | 17/59 [00:18<00:45,  0.93it/s, v_num=0, train_loss_step=-4.89e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\n",
            "Epoch 17:  31%|███       | 18/59 [00:19<00:43,  0.94it/s, v_num=0, train_loss_step=-4.93e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\n",
            "Epoch 17:  32%|███▏      | 19/59 [00:20<00:42,  0.94it/s, v_num=0, train_loss_step=-5.35e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\n",
            "Epoch 17:  34%|███▍      | 20/59 [00:21<00:41,  0.94it/s, v_num=0, train_loss_step=-4.97e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\n",
            "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:05<00:05,  0.19it/s]\n",
            "Epoch 17:  36%|███▌      | 21/59 [00:22<00:40,  0.95it/s, v_num=0, train_loss_step=-5.21e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\n",
            "Epoch 17:  37%|███▋      | 22/59 [00:23<00:39,  0.95it/s, v_num=0, train_loss_step=-5.52e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\n",
            "Epoch 17:  39%|███▉      | 23/59 [00:24<00:37,  0.95it/s, v_num=0, train_loss_step=-4.69e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\n",
            "Epoch 17:  41%|████      | 24/59 [00:25<00:37,  0.93it/s, v_num=0, train_loss_step=-5.32e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\n",
            "Epoch 17:  42%|████▏     | 25/59 [00:27<00:37,  0.91it/s, v_num=0, train_loss_step=-4.53e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\n",
            "Epoch 0:   0%|          | 0/59 [00:00<?, ?it/s] \n",
            "Epoch 17:  44%|████▍     | 26/59 [00:29<00:36,  0.89it/s, v_num=0, train_loss_step=-4.88e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\n",
            "Epoch 17:  46%|████▌     | 27/59 [00:30<00:36,  0.89it/s, v_num=0, train_loss_step=-5.26e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\n",
            "Epoch 17:  47%|████▋     | 28/59 [00:31<00:34,  0.89it/s, v_num=0, train_loss_step=-5.27e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\n",
            "Epoch 17:  49%|████▉     | 29/59 [00:32<00:33,  0.89it/s, v_num=0, train_loss_step=-4.89e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\n",
            "Epoch 17:  51%|█████     | 30/59 [00:33<00:32,  0.90it/s, v_num=0, train_loss_step=-5.43e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\n",
            "Trial status: 2 RUNNING | 1 TERMINATED | 1 PENDING\n",
            "Current time: 2025-05-05 11:20:37. Total running time: 22min 2s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: c410a352 with val_loss=-467179616.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2200, 'ffn_num_layers': 2, 'message_hidden_dim': 400}}\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1       val_loss |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                           2                     2000                        2                      500       17            1275.91   -4.70688e+08        -5.09762e+08   0.358382   -4.58879e+08 |\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000                                                                                          |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20            1225.47   -4.8756e+08         -5.17014e+08   0.266667   -4.6718e+08  |\n",
            "| TorchTrainer_b3fd7458   PENDING                           4                     2300                        1                      300                                                                                          |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 17:  53%|█████▎    | 31/59 [00:34<00:31,  0.90it/s, v_num=0, train_loss_step=-5.43e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\n",
            "Epoch 17:  54%|█████▍    | 32/59 [00:35<00:29,  0.90it/s, v_num=0, train_loss_step=-5.45e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\n",
            "Epoch 17:  56%|█████▌    | 33/59 [00:36<00:28,  0.90it/s, v_num=0, train_loss_step=-5.36e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\n",
            "Epoch 17:  63%|██████▎   | 37/59 [00:40<00:24,  0.90it/s, v_num=0, train_loss_step=-5.32e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 17:  69%|██████▉   | 41/59 [00:46<00:20,  0.88it/s, v_num=0, train_loss_step=-4.99e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 17:  78%|███████▊  | 46/59 [00:51<00:14,  0.89it/s, v_num=0, train_loss_step=-5.45e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 17:  86%|████████▋ | 51/59 [00:57<00:09,  0.88it/s, v_num=0, train_loss_step=-5.27e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 17:  92%|█████████▏| 54/59 [01:01<00:05,  0.87it/s, v_num=0, train_loss_step=-4.23e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\n",
            "Epoch 17:  93%|█████████▎| 55/59 [01:02<00:04,  0.88it/s, v_num=0, train_loss_step=-5.45e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\n",
            "Trial status: 2 RUNNING | 1 TERMINATED | 1 PENDING\n",
            "Current time: 2025-05-05 11:21:07. Total running time: 22min 32s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: c410a352 with val_loss=-467179616.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2200, 'ffn_num_layers': 2, 'message_hidden_dim': 400}}\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1       val_loss |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                           2                     2000                        2                      500       17            1275.91   -4.70688e+08        -5.09762e+08   0.358382   -4.58879e+08 |\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000                                                                                          |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20            1225.47   -4.8756e+08         -5.17014e+08   0.266667   -4.6718e+08  |\n",
            "| TorchTrainer_b3fd7458   PENDING                           4                     2300                        1                      300                                                                                          |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 17:  95%|█████████▍| 56/59 [01:03<00:03,  0.88it/s, v_num=0, train_loss_step=-5.32e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\n",
            "Epoch 0:   7%|▋         | 4/59 [00:32<07:29,  0.12it/s, v_num=0, train_loss_step=0.344]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "Epoch 17:  97%|█████████▋| 57/59 [01:04<00:02,  0.88it/s, v_num=0, train_loss_step=-5.26e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\n",
            "Epoch 17:  98%|█████████▊| 58/59 [01:05<00:01,  0.88it/s, v_num=0, train_loss_step=-5.34e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\n",
            "Epoch 17: 100%|██████████| 59/59 [01:06<00:00,  0.89it/s, v_num=0, train_loss_step=-5.32e+8, val_loss=-4.59e+8, train_loss_epoch=-4.71e+8]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:04,  1.49it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:04,  1.37it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:02<00:03,  1.35it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:02<00:02,  1.34it/s]\u001b[A\n",
            "Epoch 0:   8%|▊         | 5/59 [00:39<07:06,  0.13it/s, v_num=0, train_loss_step=0.450]\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:03<00:02,  1.31it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:05<00:01,  1.17it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:06<00:00,  1.07it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:06<00:00,  1.15it/s]\u001b[A\n",
            "Epoch 17: 100%|██████████| 59/59 [01:13<00:00,  0.80it/s, v_num=0, train_loss_step=-5.32e+8, val_loss=-4.35e+8, train_loss_epoch=-4.71e+8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/8da927e1/checkpoint_000017)\n",
            "2025-05-05 11:21:18,097\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17: 100%|██████████| 59/59 [01:14<00:00,  0.79it/s, v_num=0, train_loss_step=-5.32e+8, val_loss=-4.35e+8, train_loss_epoch=-5.02e+8]\n",
            "Epoch 18:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-5.32e+8, val_loss=-4.35e+8, train_loss_epoch=-5.02e+8]\n",
            "Epoch 18:   2%|▏         | 1/59 [00:01<01:06,  0.87it/s, v_num=0, train_loss_step=-5.53e+8, val_loss=-4.35e+8, train_loss_epoch=-5.02e+8]\n",
            "Epoch 18:   3%|▎         | 2/59 [00:02<01:01,  0.93it/s, v_num=0, train_loss_step=-5.82e+8, val_loss=-4.35e+8, train_loss_epoch=-5.02e+8]\n",
            "Epoch 18:   5%|▌         | 3/59 [00:03<01:00,  0.92it/s, v_num=0, train_loss_step=-5.13e+8, val_loss=-4.35e+8, train_loss_epoch=-5.02e+8]\n",
            "Epoch 18:  10%|█         | 6/59 [00:06<00:55,  0.96it/s, v_num=0, train_loss_step=-5.29e+8, val_loss=-4.35e+8, train_loss_epoch=-5.02e+8]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "Epoch 18:  19%|█▊        | 11/59 [00:11<00:52,  0.92it/s, v_num=0, train_loss_step=-5.18e+8, val_loss=-4.35e+8, train_loss_epoch=-5.02e+8]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 18:  25%|██▌       | 15/59 [00:17<00:51,  0.86it/s, v_num=0, train_loss_step=-4.59e+8, val_loss=-4.35e+8, train_loss_epoch=-5.02e+8]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "Trial status: 2 RUNNING | 1 TERMINATED | 1 PENDING\n",
            "Current time: 2025-05-05 11:21:37. Total running time: 23min 2s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: c410a352 with val_loss=-467179616.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2200, 'ffn_num_layers': 2, 'message_hidden_dim': 400}}\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1       val_loss |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                           2                     2000                        2                      500       18            1350.52   -5.01576e+08        -5.31705e+08   0.414508   -4.34686e+08 |\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000                                                                                          |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20            1225.47   -4.8756e+08         -5.17014e+08   0.266667   -4.6718e+08  |\n",
            "| TorchTrainer_b3fd7458   PENDING                           4                     2300                        1                      300                                                                                          |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 18:  34%|███▍      | 20/59 [00:22<00:43,  0.89it/s, v_num=0, train_loss_step=-5.42e+8, val_loss=-4.35e+8, train_loss_epoch=-5.02e+8]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 0:  15%|█▌        | 9/59 [01:13<06:48,  0.12it/s, v_num=0, train_loss_step=-0.251] \u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 18:  47%|████▋     | 28/59 [00:32<00:36,  0.86it/s, v_num=0, train_loss_step=-3.14e+8, val_loss=-4.35e+8, train_loss_epoch=-5.02e+8]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "Epoch 18:  58%|█████▊    | 34/59 [00:38<00:28,  0.88it/s, v_num=0, train_loss_step=-5.3e+8, val_loss=-4.35e+8, train_loss_epoch=-5.02e+8] \u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 0:  19%|█▊        | 11/59 [01:29<06:30,  0.12it/s, v_num=0, train_loss_step=-0.182]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "Trial status: 2 RUNNING | 1 TERMINATED | 1 PENDING\n",
            "Current time: 2025-05-05 11:22:07. Total running time: 23min 32s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: c410a352 with val_loss=-467179616.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2200, 'ffn_num_layers': 2, 'message_hidden_dim': 400}}\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1       val_loss |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                           2                     2000                        2                      500       18            1350.52   -5.01576e+08        -5.31705e+08   0.414508   -4.34686e+08 |\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000                                                                                          |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20            1225.47   -4.8756e+08         -5.17014e+08   0.266667   -4.6718e+08  |\n",
            "| TorchTrainer_b3fd7458   PENDING                           4                     2300                        1                      300                                                                                          |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 18:  73%|███████▎  | 43/59 [00:49<00:18,  0.87it/s, v_num=0, train_loss_step=-4.96e+8, val_loss=-4.35e+8, train_loss_epoch=-5.02e+8]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 18:  81%|████████▏ | 48/59 [00:54<00:12,  0.88it/s, v_num=0, train_loss_step=-5.83e+8, val_loss=-4.35e+8, train_loss_epoch=-5.02e+8]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 0:  22%|██▏       | 13/59 [01:45<06:14,  0.12it/s, v_num=0, train_loss_step=-0.746] \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "Epoch 18:  92%|█████████▏| 54/59 [01:02<00:05,  0.86it/s, v_num=0, train_loss_step=-4.88e+8, val_loss=-4.35e+8, train_loss_epoch=-5.02e+8]\n",
            "Epoch 18:  93%|█████████▎| 55/59 [01:03<00:04,  0.87it/s, v_num=0, train_loss_step=-5.37e+8, val_loss=-4.35e+8, train_loss_epoch=-5.02e+8]\n",
            "Epoch 18:  95%|█████████▍| 56/59 [01:04<00:03,  0.87it/s, v_num=0, train_loss_step=-5.81e+8, val_loss=-4.35e+8, train_loss_epoch=-5.02e+8]\n",
            "Epoch 18:  97%|█████████▋| 57/59 [01:05<00:02,  0.87it/s, v_num=0, train_loss_step=-4.37e+8, val_loss=-4.35e+8, train_loss_epoch=-5.02e+8]\n",
            "Epoch 18:  90%|████████▉ | 53/59 [01:01<00:06,  0.86it/s, v_num=0, train_loss_step=-5.42e+8, val_loss=-4.35e+8, train_loss_epoch=-5.02e+8]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "Epoch 18:  98%|█████████▊| 58/59 [01:06<00:01,  0.87it/s, v_num=0, train_loss_step=-5.38e+8, val_loss=-4.35e+8, train_loss_epoch=-5.02e+8]\n",
            "Epoch 18: 100%|██████████| 59/59 [01:07<00:00,  0.88it/s, v_num=0, train_loss_step=-4.27e+8, val_loss=-4.35e+8, train_loss_epoch=-5.02e+8]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:04,  1.52it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:04,  1.43it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:02<00:03,  1.37it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:03<00:03,  1.33it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:03<00:02,  1.30it/s]\u001b[A\n",
            "Epoch 0:  24%|██▎       | 14/59 [01:52<06:02,  0.12it/s, v_num=0, train_loss_step=-0.45] \n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:05<00:01,  1.15it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:06<00:00,  1.05it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:07<00:00,  1.13it/s]\u001b[A\n",
            "Epoch 18: 100%|██████████| 59/59 [01:14<00:00,  0.79it/s, v_num=0, train_loss_step=-4.27e+8, val_loss=-5.23e+8, train_loss_epoch=-5.02e+8]\n",
            "Epoch 18: 100%|██████████| 59/59 [01:15<00:00,  0.78it/s, v_num=0, train_loss_step=-4.27e+8, val_loss=-5.23e+8, train_loss_epoch=-5.24e+8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/8da927e1/checkpoint_000018)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-4.27e+8, val_loss=-5.23e+8, train_loss_epoch=-5.24e+8]\n",
            "Epoch 0:  25%|██▌       | 15/59 [02:01<05:57,  0.12it/s, v_num=0, train_loss_step=-1.33]\n",
            "Trial status: 2 RUNNING | 1 TERMINATED | 1 PENDING\n",
            "Current time: 2025-05-05 11:22:37. Total running time: 24min 2s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-522688640.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1       val_loss |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                           2                     2000                        2                      500       19            1426.25   -5.24423e+08        -4.26681e+08   0.302632   -5.22689e+08 |\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000                                                                                          |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20            1225.47   -4.8756e+08         -5.17014e+08   0.266667   -4.6718e+08  |\n",
            "| TorchTrainer_b3fd7458   PENDING                           4                     2300                        1                      300                                                                                          |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 19:  10%|█         | 6/59 [00:06<00:55,  0.95it/s, v_num=0, train_loss_step=-5.56e+8, val_loss=-5.23e+8, train_loss_epoch=-5.24e+8]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 19:  19%|█▊        | 11/59 [00:11<00:50,  0.95it/s, v_num=0, train_loss_step=-5.63e+8, val_loss=-5.23e+8, train_loss_epoch=-5.24e+8]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 19:  25%|██▌       | 15/59 [00:17<00:51,  0.86it/s, v_num=0, train_loss_step=-4.98e+8, val_loss=-5.23e+8, train_loss_epoch=-5.24e+8]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 19:  34%|███▍      | 20/59 [00:22<00:43,  0.89it/s, v_num=0, train_loss_step=-5.25e+8, val_loss=-5.23e+8, train_loss_epoch=-5.24e+8]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 19:  42%|████▏     | 25/59 [00:28<00:38,  0.88it/s, v_num=0, train_loss_step=-5.39e+8, val_loss=-5.23e+8, train_loss_epoch=-5.24e+8]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Trial status: 2 RUNNING | 1 TERMINATED | 1 PENDING\n",
            "Current time: 2025-05-05 11:23:07. Total running time: 24min 32s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-522688640.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1       val_loss |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                           2                     2000                        2                      500       19            1426.25   -5.24423e+08        -4.26681e+08   0.302632   -5.22689e+08 |\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000                                                                                          |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20            1225.47   -4.8756e+08         -5.17014e+08   0.266667   -4.6718e+08  |\n",
            "| TorchTrainer_b3fd7458   PENDING                           4                     2300                        1                      300                                                                                          |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 19:  49%|████▉     | 29/59 [00:33<00:34,  0.87it/s, v_num=0, train_loss_step=-5.66e+8, val_loss=-5.23e+8, train_loss_epoch=-5.24e+8]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 19:  58%|█████▊    | 34/59 [00:38<00:28,  0.88it/s, v_num=0, train_loss_step=-5.47e+8, val_loss=-5.23e+8, train_loss_epoch=-5.24e+8]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 19:  64%|██████▍   | 38/59 [00:43<00:24,  0.87it/s, v_num=0, train_loss_step=-5.68e+8, val_loss=-5.23e+8, train_loss_epoch=-5.24e+8]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 19:  73%|███████▎  | 43/59 [00:49<00:18,  0.87it/s, v_num=0, train_loss_step=-4.72e+8, val_loss=-5.23e+8, train_loss_epoch=-5.24e+8]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 19:  81%|████████▏ | 48/59 [00:54<00:12,  0.88it/s, v_num=0, train_loss_step=-5.7e+8, val_loss=-5.23e+8, train_loss_epoch=-5.24e+8]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 19:  88%|████████▊ | 52/59 [01:00<00:08,  0.86it/s, v_num=0, train_loss_step=-5.18e+8, val_loss=-5.23e+8, train_loss_epoch=-5.24e+8]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "Epoch 19:  92%|█████████▏| 54/59 [01:02<00:05,  0.86it/s, v_num=0, train_loss_step=-5.22e+8, val_loss=-5.23e+8, train_loss_epoch=-5.24e+8]\n",
            "Trial status: 2 RUNNING | 1 TERMINATED | 1 PENDING\n",
            "Current time: 2025-05-05 11:23:37. Total running time: 25min 2s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-522688640.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1       val_loss |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_8da927e1   RUNNING                           2                     2000                        2                      500       19            1426.25   -5.24423e+08        -4.26681e+08   0.302632   -5.22689e+08 |\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000                                                                                          |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20            1225.47   -4.8756e+08         -5.17014e+08   0.266667   -4.6718e+08  |\n",
            "| TorchTrainer_b3fd7458   PENDING                           4                     2300                        1                      300                                                                                          |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 19:  93%|█████████▎| 55/59 [01:03<00:04,  0.86it/s, v_num=0, train_loss_step=-5.54e+8, val_loss=-5.23e+8, train_loss_epoch=-5.24e+8]\n",
            "Epoch 19:  95%|█████████▍| 56/59 [01:04<00:03,  0.86it/s, v_num=0, train_loss_step=-5.73e+8, val_loss=-5.23e+8, train_loss_epoch=-5.24e+8]\n",
            "Epoch 19:  97%|█████████▋| 57/59 [01:05<00:02,  0.87it/s, v_num=0, train_loss_step=-6.17e+8, val_loss=-5.23e+8, train_loss_epoch=-5.24e+8]\n",
            "Epoch 0:  39%|███▉      | 23/59 [03:04<04:49,  0.12it/s, v_num=0, train_loss_step=-1.27]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "Epoch 19:  98%|█████████▊| 58/59 [01:06<00:01,  0.87it/s, v_num=0, train_loss_step=-5.52e+8, val_loss=-5.23e+8, train_loss_epoch=-5.24e+8]\n",
            "Epoch 19: 100%|██████████| 59/59 [01:07<00:00,  0.87it/s, v_num=0, train_loss_step=-5.85e+8, val_loss=-5.23e+8, train_loss_epoch=-5.24e+8]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:04,  1.56it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:04,  1.41it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:02<00:03,  1.36it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:02<00:02,  1.35it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:03<00:02,  1.34it/s]\u001b[A\n",
            "Epoch 0:  41%|████      | 24/59 [03:11<04:39,  0.13it/s, v_num=0, train_loss_step=-2.81]\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:04<00:01,  1.21it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:06<00:00,  1.09it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:06<00:00,  1.16it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m \n",
            "                                                                      \u001b[A\n",
            "Epoch 19: 100%|██████████| 59/59 [01:14<00:00,  0.79it/s, v_num=0, train_loss_step=-5.85e+8, val_loss=-5.34e+8, train_loss_epoch=-5.24e+8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/8da927e1/checkpoint_000019)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19: 100%|██████████| 59/59 [01:15<00:00,  0.78it/s, v_num=0, train_loss_step=-5.85e+8, val_loss=-5.34e+8, train_loss_epoch=-5.5e+8] \n",
            "Epoch 19: 100%|██████████| 59/59 [01:15<00:00,  0.78it/s, v_num=0, train_loss_step=-5.85e+8, val_loss=-5.34e+8, train_loss_epoch=-5.5e+8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=2908)\u001b[0m `Trainer.fit` stopped: `max_epochs=20` reached.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial TorchTrainer_8da927e1 completed after 20 iterations at 2025-05-05 11:23:51. Total running time: 25min 15s\n",
            "+----------------------------------------------------------+\n",
            "| Trial TorchTrainer_8da927e1 result                       |\n",
            "+----------------------------------------------------------+\n",
            "| checkpoint_dir_name                    checkpoint_000019 |\n",
            "| time_this_iter_s                                75.67562 |\n",
            "| time_total_s                                  1501.92174 |\n",
            "| training_iteration                                    20 |\n",
            "| epoch                                                 19 |\n",
            "| step                                                1180 |\n",
            "| train_loss                                   -550448640. |\n",
            "| train_loss_epoch                             -550448640. |\n",
            "| train_loss_step                              -585040704. |\n",
            "| val/f1                                            0.3787 |\n",
            "| val_loss                                     -533901632. |\n",
            "+----------------------------------------------------------+\n",
            "Epoch 0:  42%|████▏     | 25/59 [03:20<04:32,  0.12it/s, v_num=0, train_loss_step=-2.43]\n",
            "Epoch 0:  44%|████▍     | 26/59 [03:31<04:28,  0.12it/s, v_num=0, train_loss_step=-1.14]\n",
            "\n",
            "Trial status: 2 TERMINATED | 1 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:24:07. Total running time: 25min 32s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1       val_loss |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000                                                                                          |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20            1501.92   -5.50449e+08        -5.85041e+08   0.378698   -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20            1225.47   -4.8756e+08         -5.17014e+08   0.266667   -4.6718e+08  |\n",
            "| TorchTrainer_b3fd7458   PENDING                           4                     2300                        1                      300                                                                                          |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 0:  46%|████▌     | 27/59 [03:38<04:18,  0.12it/s, v_num=0, train_loss_step=-0.434]\n",
            "\n",
            "Trial TorchTrainer_b3fd7458 started with configuration:\n",
            "+---------------------------------------------+\n",
            "| Trial TorchTrainer_b3fd7458 config          |\n",
            "+---------------------------------------------+\n",
            "| train_loop_config/depth                   4 |\n",
            "| train_loop_config/ffn_hidden_dim       2300 |\n",
            "| train_loop_config/ffn_num_layers          1 |\n",
            "| train_loop_config/message_hidden_dim    300 |\n",
            "+---------------------------------------------+\n",
            "Epoch 0:  47%|████▋     | 28/59 [03:46<04:10,  0.12it/s, v_num=0, train_loss_step=-5.16] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(TorchTrainer pid=9344)\u001b[0m Started distributed worker processes: \n",
            "\u001b[36m(TorchTrainer pid=9344)\u001b[0m - (node_id=85d1826776a8b04eb71395ec08130d8b282f617a724b5c6551ccdd9f, ip=172.28.0.12, pid=9465) world_rank=0, local_rank=0, node_rank=0\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m \rEpoch 0:  49%|████▉     | 29/59 [03:53<04:01,  0.12it/s, v_num=0, train_loss_step=-5.16]\rEpoch 0:  49%|████▉     | 29/59 [03:53<04:01,  0.12it/s, v_num=0, train_loss_step=-3.95]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m GPU available: False, used: False\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m \rEpoch 0:  51%|█████     | 30/59 [04:00<03:52,  0.12it/s, v_num=0, train_loss_step=-3.95]\rEpoch 0:  51%|█████     | 30/59 [04:00<03:52,  0.12it/s, v_num=0, train_loss_step=-1.56]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m 2025-05-05 11:24:34.277955: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m E0000 00:00:1746444274.378895    9581 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m E0000 00:00:1746444274.397873    9581 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m 2025-05-05 11:24:34.497433: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:24:37. Total running time: 26min 2s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1       val_loss |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000                                                                                          |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300                                                                                          |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20            1501.92   -5.50449e+08        -5.85041e+08   0.378698   -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20            1225.47   -4.8756e+08         -5.17014e+08   0.266667   -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                          |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n",
            "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m   | Name            | Type                    | Params | Mode \n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m --------------------------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m 0 | message_passing | BondMessagePassing      | 227 K  | train\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m 1 | agg             | MeanAggregation         | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m 2 | bn              | BatchNorm1d             | 600    | train\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m 3 | predictor       | BinaryClassificationFFN | 694 K  | train\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m 4 | X_d_transform   | Identity                | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m 5 | metrics         | ModuleList              | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m --------------------------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m 922 K     Trainable params\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m 0         Non-trainable params\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m 922 K     Total params\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m 3.692     Total estimated model params size (MB)\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m 24        Modules in train mode\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m 0         Modules in eval mode\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m /usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m \rEpoch 0:  53%|█████▎    | 31/59 [04:08<03:44,  0.12it/s, v_num=0, train_loss_step=-1.56]\rEpoch 0:  53%|█████▎    | 31/59 [04:08<03:44,  0.12it/s, v_num=0, train_loss_step=-6.11]\n",
            "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00,  1.29it/s]\n",
            "Epoch 0:   0%|          | 0/59 [00:00<?, ?it/s] \n",
            "Epoch 0:   8%|▊         | 5/59 [00:04<00:49,  1.09it/s, v_num=0, train_loss_step=0.559]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 0:  15%|█▌        | 9/59 [00:10<00:57,  0.87it/s, v_num=0, train_loss_step=0.0545]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 0:  25%|██▌       | 15/59 [00:15<00:45,  0.97it/s, v_num=0, train_loss_step=-0.435] \u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 0:  36%|███▌      | 21/59 [00:21<00:38,  0.98it/s, v_num=0, train_loss_step=0.202]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:25:07. Total running time: 26min 32s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1       val_loss |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000                                                                                          |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300                                                                                          |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20            1501.92   -5.50449e+08        -5.85041e+08   0.378698   -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20            1225.47   -4.8756e+08         -5.17014e+08   0.266667   -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                          |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 0:  44%|████▍     | 26/59 [00:26<00:34,  0.97it/s, v_num=0, train_loss_step=-0.0441]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 0:  54%|█████▍    | 32/59 [00:31<00:26,  1.00it/s, v_num=0, train_loss_step=-0.829]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 0:  61%|██████    | 36/59 [04:46<03:03,  0.13it/s, v_num=0, train_loss_step=-16.1]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 0:  71%|███████   | 42/59 [00:42<00:17,  0.98it/s, v_num=0, train_loss_step=-2.89]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 0:  81%|████████▏ | 48/59 [00:47<00:10,  1.01it/s, v_num=0, train_loss_step=-2.53] \u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 0:  64%|██████▍   | 38/59 [05:02<02:47,  0.13it/s, v_num=0, train_loss_step=-17.4]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 0:  92%|█████████▏| 54/59 [00:54<00:05,  0.98it/s, v_num=0, train_loss_step=-2.38]\n",
            "Epoch 0:  93%|█████████▎| 55/59 [00:55<00:04,  0.99it/s, v_num=0, train_loss_step=-2.40]\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:25:37. Total running time: 27min 2s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step     val/f1       val_loss |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000                                                                                          |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300                                                                                          |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20            1501.92   -5.50449e+08        -5.85041e+08   0.378698   -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20            1225.47   -4.8756e+08         -5.17014e+08   0.266667   -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                          |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 0:  95%|█████████▍| 56/59 [00:56<00:03,  0.99it/s, v_num=0, train_loss_step=-5.72]\n",
            "Epoch 0:  97%|█████████▋| 57/59 [00:57<00:02,  0.99it/s, v_num=0, train_loss_step=-0.936]\n",
            "Epoch 0:  98%|█████████▊| 58/59 [00:58<00:01,  0.99it/s, v_num=0, train_loss_step=-4.41] \n",
            "Epoch 0: 100%|██████████| 59/59 [00:58<00:00,  1.00it/s, v_num=0, train_loss_step=-0.0454]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  90%|████████▉ | 53/59 [00:53<00:06,  0.99it/s, v_num=0, train_loss_step=-2.61] \n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:04,  1.59it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:04,  1.49it/s]\u001b[A\n",
            "Epoch 0:  66%|██████▌   | 39/59 [05:09<02:38,  0.13it/s, v_num=0, train_loss_step=-18.0]\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:02<00:03,  1.44it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:02<00:02,  1.41it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:03<00:02,  1.41it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:04<00:01,  1.40it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:05<00:00,  1.39it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:05<00:00,  1.52it/s]\u001b[A\n",
            "Epoch 0: 100%|██████████| 59/59 [01:04<00:00,  0.92it/s, v_num=0, train_loss_step=-0.0454, val_loss=-1.69]\n",
            "Epoch 1:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-0.0454, val_loss=-1.69, train_loss_epoch=-0.744]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/b3fd7458/checkpoint_000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1:   3%|▎         | 2/59 [00:02<01:09,  0.82it/s, v_num=0, train_loss_step=-0.47, val_loss=-1.69, train_loss_epoch=-0.744]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "Epoch 1:  12%|█▏        | 7/59 [00:08<01:01,  0.85it/s, v_num=0, train_loss_step=-1.99, val_loss=-1.69, train_loss_epoch=-0.744]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 0:  69%|██████▉   | 41/59 [05:26<02:23,  0.13it/s, v_num=0, train_loss_step=-20.2]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 1:  31%|███       | 18/59 [00:19<00:43,  0.95it/s, v_num=0, train_loss_step=-23.4, val_loss=-1.69, train_loss_epoch=-0.744]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:26:07. Total running time: 27min 32s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step      val/f1       val_loss |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000                                                                                           |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300        1             92.329   -0.743864           -0.0453701     0.0672269   -1.68797     |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20           1501.92    -5.50449e+08        -5.85041e+08   0.378698    -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20           1225.47    -4.8756e+08         -5.17014e+08   0.266667    -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                           |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 1:  39%|███▉      | 23/59 [00:24<00:37,  0.95it/s, v_num=0, train_loss_step=-35.6, val_loss=-1.69, train_loss_epoch=-0.744]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 1:  49%|████▉     | 29/59 [00:29<00:30,  0.99it/s, v_num=0, train_loss_step=-38.3, val_loss=-1.69, train_loss_epoch=-0.744]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 1:  58%|█████▊    | 34/59 [00:34<00:25,  0.97it/s, v_num=0, train_loss_step=-73.1, val_loss=-1.69, train_loss_epoch=-0.744]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 1:  68%|██████▊   | 40/59 [00:40<00:19,  0.98it/s, v_num=0, train_loss_step=-99.3, val_loss=-1.69, train_loss_epoch=-0.744]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 1:  78%|███████▊  | 46/59 [00:45<00:12,  1.00it/s, v_num=0, train_loss_step=-77.5, val_loss=-1.69, train_loss_epoch=-0.744]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 1:  85%|████████▍ | 50/59 [00:51<00:09,  0.98it/s, v_num=0, train_loss_step=-237., val_loss=-1.69, train_loss_epoch=-0.744]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:26:37. Total running time: 28min 2s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)     train_loss     train_loss_step      val/f1       val_loss |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000                                                                                           |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300        1             92.329   -0.743864           -0.0453701     0.0672269   -1.68797     |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20           1501.92    -5.50449e+08        -5.85041e+08   0.378698    -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20           1225.47    -4.8756e+08         -5.17014e+08   0.266667    -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                           |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 1:  92%|█████████▏| 54/59 [00:54<00:05,  0.98it/s, v_num=0, train_loss_step=-213., val_loss=-1.69, train_loss_epoch=-0.744]\n",
            "Epoch 1:  93%|█████████▎| 55/59 [00:55<00:04,  0.99it/s, v_num=0, train_loss_step=-92.9, val_loss=-1.69, train_loss_epoch=-0.744]\n",
            "Epoch 1:  95%|█████████▍| 56/59 [00:56<00:03,  0.99it/s, v_num=0, train_loss_step=-299., val_loss=-1.69, train_loss_epoch=-0.744]\n",
            "Epoch 1:  90%|████████▉ | 53/59 [00:53<00:06,  0.98it/s, v_num=0, train_loss_step=-81.2, val_loss=-1.69, train_loss_epoch=-0.744]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "Epoch 1:  97%|█████████▋| 57/59 [00:57<00:02,  0.99it/s, v_num=0, train_loss_step=-377., val_loss=-1.69, train_loss_epoch=-0.744]\n",
            "Epoch 1:  98%|█████████▊| 58/59 [00:58<00:01,  0.99it/s, v_num=0, train_loss_step=-395., val_loss=-1.69, train_loss_epoch=-0.744]\n",
            "Epoch 1: 100%|██████████| 59/59 [00:58<00:00,  1.00it/s, v_num=0, train_loss_step=-318., val_loss=-1.69, train_loss_epoch=-0.744]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:04,  1.64it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:03,  1.52it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:02<00:03,  1.44it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:02<00:02,  1.41it/s]\u001b[A\n",
            "Epoch 0:  80%|███████▉  | 47/59 [06:12<01:35,  0.13it/s, v_num=0, train_loss_step=-44.1]\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:04<00:02,  1.25it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:05<00:01,  1.13it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:06<00:00,  1.05it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:07<00:00,  1.13it/s]\u001b[A\n",
            "Epoch 1: 100%|██████████| 59/59 [01:05<00:00,  0.90it/s, v_num=0, train_loss_step=-318., val_loss=-333., train_loss_epoch=-0.744]\n",
            "Epoch 1: 100%|██████████| 59/59 [01:05<00:00,  0.89it/s, v_num=0, train_loss_step=-318., val_loss=-333., train_loss_epoch=-73.8] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/b3fd7458/checkpoint_000001)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-318., val_loss=-333., train_loss_epoch=-73.8]\n",
            "Epoch 2:   2%|▏         | 1/59 [00:01<01:07,  0.85it/s, v_num=0, train_loss_step=-450., val_loss=-333., train_loss_epoch=-73.8]\n",
            "Epoch 2:  12%|█▏        | 7/59 [00:06<00:47,  1.10it/s, v_num=0, train_loss_step=-295., val_loss=-333., train_loss_epoch=-73.8]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 2:  22%|██▏       | 13/59 [00:12<00:42,  1.08it/s, v_num=0, train_loss_step=-661., val_loss=-333., train_loss_epoch=-73.8]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:27:07. Total running time: 28min 32s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)      train_loss     train_loss_step     val/f1         val_loss |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000                                                                                             |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300        2            158.372   -73.8009           -318.415         0.265957   -333.49        |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20           1501.92     -5.50449e+08        -5.85041e+08   0.378698     -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20           1225.47     -4.8756e+08         -5.17014e+08   0.266667     -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                             |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 2:  29%|██▉       | 17/59 [00:17<00:42,  0.99it/s, v_num=0, train_loss_step=-959., val_loss=-333., train_loss_epoch=-73.8]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "Epoch 2:  39%|███▉      | 23/59 [00:22<00:34,  1.03it/s, v_num=0, train_loss_step=-443., val_loss=-333., train_loss_epoch=-73.8]   \u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 2:  49%|████▉     | 29/59 [00:28<00:29,  1.02it/s, v_num=0, train_loss_step=-1.91e+3, val_loss=-333., train_loss_epoch=-73.8]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 0:  88%|████████▊ | 52/59 [06:53<00:55,  0.13it/s, v_num=0, train_loss_step=105.0]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 2:  68%|██████▊   | 40/59 [00:38<00:18,  1.03it/s, v_num=0, train_loss_step=-2.35e+3, val_loss=-333., train_loss_epoch=-73.8]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 2:  76%|███████▋  | 45/59 [00:44<00:13,  1.01it/s, v_num=0, train_loss_step=-1.78e+3, val_loss=-333., train_loss_epoch=-73.8]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:27:37. Total running time: 29min 2s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)      train_loss     train_loss_step     val/f1         val_loss |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000                                                                                             |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300        2            158.372   -73.8009           -318.415         0.265957   -333.49        |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20           1501.92     -5.50449e+08        -5.85041e+08   0.378698     -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20           1225.47     -4.8756e+08         -5.17014e+08   0.266667     -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                             |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 0:  92%|█████████▏| 54/59 [07:07<00:39,  0.13it/s, v_num=0, train_loss_step=-184.]\n",
            "Epoch 2:  86%|████████▋ | 51/59 [00:50<00:07,  1.01it/s, v_num=0, train_loss_step=-2.23e+3, val_loss=-333., train_loss_epoch=-73.8]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 2:  93%|█████████▎| 55/59 [00:53<00:03,  1.02it/s, v_num=0, train_loss_step=-2.72e+3, val_loss=-333., train_loss_epoch=-73.8]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "Epoch 2:  90%|████████▉ | 53/59 [00:52<00:05,  1.02it/s, v_num=0, train_loss_step=-2.65e+3, val_loss=-333., train_loss_epoch=-73.8]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:01<00:07,  0.91it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:02<00:06,  0.87it/s]\u001b[A\n",
            "Epoch 2: 100%|██████████| 59/59 [00:57<00:00,  1.03it/s, v_num=0, train_loss_step=-4.47e+3, val_loss=-333., train_loss_epoch=-73.8]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:03<00:05,  0.87it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:04<00:04,  0.95it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:04<00:02,  1.01it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:05<00:01,  1.05it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:06<00:00,  1.09it/s]\u001b[A\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/b3fd7458/checkpoint_000002)\n",
            "2025-05-05 11:27:56,723\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 8/8 [00:06<00:00,  1.20it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \r                                                                      \u001b[A\rEpoch 2: 100%|██████████| 59/59 [01:04<00:00,  0.92it/s, v_num=0, train_loss_step=-4.47e+3, val_loss=-1.7e+3, train_loss_epoch=-73.8]\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \rEpoch 2: 100%|██████████| 59/59 [01:04<00:00,  0.92it/s, v_num=0, train_loss_step=-4.47e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-4.47e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:   2%|▏         | 1/59 [00:00<00:50,  1.14it/s, v_num=0, train_loss_step=-3.66e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 0:  95%|█████████▍| 56/59 [07:23<00:23,  0.13it/s, v_num=0, train_loss_step=-342.]\n",
            "Epoch 3:   3%|▎         | 2/59 [00:01<00:50,  1.12it/s, v_num=0, train_loss_step=-4.45e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:   5%|▌         | 3/59 [00:02<00:50,  1.12it/s, v_num=0, train_loss_step=-4.54e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:   7%|▋         | 4/59 [00:03<00:48,  1.13it/s, v_num=0, train_loss_step=-4.39e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:   8%|▊         | 5/59 [00:04<00:47,  1.14it/s, v_num=0, train_loss_step=-4.55e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:  10%|█         | 6/59 [00:05<00:46,  1.15it/s, v_num=0, train_loss_step=-2.95e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:  12%|█▏        | 7/59 [00:06<00:45,  1.14it/s, v_num=0, train_loss_step=-4.68e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 0:  97%|█████████▋| 57/59 [07:30<00:15,  0.13it/s, v_num=0, train_loss_step=-22.0]\n",
            "Epoch 3:  14%|█▎        | 8/59 [00:07<00:45,  1.12it/s, v_num=0, train_loss_step=-5.86e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:  15%|█▌        | 9/59 [00:08<00:48,  1.03it/s, v_num=0, train_loss_step=-4.28e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:  17%|█▋        | 10/59 [00:10<00:49,  0.99it/s, v_num=0, train_loss_step=-3.7e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3] \n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:28:08. Total running time: 29min 32s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)        train_loss     train_loss_step      val/f1          val_loss |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000                                                                                                 |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300        3            222.916   -1425.9             -4474.44          0.0707965   -1698.66        |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20           1501.92       -5.50449e+08        -5.85041e+08   0.378698       -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20           1225.47       -4.8756e+08         -5.17014e+08   0.266667       -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                                 |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 3:  19%|█▊        | 11/59 [00:11<00:49,  0.96it/s, v_num=0, train_loss_step=-3.55e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:  20%|██        | 12/59 [00:12<00:48,  0.97it/s, v_num=0, train_loss_step=-5.82e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:  22%|██▏       | 13/59 [00:13<00:46,  0.98it/s, v_num=0, train_loss_step=-2.67e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:  24%|██▎       | 14/59 [00:14<00:45,  0.99it/s, v_num=0, train_loss_step=-6.79e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:  25%|██▌       | 15/59 [00:14<00:43,  1.00it/s, v_num=0, train_loss_step=-6.12e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 0:  98%|█████████▊| 58/59 [07:39<00:07,  0.13it/s, v_num=0, train_loss_step=-184.]\n",
            "Epoch 3:  27%|██▋       | 16/59 [00:15<00:42,  1.01it/s, v_num=0, train_loss_step=-5.77e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:  29%|██▉       | 17/59 [00:16<00:41,  1.02it/s, v_num=0, train_loss_step=-6.28e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:  31%|███       | 18/59 [00:17<00:40,  1.02it/s, v_num=0, train_loss_step=-3.05e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 0: 100%|██████████| 59/59 [07:42<00:00,  0.13it/s, v_num=0, train_loss_step=-30.6]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3:  32%|███▏      | 19/59 [00:18<00:38,  1.03it/s, v_num=0, train_loss_step=-6.86e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:  34%|███▍      | 20/59 [00:19<00:37,  1.04it/s, v_num=0, train_loss_step=-6.89e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:  36%|███▌      | 21/59 [00:20<00:36,  1.04it/s, v_num=0, train_loss_step=-6.54e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:  37%|███▋      | 22/59 [00:21<00:35,  1.04it/s, v_num=0, train_loss_step=-1.76e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:  39%|███▉      | 23/59 [00:22<00:34,  1.04it/s, v_num=0, train_loss_step=-7.14e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:  41%|████      | 24/59 [00:23<00:34,  1.02it/s, v_num=0, train_loss_step=-7.14e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:  41%|████      | 24/59 [00:23<00:34,  1.02it/s, v_num=0, train_loss_step=-6.13e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:06<00:42,  0.17it/s]\u001b[A\n",
            "Epoch 3:  42%|████▏     | 25/59 [00:25<00:34,  1.00it/s, v_num=0, train_loss_step=-5.42e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:  44%|████▍     | 26/59 [00:26<00:33,  0.99it/s, v_num=0, train_loss_step=-8.22e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:  46%|████▌     | 27/59 [00:27<00:32,  0.98it/s, v_num=0, train_loss_step=-8.34e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:  47%|████▋     | 28/59 [00:28<00:31,  0.99it/s, v_num=0, train_loss_step=-6.4e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3] \n",
            "Epoch 3:  49%|████▉     | 29/59 [00:29<00:30,  0.99it/s, v_num=0, train_loss_step=-2.38e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:  51%|█████     | 30/59 [00:29<00:28,  1.00it/s, v_num=0, train_loss_step=-7.54e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:12<00:36,  0.17it/s]\u001b[A\n",
            "Epoch 3:  53%|█████▎    | 31/59 [00:30<00:27,  1.01it/s, v_num=0, train_loss_step=-2.28e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:  54%|█████▍    | 32/59 [00:31<00:26,  1.01it/s, v_num=0, train_loss_step=-6.53e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:  56%|█████▌    | 33/59 [00:32<00:25,  1.02it/s, v_num=0, train_loss_step=-8.89e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:  58%|█████▊    | 34/59 [00:33<00:24,  1.02it/s, v_num=0, train_loss_step=-8.48e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:  59%|█████▉    | 35/59 [00:34<00:23,  1.03it/s, v_num=0, train_loss_step=-6.18e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:  61%|██████    | 36/59 [00:34<00:22,  1.03it/s, v_num=0, train_loss_step=-8.26e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:  63%|██████▎   | 37/59 [00:35<00:21,  1.03it/s, v_num=0, train_loss_step=-7.18e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:17<00:29,  0.17it/s]\u001b[A\n",
            "Epoch 3:  64%|██████▍   | 38/59 [00:36<00:20,  1.04it/s, v_num=0, train_loss_step=-3.89e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:  66%|██████▌   | 39/59 [00:37<00:19,  1.03it/s, v_num=0, train_loss_step=-7.69e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:  68%|██████▊   | 40/59 [00:39<00:18,  1.01it/s, v_num=0, train_loss_step=-6.02e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:  69%|██████▉   | 41/59 [00:41<00:18,  1.00it/s, v_num=0, train_loss_step=-6.47e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:28:38. Total running time: 30min 2s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)        train_loss     train_loss_step      val/f1          val_loss |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000                                                                                                 |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300        3            222.916   -1425.9             -4474.44          0.0707965   -1698.66        |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20           1501.92       -5.50449e+08        -5.85041e+08   0.378698       -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20           1225.47       -4.8756e+08         -5.17014e+08   0.266667       -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                                 |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 3:  71%|███████   | 42/59 [00:42<00:17,  1.00it/s, v_num=0, train_loss_step=-6.72e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:  73%|███████▎  | 43/59 [00:43<00:16,  1.00it/s, v_num=0, train_loss_step=-1.05e+4, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:24<00:24,  0.16it/s]\u001b[A\n",
            "Epoch 3:  75%|███████▍  | 44/59 [00:43<00:14,  1.00it/s, v_num=0, train_loss_step=-4.37e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:  76%|███████▋  | 45/59 [00:44<00:13,  1.01it/s, v_num=0, train_loss_step=-4.01e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:  78%|███████▊  | 46/59 [00:45<00:12,  1.01it/s, v_num=0, train_loss_step=-5.58e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:  80%|███████▉  | 47/59 [00:46<00:11,  1.01it/s, v_num=0, train_loss_step=-9.14e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:  81%|████████▏ | 48/59 [00:47<00:10,  1.01it/s, v_num=0, train_loss_step=-8.82e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:29<00:17,  0.17it/s]\u001b[A\n",
            "Epoch 3:  83%|████████▎ | 49/59 [00:48<00:09,  1.02it/s, v_num=0, train_loss_step=-8.56e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:  85%|████████▍ | 50/59 [00:49<00:08,  1.02it/s, v_num=0, train_loss_step=-9.16e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:  86%|████████▋ | 51/59 [00:49<00:07,  1.02it/s, v_num=0, train_loss_step=-8.34e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:  88%|████████▊ | 52/59 [00:50<00:06,  1.02it/s, v_num=0, train_loss_step=-1.09e+4, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:  90%|████████▉ | 53/59 [00:51<00:05,  1.03it/s, v_num=0, train_loss_step=-5.9e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3] \n",
            "Epoch 3:  92%|█████████▏| 54/59 [00:53<00:04,  1.01it/s, v_num=0, train_loss_step=-9.63e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:  93%|█████████▎| 55/59 [00:54<00:03,  1.00it/s, v_num=0, train_loss_step=-9.32e+3, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:36<00:12,  0.16it/s]\u001b[A\n",
            "Epoch 3:  95%|█████████▍| 56/59 [00:56<00:03,  1.00it/s, v_num=0, train_loss_step=-1.1e+4, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3] \n",
            "Epoch 3:  97%|█████████▋| 57/59 [00:57<00:02,  0.99it/s, v_num=0, train_loss_step=-1.15e+4, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3:  98%|█████████▊| 58/59 [00:58<00:01,  1.00it/s, v_num=0, train_loss_step=-1.22e+4, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 3: 100%|██████████| 59/59 [00:58<00:00,  1.01it/s, v_num=0, train_loss_step=-1.06e+4, val_loss=-1.7e+3, train_loss_epoch=-1.43e+3]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:02<00:03,  1.43it/s]\u001b[A\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:44<00:00,  0.18it/s]\u001b[A\n",
            "Epoch 0: 100%|██████████| 59/59 [08:26<00:00,  0.12it/s, v_num=0, train_loss_step=-30.6, val_loss=-399.]\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/b3fd7458/checkpoint_000003)\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/33384398/checkpoint_000000)\n",
            "2025-05-05 11:29:04,301\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \rEpoch 3: 100%|██████████| 59/59 [01:07<00:00,  0.88it/s, v_num=0, train_loss_step=-1.06e+4, val_loss=-7.32e+3, train_loss_epoch=-6.49e+3]\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \rValidation DataLoader 0:  88%|████████▊ | 7/8 [00:04<00:00,  1.44it/s]\u001b[A\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 8/8 [00:05<00:00,  1.60it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \r                                                                      \u001b[A\rEpoch 3: 100%|██████████| 59/59 [01:03<00:00,  0.93it/s, v_num=0, train_loss_step=-1.06e+4, val_loss=-7.32e+3, train_loss_epoch=-1.43e+3]\n",
            "Epoch 4:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-1.06e+4, val_loss=-7.32e+3, train_loss_epoch=-6.49e+3]\n",
            "Epoch 4:   2%|▏         | 1/59 [00:01<01:29,  0.64it/s, v_num=0, train_loss_step=-1.41e+4, val_loss=-7.32e+3, train_loss_epoch=-6.49e+3]\n",
            "Epoch 4:   3%|▎         | 2/59 [00:02<01:25,  0.67it/s, v_num=0, train_loss_step=-1.47e+4, val_loss=-7.32e+3, train_loss_epoch=-6.49e+3]\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:29:08. Total running time: 30min 32s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)        train_loss     train_loss_step     val/f1          val_loss |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000        1            550.186     -27.2132            -30.5904        0           -398.654       |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300        4            290.367   -6485.99           -10622.4           0.128      -7324.98        |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20           1501.92       -5.50449e+08        -5.85041e+08   0.378698      -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20           1225.47       -4.8756e+08         -5.17014e+08   0.266667      -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                                |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 4:   5%|▌         | 3/59 [00:04<01:17,  0.72it/s, v_num=0, train_loss_step=-1.18e+4, val_loss=-7.32e+3, train_loss_epoch=-6.49e+3]\n",
            "Epoch 4:   7%|▋         | 4/59 [00:05<01:09,  0.79it/s, v_num=0, train_loss_step=-1.34e+4, val_loss=-7.32e+3, train_loss_epoch=-6.49e+3]\n",
            "Epoch 0: 100%|██████████| 59/59 [08:31<00:00,  0.12it/s, v_num=0, train_loss_step=-30.6, val_loss=-399., train_loss_epoch=-27.2]\n",
            "Epoch 1:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-30.6, val_loss=-399., train_loss_epoch=-27.2]\n",
            "Epoch 4:   8%|▊         | 5/59 [00:05<01:04,  0.84it/s, v_num=0, train_loss_step=-1.12e+4, val_loss=-7.32e+3, train_loss_epoch=-6.49e+3]\n",
            "Epoch 4:  10%|█         | 6/59 [00:06<01:00,  0.88it/s, v_num=0, train_loss_step=-4.01e+3, val_loss=-7.32e+3, train_loss_epoch=-6.49e+3]\n",
            "Epoch 4:  12%|█▏        | 7/59 [00:07<00:57,  0.91it/s, v_num=0, train_loss_step=-7.66e+3, val_loss=-7.32e+3, train_loss_epoch=-6.49e+3]\n",
            "Epoch 4:  14%|█▎        | 8/59 [00:08<00:54,  0.93it/s, v_num=0, train_loss_step=-1.15e+4, val_loss=-7.32e+3, train_loss_epoch=-6.49e+3]\n",
            "Epoch 4:  22%|██▏       | 13/59 [00:12<00:45,  1.02it/s, v_num=0, train_loss_step=-1.37e+4, val_loss=-7.32e+3, train_loss_epoch=-6.49e+3]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 4:  31%|███       | 18/59 [00:18<00:42,  0.96it/s, v_num=0, train_loss_step=-1.43e+4, val_loss=-7.32e+3, train_loss_epoch=-6.49e+3]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 4:  41%|████      | 24/59 [00:24<00:35,  1.00it/s, v_num=0, train_loss_step=-1.03e+4, val_loss=-7.32e+3, train_loss_epoch=-6.49e+3]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 4:  51%|█████     | 30/59 [00:29<00:28,  1.02it/s, v_num=0, train_loss_step=-1.11e+4, val_loss=-7.32e+3, train_loss_epoch=-6.49e+3]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:29:38. Total running time: 31min 2s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)        train_loss     train_loss_step     val/f1          val_loss |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000        1            550.186     -27.2132            -30.5904        0           -398.654       |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300        4            290.367   -6485.99           -10622.4           0.128      -7324.98        |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20           1501.92       -5.50449e+08        -5.85041e+08   0.378698      -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20           1225.47       -4.8756e+08         -5.17014e+08   0.266667      -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                                |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 4:  58%|█████▊    | 34/59 [00:34<00:25,  0.98it/s, v_num=0, train_loss_step=-1.92e+4, val_loss=-7.32e+3, train_loss_epoch=-6.49e+3]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 1:   8%|▊         | 5/59 [00:38<07:00,  0.13it/s, v_num=0, train_loss_step=-106., val_loss=-399., train_loss_epoch=-27.2]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 4:  78%|███████▊  | 46/59 [00:45<00:12,  1.00it/s, v_num=0, train_loss_step=-2.1e+4, val_loss=-7.32e+3, train_loss_epoch=-6.49e+3] \u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 4:  86%|████████▋ | 51/59 [00:51<00:08,  0.99it/s, v_num=0, train_loss_step=-2.05e+4, val_loss=-7.32e+3, train_loss_epoch=-6.49e+3]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 4:  92%|█████████▏| 54/59 [00:53<00:04,  1.00it/s, v_num=0, train_loss_step=-1.96e+4, val_loss=-7.32e+3, train_loss_epoch=-6.49e+3]\n",
            "Epoch 4:  93%|█████████▎| 55/59 [00:54<00:03,  1.00it/s, v_num=0, train_loss_step=-1.9e+4, val_loss=-7.32e+3, train_loss_epoch=-6.49e+3] \n",
            "Epoch 4:  95%|█████████▍| 56/59 [00:55<00:02,  1.00it/s, v_num=0, train_loss_step=-2.16e+4, val_loss=-7.32e+3, train_loss_epoch=-6.49e+3]\n",
            "Epoch 1:  12%|█▏        | 7/59 [00:55<06:53,  0.13it/s, v_num=0, train_loss_step=222.0, val_loss=-399., train_loss_epoch=-27.2]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "Epoch 4:  97%|█████████▋| 57/59 [00:56<00:01,  1.01it/s, v_num=0, train_loss_step=-2.05e+4, val_loss=-7.32e+3, train_loss_epoch=-6.49e+3]\n",
            "Epoch 4:  98%|█████████▊| 58/59 [00:57<00:00,  1.01it/s, v_num=0, train_loss_step=-9.19e+3, val_loss=-7.32e+3, train_loss_epoch=-6.49e+3]\n",
            "Epoch 4: 100%|██████████| 59/59 [00:58<00:00,  1.02it/s, v_num=0, train_loss_step=-28944.5, val_loss=-7.32e+3, train_loss_epoch=-6.49e+3]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:04,  1.65it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:04,  1.44it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:02<00:04,  1.07it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:04<00:04,  0.98it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:05<00:03,  0.95it/s]\u001b[A\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:30:08. Total running time: 31min 32s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)        train_loss     train_loss_step     val/f1          val_loss |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000        1            550.186     -27.2132            -30.5904        0           -398.654       |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300        4            290.367   -6485.99           -10622.4           0.128      -7324.98        |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20           1501.92       -5.50449e+08        -5.85041e+08   0.378698      -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20           1225.47       -4.8756e+08         -5.17014e+08   0.266667      -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                                |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:06<00:02,  0.93it/s]\u001b[A\n",
            "Epoch 1:  14%|█▎        | 8/59 [01:04<06:51,  0.12it/s, v_num=0, train_loss_step=-548., val_loss=-399., train_loss_epoch=-27.2]\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:07<00:01,  0.98it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:07<00:00,  1.08it/s]\u001b[A\n",
            "Epoch 4: 100%|██████████| 59/59 [01:05<00:00,  0.90it/s, v_num=0, train_loss_step=-28944.5, val_loss=-2e+4, train_loss_epoch=-1.49e+4]\n",
            "Epoch 5:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-28944.5, val_loss=-2e+4, train_loss_epoch=-1.49e+4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/b3fd7458/checkpoint_000004)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5:  10%|█         | 6/59 [00:05<00:47,  1.13it/s, v_num=0, train_loss_step=-2.94e+4, val_loss=-2e+4, train_loss_epoch=-1.49e+4]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 5:  20%|██        | 12/59 [00:11<00:45,  1.04it/s, v_num=0, train_loss_step=-2.59e+4, val_loss=-2e+4, train_loss_epoch=-1.49e+4]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 5:  29%|██▉       | 17/59 [00:16<00:41,  1.01it/s, v_num=0, train_loss_step=-1.98e+4, val_loss=-2e+4, train_loss_epoch=-1.49e+4]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 1:  19%|█▊        | 11/59 [01:27<06:19,  0.13it/s, v_num=0, train_loss_step=-424., val_loss=-399., train_loss_epoch=-27.2]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 5:  47%|████▋     | 28/59 [00:28<00:31,  1.00it/s, v_num=0, train_loss_step=-3.14e+4, val_loss=-2e+4, train_loss_epoch=-1.49e+4]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:30:38. Total running time: 32min 3s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)         train_loss     train_loss_step      val/f1           val_loss |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000        1            550.186      -27.2132            -30.5904        0             -398.654       |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300        5            356.145   -14863.6            -28944.5           0.0862069   -20016.4         |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20           1501.92        -5.50449e+08        -5.85041e+08   0.378698        -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20           1225.47        -4.8756e+08         -5.17014e+08   0.266667        -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                                   |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 5:  58%|█████▊    | 34/59 [00:33<00:24,  1.01it/s, v_num=0, train_loss_step=-3.23e+4, val_loss=-2e+4, train_loss_epoch=-1.49e+4]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 5:  68%|██████▊   | 40/59 [00:39<00:18,  1.03it/s, v_num=0, train_loss_step=-2.93e+4, val_loss=-2e+4, train_loss_epoch=-1.49e+4]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 5:  75%|███████▍  | 44/59 [00:44<00:15,  0.99it/s, v_num=0, train_loss_step=-3.85e+4, val_loss=-2e+4, train_loss_epoch=-1.49e+4]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "Epoch 5:  85%|████████▍ | 50/59 [00:49<00:08,  1.01it/s, v_num=0, train_loss_step=-3.74e+4, val_loss=-2e+4, train_loss_epoch=-1.49e+4]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 5:  92%|█████████▏| 54/59 [00:52<00:04,  1.02it/s, v_num=0, train_loss_step=-2.81e+4, val_loss=-2e+4, train_loss_epoch=-1.49e+4]\n",
            "Epoch 5:  93%|█████████▎| 55/59 [00:53<00:03,  1.03it/s, v_num=0, train_loss_step=-4.02e+4, val_loss=-2e+4, train_loss_epoch=-1.49e+4]\n",
            "Epoch 5:  95%|█████████▍| 56/59 [00:54<00:02,  1.03it/s, v_num=0, train_loss_step=-3.11e+4, val_loss=-2e+4, train_loss_epoch=-1.49e+4]\n",
            "Epoch 1:  25%|██▌       | 15/59 [01:58<05:48,  0.13it/s, v_num=0, train_loss_step=-1e+3, val_loss=-399., train_loss_epoch=-27.2]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "Epoch 5:  97%|█████████▋| 57/59 [00:55<00:01,  1.02it/s, v_num=0, train_loss_step=-4.07e+4, val_loss=-2e+4, train_loss_epoch=-1.49e+4]\n",
            "Epoch 5:  98%|█████████▊| 58/59 [00:57<00:00,  1.01it/s, v_num=0, train_loss_step=-3.78e+4, val_loss=-2e+4, train_loss_epoch=-1.49e+4]\n",
            "Epoch 5: 100%|██████████| 59/59 [00:58<00:00,  1.02it/s, v_num=0, train_loss_step=-3.63e+4, val_loss=-2e+4, train_loss_epoch=-1.49e+4]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:31:08. Total running time: 32min 33s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)         train_loss     train_loss_step      val/f1           val_loss |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000        1            550.186      -27.2132            -30.5904        0             -398.654       |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300        5            356.145   -14863.6            -28944.5           0.0862069   -20016.4         |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20           1501.92        -5.50449e+08        -5.85041e+08   0.378698        -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20           1225.47        -4.8756e+08         -5.17014e+08   0.266667        -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                                   |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:06,  1.12it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:05,  1.13it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:02<00:04,  1.18it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:03<00:03,  1.20it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:04<00:02,  1.24it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:04<00:01,  1.26it/s]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 16/59 [02:08<05:44,  0.12it/s, v_num=0, train_loss_step=-746., val_loss=-399., train_loss_epoch=-27.2]\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:05<00:00,  1.28it/s]\u001b[A\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/b3fd7458/checkpoint_000005)\n",
            "2025-05-05 11:31:14,232\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 8/8 [00:05<00:00,  1.39it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \r                                                                      \u001b[A\rEpoch 5: 100%|██████████| 59/59 [01:04<00:00,  0.92it/s, v_num=0, train_loss_step=-3.63e+4, val_loss=-3.56e+4, train_loss_epoch=-1.49e+4]\rEpoch 5: 100%|██████████| 59/59 [01:04<00:00,  0.92it/s, v_num=0, train_loss_step=-3.63e+4, val_loss=-3.56e+4, train_loss_epoch=-2.93e+4]\n",
            "Epoch 6:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-3.63e+4, val_loss=-3.56e+4, train_loss_epoch=-2.93e+4]\n",
            "Epoch 6:   8%|▊         | 5/59 [00:04<00:46,  1.17it/s, v_num=0, train_loss_step=-4.12e+4, val_loss=-3.56e+4, train_loss_epoch=-2.93e+4]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 6:  17%|█▋        | 10/59 [00:10<00:51,  0.95it/s, v_num=0, train_loss_step=-4.55e+4, val_loss=-3.56e+4, train_loss_epoch=-2.93e+4]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 6:  27%|██▋       | 16/59 [00:15<00:42,  1.02it/s, v_num=0, train_loss_step=-3.09e+4, val_loss=-3.56e+4, train_loss_epoch=-2.93e+4]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 6:  37%|███▋      | 22/59 [00:21<00:35,  1.05it/s, v_num=0, train_loss_step=-4.53e+4, val_loss=-3.56e+4, train_loss_epoch=-2.93e+4]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:31:38. Total running time: 33min 3s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)         train_loss     train_loss_step     val/f1           val_loss |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000        1            550.186      -27.2132            -30.5904        0            -398.654       |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300        6            420.329   -29313.9            -36267.1           0.119658   -35611.6         |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20           1501.92        -5.50449e+08        -5.85041e+08   0.378698       -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20           1225.47        -4.8756e+08         -5.17014e+08   0.266667       -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                                  |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 6:  44%|████▍     | 26/59 [00:26<00:33,  0.99it/s, v_num=0, train_loss_step=-4.98e+4, val_loss=-3.56e+4, train_loss_epoch=-2.93e+4]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 6:  54%|█████▍    | 32/59 [00:31<00:26,  1.02it/s, v_num=0, train_loss_step=-5.41e+4, val_loss=-3.56e+4, train_loss_epoch=-2.93e+4]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 6:  63%|██████▎   | 37/59 [00:36<00:21,  1.01it/s, v_num=0, train_loss_step=-5.16e+4, val_loss=-3.56e+4, train_loss_epoch=-2.93e+4]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 6:  71%|███████   | 42/59 [00:42<00:17,  1.00it/s, v_num=0, train_loss_step=-2.76e+4, val_loss=-3.56e+4, train_loss_epoch=-2.93e+4]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 6:  81%|████████▏ | 48/59 [00:47<00:10,  1.01it/s, v_num=0, train_loss_step=-3.34e+4, val_loss=-3.56e+4, train_loss_epoch=-2.93e+4]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 6:  92%|█████████▏| 54/59 [00:53<00:04,  1.00it/s, v_num=0, train_loss_step=-5.18e+4, val_loss=-3.56e+4, train_loss_epoch=-2.93e+4]\n",
            "Epoch 6:  90%|████████▉ | 53/59 [00:52<00:05,  1.01it/s, v_num=0, train_loss_step=-5.74e+4, val_loss=-3.56e+4, train_loss_epoch=-2.93e+4]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:32:08. Total running time: 33min 33s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)         train_loss     train_loss_step     val/f1           val_loss |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000        1            550.186      -27.2132            -30.5904        0            -398.654       |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300        6            420.329   -29313.9            -36267.1           0.119658   -35611.6         |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20           1501.92        -5.50449e+08        -5.85041e+08   0.378698       -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20           1225.47        -4.8756e+08         -5.17014e+08   0.266667       -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                                  |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 6:  93%|█████████▎| 55/59 [00:55<00:04,  1.00it/s, v_num=0, train_loss_step=-5.86e+4, val_loss=-3.56e+4, train_loss_epoch=-2.93e+4]\n",
            "Epoch 6:  95%|█████████▍| 56/59 [00:56<00:03,  0.99it/s, v_num=0, train_loss_step=-6.54e+4, val_loss=-3.56e+4, train_loss_epoch=-2.93e+4]\n",
            "Epoch 6:  97%|█████████▋| 57/59 [00:57<00:02,  1.00it/s, v_num=0, train_loss_step=-6.66e+4, val_loss=-3.56e+4, train_loss_epoch=-2.93e+4]\n",
            "Epoch 6:  98%|█████████▊| 58/59 [00:57<00:00,  1.00it/s, v_num=0, train_loss_step=-5.46e+4, val_loss=-3.56e+4, train_loss_epoch=-2.93e+4]\n",
            "Epoch 6: 100%|██████████| 59/59 [00:58<00:00,  1.01it/s, v_num=0, train_loss_step=-6.52e+4, val_loss=-3.56e+4, train_loss_epoch=-2.93e+4]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:03,  1.76it/s]\u001b[A\n",
            "Epoch 1:  39%|███▉      | 23/59 [03:03<04:47,  0.13it/s, v_num=0, train_loss_step=-2.71e+3, val_loss=-399., train_loss_epoch=-27.2]\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:03,  1.51it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:02<00:03,  1.40it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:02<00:02,  1.40it/s]\u001b[A\n",
            "Epoch 1:  41%|████      | 24/59 [03:10<04:38,  0.13it/s, v_num=0, train_loss_step=-3.73e+3, val_loss=-399., train_loss_epoch=-27.2]\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:03<00:02,  1.40it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:04<00:01,  1.38it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:05<00:00,  1.39it/s]\u001b[A\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-05 11:32:18,194\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 8/8 [00:05<00:00,  1.51it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \r                                                                      \u001b[A\rEpoch 6: 100%|██████████| 59/59 [01:03<00:00,  0.92it/s, v_num=0, train_loss_step=-6.52e+4, val_loss=-4.61e+4, train_loss_epoch=-2.93e+4]\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \rEpoch 6: 100%|██████████| 59/59 [01:03<00:00,  0.92it/s, v_num=0, train_loss_step=-6.52e+4, val_loss=-4.61e+4, train_loss_epoch=-4.73e+4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/b3fd7458/checkpoint_000006)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-6.52e+4, val_loss=-4.61e+4, train_loss_epoch=-4.73e+4]\n",
            "Epoch 7:   5%|▌         | 3/59 [00:03<00:59,  0.95it/s, v_num=0, train_loss_step=-4.53e+4, val_loss=-4.61e+4, train_loss_epoch=-4.73e+4]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "Epoch 7:  14%|█▎        | 8/59 [00:08<00:56,  0.90it/s, v_num=0, train_loss_step=-7.11e+4, val_loss=-4.61e+4, train_loss_epoch=-4.73e+4]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 7:  24%|██▎       | 14/59 [00:14<00:45,  0.99it/s, v_num=0, train_loss_step=-6.44e+4, val_loss=-4.61e+4, train_loss_epoch=-4.73e+4]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 7:  32%|███▏      | 19/59 [00:19<00:41,  0.98it/s, v_num=0, train_loss_step=-6.44e+4, val_loss=-4.61e+4, train_loss_epoch=-4.73e+4]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:32:38. Total running time: 34min 3s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)         train_loss     train_loss_step      val/f1           val_loss |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000        1            550.186      -27.2132            -30.5904        0             -398.654       |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300        7            484.284   -47343.7            -65206.1           0.0373832   -46144.9         |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20           1501.92        -5.50449e+08        -5.85041e+08   0.378698        -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20           1225.47        -4.8756e+08         -5.17014e+08   0.266667        -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                                   |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 7:  41%|████      | 24/59 [00:24<00:35,  0.97it/s, v_num=0, train_loss_step=-6.68e+4, val_loss=-4.61e+4, train_loss_epoch=-4.73e+4]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 7:  51%|█████     | 30/59 [00:29<00:28,  1.01it/s, v_num=0, train_loss_step=-7.55e+4, val_loss=-4.61e+4, train_loss_epoch=-4.73e+4]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 7:  59%|█████▉    | 35/59 [00:35<00:24,  0.98it/s, v_num=0, train_loss_step=-6.76e+4, val_loss=-4.61e+4, train_loss_epoch=-4.73e+4]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 7:  69%|██████▉   | 41/59 [00:41<00:18,  0.99it/s, v_num=0, train_loss_step=-6.22e+4, val_loss=-4.61e+4, train_loss_epoch=-4.73e+4]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 7:  80%|███████▉  | 47/59 [00:46<00:11,  1.01it/s, v_num=0, train_loss_step=-5.04e+4, val_loss=-4.61e+4, train_loss_epoch=-4.73e+4]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:33:08. Total running time: 34min 33s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)         train_loss     train_loss_step      val/f1           val_loss |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000        1            550.186      -27.2132            -30.5904        0             -398.654       |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300        7            484.284   -47343.7            -65206.1           0.0373832   -46144.9         |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20           1501.92        -5.50449e+08        -5.85041e+08   0.378698        -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20           1225.47        -4.8756e+08         -5.17014e+08   0.266667        -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                                   |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 7:  86%|████████▋ | 51/59 [00:51<00:08,  0.99it/s, v_num=0, train_loss_step=-7.69e+4, val_loss=-4.61e+4, train_loss_epoch=-4.73e+4]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "Epoch 7:  92%|█████████▏| 54/59 [00:54<00:05,  0.99it/s, v_num=0, train_loss_step=-7.51e+4, val_loss=-4.61e+4, train_loss_epoch=-4.73e+4]\n",
            "Epoch 7:  93%|█████████▎| 55/59 [00:55<00:04,  1.00it/s, v_num=0, train_loss_step=-8.12e+4, val_loss=-4.61e+4, train_loss_epoch=-4.73e+4]\n",
            "Epoch 7:  95%|█████████▍| 56/59 [00:56<00:03,  1.00it/s, v_num=0, train_loss_step=-8.52e+4, val_loss=-4.61e+4, train_loss_epoch=-4.73e+4]\n",
            "Epoch 7:  97%|█████████▋| 57/59 [00:56<00:01,  1.00it/s, v_num=0, train_loss_step=-7.62e+4, val_loss=-4.61e+4, train_loss_epoch=-4.73e+4]\n",
            "Epoch 7:  90%|████████▉ | 53/59 [00:53<00:06,  0.99it/s, v_num=0, train_loss_step=-7.74e+4, val_loss=-4.61e+4, train_loss_epoch=-4.73e+4]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "Epoch 7:  98%|█████████▊| 58/59 [00:57<00:00,  1.00it/s, v_num=0, train_loss_step=-8.72e+4, val_loss=-4.61e+4, train_loss_epoch=-4.73e+4]\n",
            "Epoch 7: 100%|██████████| 59/59 [00:58<00:00,  1.01it/s, v_num=0, train_loss_step=-6.93e+4, val_loss=-4.61e+4, train_loss_epoch=-4.73e+4]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:04,  1.57it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:04,  1.47it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:02<00:03,  1.42it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:02<00:02,  1.38it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:03<00:02,  1.38it/s]\u001b[A\n",
            "Epoch 1:  54%|█████▍    | 32/59 [04:12<03:33,  0.13it/s, v_num=0, train_loss_step=-4.58e+3, val_loss=-399., train_loss_epoch=-27.2]\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:04<00:01,  1.23it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:06<00:00,  1.13it/s]\u001b[A\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/b3fd7458/checkpoint_000007)\n",
            "2025-05-05 11:33:23,380\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 8/8 [00:06<00:00,  1.21it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \r                                                                      \u001b[A\rEpoch 7: 100%|██████████| 59/59 [01:04<00:00,  0.91it/s, v_num=0, train_loss_step=-6.93e+4, val_loss=-9.03e+4, train_loss_epoch=-4.73e+4]\n",
            "Epoch 7: 100%|██████████| 59/59 [01:05<00:00,  0.91it/s, v_num=0, train_loss_step=-6.93e+4, val_loss=-9.03e+4, train_loss_epoch=-6.69e+4]\n",
            "Epoch 8:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-6.93e+4, val_loss=-9.03e+4, train_loss_epoch=-6.69e+4]\n",
            "Epoch 8:   2%|▏         | 1/59 [00:01<01:14,  0.78it/s, v_num=0, train_loss_step=-7.07e+4, val_loss=-9.03e+4, train_loss_epoch=-6.69e+4]\n",
            "Epoch 8:   3%|▎         | 2/59 [00:02<01:10,  0.81it/s, v_num=0, train_loss_step=-8.67e+4, val_loss=-9.03e+4, train_loss_epoch=-6.69e+4]\n",
            "Epoch 8:  12%|█▏        | 7/59 [00:06<00:51,  1.01it/s, v_num=0, train_loss_step=-8.36e+4, val_loss=-9.03e+4, train_loss_epoch=-6.69e+4]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 8:  22%|██▏       | 13/59 [00:12<00:43,  1.07it/s, v_num=0, train_loss_step=-9.37e+4, val_loss=-9.03e+4, train_loss_epoch=-6.69e+4]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:33:38. Total running time: 35min 3s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)         train_loss     train_loss_step     val/f1           val_loss |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000        1            550.186      -27.2132            -30.5904        0            -398.654       |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300        8            549.441   -66863.2            -69316.3           0.534562   -90315.4         |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20           1501.92        -5.50449e+08        -5.85041e+08   0.378698       -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20           1225.47        -4.8756e+08         -5.17014e+08   0.266667       -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                                  |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 8:  29%|██▉       | 17/59 [00:17<00:42,  0.98it/s, v_num=0, train_loss_step=-9.21e+4, val_loss=-9.03e+4, train_loss_epoch=-6.69e+4]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "Epoch 8:  39%|███▉      | 23/59 [00:22<00:35,  1.02it/s, v_num=0, train_loss_step=-9.46e+4, val_loss=-9.03e+4, train_loss_epoch=-6.69e+4]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 8:  49%|████▉     | 29/59 [00:28<00:29,  1.03it/s, v_num=0, train_loss_step=-9.65e+4, val_loss=-9.03e+4, train_loss_epoch=-6.69e+4]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 8:  56%|█████▌    | 33/59 [00:33<00:26,  0.99it/s, v_num=0, train_loss_step=-8.28e+4, val_loss=-9.03e+4, train_loss_epoch=-6.69e+4]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "Epoch 8:  66%|██████▌   | 39/59 [00:38<00:19,  1.02it/s, v_num=0, train_loss_step=-9.49e+4, val_loss=-9.03e+4, train_loss_epoch=-6.69e+4]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 8:  76%|███████▋  | 45/59 [00:44<00:13,  1.00it/s, v_num=0, train_loss_step=-1.07e+5, val_loss=-9.03e+4, train_loss_epoch=-6.69e+4]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:34:08. Total running time: 35min 33s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)         train_loss     train_loss_step     val/f1           val_loss |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000        1            550.186      -27.2132            -30.5904        0            -398.654       |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300        8            549.441   -66863.2            -69316.3           0.534562   -90315.4         |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20           1501.92        -5.50449e+08        -5.85041e+08   0.378698       -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20           1225.47        -4.8756e+08         -5.17014e+08   0.266667       -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                                  |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 8:  86%|████████▋ | 51/59 [00:50<00:07,  1.01it/s, v_num=0, train_loss_step=-8.92e+4, val_loss=-9.03e+4, train_loss_epoch=-6.69e+4]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 8:  92%|█████████▏| 54/59 [00:53<00:04,  1.02it/s, v_num=0, train_loss_step=-8.63e+4, val_loss=-9.03e+4, train_loss_epoch=-6.69e+4]\n",
            "Epoch 8:  93%|█████████▎| 55/59 [00:53<00:03,  1.02it/s, v_num=0, train_loss_step=-9.24e+4, val_loss=-9.03e+4, train_loss_epoch=-6.69e+4]\n",
            "Epoch 8:  95%|█████████▍| 56/59 [00:54<00:02,  1.02it/s, v_num=0, train_loss_step=-1.07e+5, val_loss=-9.03e+4, train_loss_epoch=-6.69e+4]\n",
            "Epoch 8:  97%|█████████▋| 57/59 [00:55<00:01,  1.02it/s, v_num=0, train_loss_step=-1.04e+5, val_loss=-9.03e+4, train_loss_epoch=-6.69e+4]\n",
            "Epoch 8:  90%|████████▉ | 53/59 [00:52<00:05,  1.02it/s, v_num=0, train_loss_step=-1.01e+5, val_loss=-9.03e+4, train_loss_epoch=-6.69e+4]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "Epoch 8:  98%|█████████▊| 58/59 [00:56<00:00,  1.03it/s, v_num=0, train_loss_step=-9.45e+4, val_loss=-9.03e+4, train_loss_epoch=-6.69e+4]\n",
            "Epoch 8: 100%|██████████| 59/59 [00:56<00:00,  1.04it/s, v_num=0, train_loss_step=-1.11e+5, val_loss=-9.03e+4, train_loss_epoch=-6.69e+4]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:06,  1.06it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:02<00:06,  0.88it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:03<00:06,  0.83it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:04<00:04,  0.82it/s]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 40/59 [05:14<02:29,  0.13it/s, v_num=0, train_loss_step=-7.99e+3, val_loss=-399., train_loss_epoch=-27.2]\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:05<00:03,  0.89it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:06<00:02,  0.94it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:07<00:01,  0.98it/s]\u001b[A\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-05 11:34:27,942\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 8/8 [00:07<00:00,  1.08it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \r                                                                      \u001b[A\rEpoch 8: 100%|██████████| 59/59 [01:04<00:00,  0.92it/s, v_num=0, train_loss_step=-1.11e+5, val_loss=-1.06e+5, train_loss_epoch=-6.69e+4]\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \rEpoch 8: 100%|██████████| 59/59 [01:04<00:00,  0.92it/s, v_num=0, train_loss_step=-1.11e+5, val_loss=-1.06e+5, train_loss_epoch=-9.14e+4]\rEpoch 8:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-1.11e+5, val_loss=-1.06e+5, train_loss_epoch=-9.14e+4]         \rEpoch 9:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-1.11e+5, val_loss=-1.06e+5, train_loss_epoch=-9.14e+4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/b3fd7458/checkpoint_000008)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1:  69%|██████▉   | 41/59 [05:23<02:21,  0.13it/s, v_num=0, train_loss_step=-5.62e+3, val_loss=-399., train_loss_epoch=-27.2]\n",
            "Epoch 9:  12%|█▏        | 7/59 [00:05<00:43,  1.19it/s, v_num=0, train_loss_step=-1.09e+5, val_loss=-1.06e+5, train_loss_epoch=-9.14e+4]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:34:38. Total running time: 36min 3s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)         train_loss     train_loss_step     val/f1            val_loss |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000        1            550.186      -27.2132            -30.5904        0             -398.654       |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300        9            613.986   -91360.8           -110866             0.43299    -105624           |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20           1501.92        -5.50449e+08        -5.85041e+08   0.378698        -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20           1225.47        -4.8756e+08         -5.17014e+08   0.266667        -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                                   |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 9:  20%|██        | 12/59 [00:12<00:47,  0.99it/s, v_num=0, train_loss_step=-1.05e+5, val_loss=-1.06e+5, train_loss_epoch=-9.14e+4]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 9:  31%|███       | 18/59 [00:17<00:39,  1.03it/s, v_num=0, train_loss_step=-8.47e+4, val_loss=-1.06e+5, train_loss_epoch=-9.14e+4]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 9:  41%|████      | 24/59 [00:22<00:33,  1.06it/s, v_num=0, train_loss_step=-1.19e+5, val_loss=-1.06e+5, train_loss_epoch=-9.14e+4]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 9:  47%|████▋     | 28/59 [00:27<00:30,  1.00it/s, v_num=0, train_loss_step=-1.09e+5, val_loss=-1.06e+5, train_loss_epoch=-9.14e+4]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 9:  58%|█████▊    | 34/59 [00:33<00:24,  1.02it/s, v_num=0, train_loss_step=-1.24e+5, val_loss=-1.06e+5, train_loss_epoch=-9.14e+4]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 9:  68%|██████▊   | 40/59 [00:39<00:18,  1.02it/s, v_num=0, train_loss_step=-1.24e+5, val_loss=-1.06e+5, train_loss_epoch=-9.14e+4]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:35:08. Total running time: 36min 33s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)         train_loss     train_loss_step     val/f1            val_loss |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000        1            550.186      -27.2132            -30.5904        0             -398.654       |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300        9            613.986   -91360.8           -110866             0.43299    -105624           |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20           1501.92        -5.50449e+08        -5.85041e+08   0.378698        -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20           1225.47        -4.8756e+08         -5.17014e+08   0.266667        -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                                   |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 9:  76%|███████▋  | 45/59 [00:44<00:13,  1.01it/s, v_num=0, train_loss_step=-1.27e+5, val_loss=-1.06e+5, train_loss_epoch=-9.14e+4]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 9:  85%|████████▍ | 50/59 [00:48<00:08,  1.02it/s, v_num=0, train_loss_step=-1.27e+5, val_loss=-1.06e+5, train_loss_epoch=-9.14e+4]\n",
            "Epoch 9:  85%|████████▍ | 50/59 [00:48<00:08,  1.02it/s, v_num=0, train_loss_step=-1.32e+5, val_loss=-1.06e+5, train_loss_epoch=-9.14e+4]\n",
            "Epoch 9:  86%|████████▋ | 51/59 [00:49<00:07,  1.02it/s, v_num=0, train_loss_step=-1.11e+5, val_loss=-1.06e+5, train_loss_epoch=-9.14e+4]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 9:  92%|█████████▏| 54/59 [00:52<00:04,  1.03it/s, v_num=0, train_loss_step=-1.33e+5, val_loss=-1.06e+5, train_loss_epoch=-9.14e+4]\n",
            "Epoch 9:  93%|█████████▎| 55/59 [00:54<00:03,  1.02it/s, v_num=0, train_loss_step=-1.25e+5, val_loss=-1.06e+5, train_loss_epoch=-9.14e+4]\n",
            "Epoch 1:  81%|████████▏ | 48/59 [06:18<01:26,  0.13it/s, v_num=0, train_loss_step=-8.78e+3, val_loss=-399., train_loss_epoch=-27.2]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "Epoch 9:  95%|█████████▍| 56/59 [00:55<00:02,  1.01it/s, v_num=0, train_loss_step=-1.23e+5, val_loss=-1.06e+5, train_loss_epoch=-9.14e+4]\n",
            "Epoch 9:  97%|█████████▋| 57/59 [00:56<00:01,  1.00it/s, v_num=0, train_loss_step=-1.17e+5, val_loss=-1.06e+5, train_loss_epoch=-9.14e+4]\n",
            "Epoch 9:  98%|█████████▊| 58/59 [00:57<00:00,  1.00it/s, v_num=0, train_loss_step=-1.36e+5, val_loss=-1.06e+5, train_loss_epoch=-9.14e+4]\n",
            "Epoch 9: 100%|██████████| 59/59 [00:58<00:00,  1.01it/s, v_num=0, train_loss_step=-1.11e+5, val_loss=-1.06e+5, train_loss_epoch=-9.14e+4]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:04,  1.45it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:04,  1.38it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:02<00:03,  1.34it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:02<00:02,  1.34it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:03<00:02,  1.35it/s]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 49/59 [06:25<01:18,  0.13it/s, v_num=0, train_loss_step=-1.32e+4, val_loss=-399., train_loss_epoch=-27.2]\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:04<00:01,  1.35it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:05<00:00,  1.35it/s]\u001b[A\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/b3fd7458/checkpoint_000009)\n",
            "2025-05-05 11:35:31,816\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 8/8 [00:05<00:00,  1.47it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \r                                                                      \u001b[A\rEpoch 9: 100%|██████████| 59/59 [01:03<00:00,  0.93it/s, v_num=0, train_loss_step=-1.11e+5, val_loss=-1.08e+5, train_loss_epoch=-9.14e+4]\rEpoch 9: 100%|██████████| 59/59 [01:03<00:00,  0.92it/s, v_num=0, train_loss_step=-1.11e+5, val_loss=-1.08e+5, train_loss_epoch=-1.15e+5]\n",
            "Epoch 10:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-1.11e+5, val_loss=-1.08e+5, train_loss_epoch=-1.15e+5]\n",
            "Epoch 10:   8%|▊         | 5/59 [00:05<00:57,  0.93it/s, v_num=0, train_loss_step=-1.38e+5, val_loss=-1.08e+5, train_loss_epoch=-1.15e+5]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:35:38. Total running time: 37min 3s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)          train_loss     train_loss_step     val/f1            val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000        1            550.186       -27.2132            -30.5904        0             -398.654       |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300       10            677.825   -114922             -110680             0.639706   -107800           |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20           1501.92         -5.50449e+08        -5.85041e+08   0.378698        -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20           1225.47         -4.8756e+08         -5.17014e+08   0.266667        -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                                    |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 10:  17%|█▋        | 10/59 [00:10<00:52,  0.94it/s, v_num=0, train_loss_step=-1.2e+5, val_loss=-1.08e+5, train_loss_epoch=-1.15e+5] \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 10:  27%|██▋       | 16/59 [00:15<00:42,  1.01it/s, v_num=0, train_loss_step=-1.41e+5, val_loss=-1.08e+5, train_loss_epoch=-1.15e+5]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 10:  36%|███▌      | 21/59 [00:21<00:39,  0.97it/s, v_num=0, train_loss_step=-1.41e+5, val_loss=-1.08e+5, train_loss_epoch=-1.15e+5]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 10:  46%|████▌     | 27/59 [00:27<00:32,  0.99it/s, v_num=0, train_loss_step=-1.51e+5, val_loss=-1.08e+5, train_loss_epoch=-1.15e+5]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 10:  56%|█████▌    | 33/59 [00:32<00:25,  1.03it/s, v_num=0, train_loss_step=-1.21e+5, val_loss=-1.08e+5, train_loss_epoch=-1.15e+5]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:36:08. Total running time: 37min 33s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)          train_loss     train_loss_step     val/f1            val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000        1            550.186       -27.2132            -30.5904        0             -398.654       |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300       10            677.825   -114922             -110680             0.639706   -107800           |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20           1501.92         -5.50449e+08        -5.85041e+08   0.378698        -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20           1225.47         -4.8756e+08         -5.17014e+08   0.266667        -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                                    |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 10:  63%|██████▎   | 37/59 [00:37<00:22,  0.98it/s, v_num=0, train_loss_step=-1.35e+5, val_loss=-1.08e+5, train_loss_epoch=-1.15e+5]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "Epoch 1:  92%|█████████▏| 54/59 [07:06<00:39,  0.13it/s, v_num=0, train_loss_step=-1.72e+4, val_loss=-399., train_loss_epoch=-27.2]\n",
            "Epoch 10:  73%|███████▎  | 43/59 [00:42<00:15,  1.00it/s, v_num=0, train_loss_step=-1.55e+5, val_loss=-1.08e+5, train_loss_epoch=-1.15e+5]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 1:  93%|█████████▎| 55/59 [07:13<00:31,  0.13it/s, v_num=0, train_loss_step=-1.8e+4, val_loss=-399., train_loss_epoch=-27.2] \n",
            "Epoch 10:  83%|████████▎ | 49/59 [00:47<00:09,  1.02it/s, v_num=0, train_loss_step=-1.52e+5, val_loss=-1.08e+5, train_loss_epoch=-1.15e+5]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 10:  90%|████████▉ | 53/59 [00:53<00:06,  1.00it/s, v_num=0, train_loss_step=-1.37e+5, val_loss=-1.08e+5, train_loss_epoch=-1.15e+5]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "Epoch 10:  92%|█████████▏| 54/59 [00:54<00:05,  1.00it/s, v_num=0, train_loss_step=-1.38e+5, val_loss=-1.08e+5, train_loss_epoch=-1.15e+5]\n",
            "Epoch 10:  93%|█████████▎| 55/59 [00:54<00:03,  1.00it/s, v_num=0, train_loss_step=-1.47e+5, val_loss=-1.08e+5, train_loss_epoch=-1.15e+5]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:04,  1.64it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:04,  1.47it/s]\u001b[A\n",
            "Epoch 10: 100%|██████████| 59/59 [00:58<00:00,  1.01it/s, v_num=0, train_loss_step=-1.61e+5, val_loss=-1.08e+5, train_loss_epoch=-1.15e+5]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:02<00:03,  1.41it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:02<00:02,  1.39it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:03<00:02,  1.39it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:04<00:01,  1.38it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:05<00:00,  1.32it/s]\u001b[A\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/b3fd7458/checkpoint_000010)\n",
            "2025-05-05 11:36:36,052\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 8/8 [00:05<00:00,  1.39it/s]\u001b[A\n",
            "Epoch 10: 100%|██████████| 59/59 [01:04<00:00,  0.92it/s, v_num=0, train_loss_step=-1.61e+5, val_loss=-1.26e+5, train_loss_epoch=-1.15e+5]\n",
            "Epoch 10: 100%|██████████| 59/59 [01:04<00:00,  0.92it/s, v_num=0, train_loss_step=-1.61e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-1.61e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:   2%|▏         | 1/59 [00:01<01:25,  0.68it/s, v_num=0, train_loss_step=-1.37e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 1:  97%|█████████▋| 57/59 [07:29<00:15,  0.13it/s, v_num=0, train_loss_step=-2.62e+4, val_loss=-399., train_loss_epoch=-27.2]\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:36:38. Total running time: 38min 3s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)          train_loss     train_loss_step     val/f1            val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000        1            550.186       -27.2132            -30.5904        0             -398.654       |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300       11            742.043   -138056             -161408             0.648438   -125934           |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20           1501.92         -5.50449e+08        -5.85041e+08   0.378698        -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20           1225.47         -4.8756e+08         -5.17014e+08   0.266667        -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                                    |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 11:   3%|▎         | 2/59 [00:02<01:20,  0.71it/s, v_num=0, train_loss_step=-1.6e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5] \n",
            "Epoch 11:   5%|▌         | 3/59 [00:03<01:13,  0.76it/s, v_num=0, train_loss_step=-1.57e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:   7%|▋         | 4/59 [00:04<01:06,  0.83it/s, v_num=0, train_loss_step=-1.58e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:   8%|▊         | 5/59 [00:05<01:01,  0.88it/s, v_num=0, train_loss_step=-1.58e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:  10%|█         | 6/59 [00:06<00:58,  0.91it/s, v_num=0, train_loss_step=-1.58e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 1:  98%|█████████▊| 58/59 [07:38<00:07,  0.13it/s, v_num=0, train_loss_step=-3.37e+4, val_loss=-399., train_loss_epoch=-27.2]\n",
            "Epoch 11:  12%|█▏        | 7/59 [00:07<00:55,  0.95it/s, v_num=0, train_loss_step=-1.6e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5] \n",
            "Epoch 11:  14%|█▎        | 8/59 [00:08<00:53,  0.96it/s, v_num=0, train_loss_step=-1.58e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:  15%|█▌        | 9/59 [00:09<00:51,  0.98it/s, v_num=0, train_loss_step=-1.44e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:  17%|█▋        | 10/59 [00:10<00:49,  0.99it/s, v_num=0, train_loss_step=-1.67e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 1: 100%|██████████| 59/59 [07:41<00:00,  0.13it/s, v_num=0, train_loss_step=-5.59e+4, val_loss=-399., train_loss_epoch=-27.2]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 11:  19%|█▊        | 11/59 [00:10<00:47,  1.01it/s, v_num=0, train_loss_step=-1.52e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:  20%|██        | 12/59 [00:11<00:46,  1.01it/s, v_num=0, train_loss_step=-1.47e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:  22%|██▏       | 13/59 [00:12<00:45,  1.02it/s, v_num=0, train_loss_step=-1.51e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:  24%|██▎       | 14/59 [00:13<00:43,  1.03it/s, v_num=0, train_loss_step=-1.35e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:  25%|██▌       | 15/59 [00:15<00:44,  0.99it/s, v_num=0, train_loss_step=-1.52e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:06<00:42,  0.16it/s]\u001b[A\n",
            "Epoch 11:  27%|██▋       | 16/59 [00:16<00:44,  0.96it/s, v_num=0, train_loss_step=-1.58e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:  29%|██▉       | 17/59 [00:18<00:44,  0.94it/s, v_num=0, train_loss_step=-1.61e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:  31%|███       | 18/59 [00:18<00:43,  0.95it/s, v_num=0, train_loss_step=-1.6e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5] \n",
            "Epoch 11:  32%|███▏      | 19/59 [00:19<00:41,  0.96it/s, v_num=0, train_loss_step=-1.48e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:  34%|███▍      | 20/59 [00:20<00:40,  0.97it/s, v_num=0, train_loss_step=-1.65e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:  36%|███▌      | 21/59 [00:21<00:38,  0.98it/s, v_num=0, train_loss_step=-1.64e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:  37%|███▋      | 22/59 [00:22<00:37,  0.98it/s, v_num=0, train_loss_step=-1.58e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:12<00:36,  0.17it/s]\u001b[A\n",
            "Epoch 11:  39%|███▉      | 23/59 [00:23<00:36,  0.99it/s, v_num=0, train_loss_step=-1.64e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:  41%|████      | 24/59 [00:24<00:35,  0.99it/s, v_num=0, train_loss_step=-1.53e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:  42%|████▏     | 25/59 [00:25<00:34,  1.00it/s, v_num=0, train_loss_step=-1.64e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:  44%|████▍     | 26/59 [00:25<00:32,  1.00it/s, v_num=0, train_loss_step=-1.69e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:  46%|████▌     | 27/59 [00:26<00:31,  1.01it/s, v_num=0, train_loss_step=-1.66e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:  47%|████▋     | 28/59 [00:27<00:30,  1.01it/s, v_num=0, train_loss_step=-1.65e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:17<00:29,  0.17it/s]\u001b[A\n",
            "Epoch 11:  49%|████▉     | 29/59 [00:28<00:29,  1.01it/s, v_num=0, train_loss_step=-1.72e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:  51%|█████     | 30/59 [00:30<00:29,  0.99it/s, v_num=0, train_loss_step=-1.7e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5] \n",
            "Epoch 11:  53%|█████▎    | 31/59 [00:31<00:28,  0.97it/s, v_num=0, train_loss_step=-1.59e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:37:09. Total running time: 38min 33s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)          train_loss     train_loss_step     val/f1            val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000        1            550.186       -27.2132            -30.5904        0             -398.654       |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300       11            742.043   -138056             -161408             0.648438   -125934           |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20           1501.92         -5.50449e+08        -5.85041e+08   0.378698        -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20           1225.47         -4.8756e+08         -5.17014e+08   0.266667        -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                                    |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 11:  54%|█████▍    | 32/59 [00:33<00:27,  0.97it/s, v_num=0, train_loss_step=-1.53e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:  56%|█████▌    | 33/59 [00:33<00:26,  0.97it/s, v_num=0, train_loss_step=-1.66e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:  58%|█████▊    | 34/59 [00:34<00:25,  0.98it/s, v_num=0, train_loss_step=-1.71e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:25<00:25,  0.16it/s]\u001b[A\n",
            "Epoch 11:  59%|█████▉    | 35/59 [00:35<00:24,  0.98it/s, v_num=0, train_loss_step=-1.54e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:  61%|██████    | 36/59 [00:36<00:23,  0.98it/s, v_num=0, train_loss_step=-1.69e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:  63%|██████▎   | 37/59 [00:37<00:22,  0.99it/s, v_num=0, train_loss_step=-1.5e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5] \n",
            "Epoch 11:  64%|██████▍   | 38/59 [00:38<00:21,  0.99it/s, v_num=0, train_loss_step=-1.56e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:  66%|██████▌   | 39/59 [00:39<00:20,  1.00it/s, v_num=0, train_loss_step=-1.68e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:  68%|██████▊   | 40/59 [00:39<00:18,  1.00it/s, v_num=0, train_loss_step=-1.6e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5] \n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:29<00:17,  0.17it/s]\u001b[A\n",
            "Epoch 11:  69%|██████▉   | 41/59 [00:40<00:17,  1.00it/s, v_num=0, train_loss_step=-1.66e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:  71%|███████   | 42/59 [00:41<00:16,  1.01it/s, v_num=0, train_loss_step=-1.65e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:  73%|███████▎  | 43/59 [00:42<00:15,  1.01it/s, v_num=0, train_loss_step=-1.65e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:  75%|███████▍  | 44/59 [00:43<00:14,  1.02it/s, v_num=0, train_loss_step=-1.56e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:  76%|███████▋  | 45/59 [00:44<00:13,  1.01it/s, v_num=0, train_loss_step=-1.75e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:  78%|███████▊  | 46/59 [00:46<00:13,  0.99it/s, v_num=0, train_loss_step=-1.77e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:37<00:12,  0.16it/s]\u001b[A\n",
            "Epoch 11:  80%|███████▉  | 47/59 [00:47<00:12,  0.99it/s, v_num=0, train_loss_step=-1.43e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:  81%|████████▏ | 48/59 [00:48<00:11,  0.99it/s, v_num=0, train_loss_step=-1.63e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:  83%|████████▎ | 49/59 [00:49<00:10,  0.99it/s, v_num=0, train_loss_step=-1.78e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:  85%|████████▍ | 50/59 [00:50<00:09,  0.99it/s, v_num=0, train_loss_step=-1.82e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:  86%|████████▋ | 51/59 [00:51<00:08,  1.00it/s, v_num=0, train_loss_step=-1.68e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:  88%|████████▊ | 52/59 [00:52<00:07,  1.00it/s, v_num=0, train_loss_step=-1.53e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:  90%|████████▉ | 53/59 [00:52<00:05,  1.00it/s, v_num=0, train_loss_step=-1.8e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5] \n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:42<00:06,  0.16it/s]\u001b[A\n",
            "Epoch 11:  92%|█████████▏| 54/59 [00:53<00:04,  1.00it/s, v_num=0, train_loss_step=-1.74e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:44<00:00,  0.18it/s]\u001b[A\n",
            "Epoch 1: 100%|██████████| 59/59 [08:25<00:00,  0.12it/s, v_num=0, train_loss_step=-5.59e+4, val_loss=-3.58e+4, train_loss_epoch=-27.2]\n",
            "Epoch 11:  93%|█████████▎| 55/59 [00:54<00:03,  1.00it/s, v_num=0, train_loss_step=-1.84e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:  95%|█████████▍| 56/59 [00:55<00:02,  1.01it/s, v_num=0, train_loss_step=-1.71e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/33384398/checkpoint_000001)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: 100%|██████████| 59/59 [08:26<00:00,  0.12it/s, v_num=0, train_loss_step=-5.59e+4, val_loss=-3.58e+4, train_loss_epoch=-6.28e+3]\n",
            "Epoch 11:  97%|█████████▋| 57/59 [00:56<00:01,  1.01it/s, v_num=0, train_loss_step=-1.81e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11:  98%|█████████▊| 58/59 [00:56<00:00,  1.02it/s, v_num=0, train_loss_step=-1.55e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11: 100%|██████████| 59/59 [00:57<00:00,  1.03it/s, v_num=0, train_loss_step=-1.68e+5, val_loss=-1.26e+5, train_loss_epoch=-1.38e+5]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:01<00:02,  2.18it/s]\u001b[A\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "Epoch 2:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-5.59e+4, val_loss=-3.58e+4, train_loss_epoch=-6.28e+3]\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:37:39. Total running time: 39min 3s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)          train_loss     train_loss_step     val/f1            val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000        2           1057.79      -6278.74           -55932.6           0           -35820.2         |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300       11            742.043   -138056             -161408             0.648438   -125934           |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20           1501.92         -5.50449e+08        -5.85041e+08   0.378698        -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20           1225.47         -4.8756e+08         -5.17014e+08   0.266667        -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                                    |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:06<00:00,  1.30it/s]\u001b[A\n",
            "Epoch 11: 100%|██████████| 59/59 [01:03<00:00,  0.93it/s, v_num=0, train_loss_step=-1.68e+5, val_loss=-1.88e+5, train_loss_epoch=-1.38e+5]\n",
            "Epoch 11: 100%|██████████| 59/59 [01:03<00:00,  0.93it/s, v_num=0, train_loss_step=-1.68e+5, val_loss=-1.88e+5, train_loss_epoch=-1.62e+5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/b3fd7458/checkpoint_000011)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:05<00:00,  1.18it/s]\u001b[A\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "Epoch 12:   2%|▏         | 1/59 [00:00<00:50,  1.15it/s, v_num=0, train_loss_step=-1.81e+5, val_loss=-1.88e+5, train_loss_epoch=-1.62e+5]\n",
            "Epoch 12:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-1.68e+5, val_loss=-1.88e+5, train_loss_epoch=-1.62e+5]\n",
            "Epoch 12:   3%|▎         | 2/59 [00:01<00:48,  1.17it/s, v_num=0, train_loss_step=-1.77e+5, val_loss=-1.88e+5, train_loss_epoch=-1.62e+5]\n",
            "Epoch 12:   5%|▌         | 3/59 [00:02<00:47,  1.17it/s, v_num=0, train_loss_step=-1.76e+5, val_loss=-1.88e+5, train_loss_epoch=-1.62e+5]\n",
            "Epoch 12:  12%|█▏        | 7/59 [00:05<00:44,  1.18it/s, v_num=0, train_loss_step=-1.64e+5, val_loss=-1.88e+5, train_loss_epoch=-1.62e+5]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 2:   3%|▎         | 2/59 [00:14<06:44,  0.14it/s, v_num=0, train_loss_step=-4.67e+4, val_loss=-3.58e+4, train_loss_epoch=-6.28e+3]\n",
            "Epoch 2:   3%|▎         | 2/59 [00:14<06:44,  0.14it/s, v_num=0, train_loss_step=-6.03e+4, val_loss=-3.58e+4, train_loss_epoch=-6.28e+3]\n",
            "Epoch 12:  22%|██▏       | 13/59 [00:12<00:43,  1.06it/s, v_num=0, train_loss_step=-1.89e+5, val_loss=-1.88e+5, train_loss_epoch=-1.62e+5]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 12:  31%|███       | 18/59 [00:17<00:39,  1.03it/s, v_num=0, train_loss_step=-1.84e+5, val_loss=-1.88e+5, train_loss_epoch=-1.62e+5]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 12:  41%|████      | 24/59 [00:22<00:33,  1.06it/s, v_num=0, train_loss_step=-1.88e+5, val_loss=-1.88e+5, train_loss_epoch=-1.62e+5]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 12:  49%|████▉     | 29/59 [00:28<00:29,  1.00it/s, v_num=0, train_loss_step=-1.85e+5, val_loss=-1.88e+5, train_loss_epoch=-1.62e+5]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:38:09. Total running time: 39min 33s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)          train_loss     train_loss_step     val/f1            val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000        2           1057.79      -6278.74           -55932.6           0           -35820.2         |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300       12            805.554   -161636             -167814             0.597285   -188295           |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20           1501.92         -5.50449e+08        -5.85041e+08   0.378698        -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20           1225.47         -4.8756e+08         -5.17014e+08   0.266667        -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                                    |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 12:  59%|█████▉    | 35/59 [00:34<00:23,  1.02it/s, v_num=0, train_loss_step=-1.94e+5, val_loss=-1.88e+5, train_loss_epoch=-1.62e+5]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 12:  69%|██████▉   | 41/59 [00:39<00:17,  1.04it/s, v_num=0, train_loss_step=-1.92e+5, val_loss=-1.88e+5, train_loss_epoch=-1.62e+5]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 12:  76%|███████▋  | 45/59 [00:44<00:13,  1.00it/s, v_num=0, train_loss_step=-1.9e+5, val_loss=-1.88e+5, train_loss_epoch=-1.62e+5] \u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 12:  86%|████████▋ | 51/59 [00:50<00:07,  1.02it/s, v_num=0, train_loss_step=-1.82e+5, val_loss=-1.88e+5, train_loss_epoch=-1.62e+5]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 12:  92%|█████████▏| 54/59 [00:52<00:04,  1.03it/s, v_num=0, train_loss_step=-1.79e+5, val_loss=-1.88e+5, train_loss_epoch=-1.62e+5]\n",
            "Epoch 12:  93%|█████████▎| 55/59 [00:53<00:03,  1.03it/s, v_num=0, train_loss_step=-1.87e+5, val_loss=-1.88e+5, train_loss_epoch=-1.62e+5]\n",
            "Epoch 12:  95%|█████████▍| 56/59 [00:54<00:02,  1.03it/s, v_num=0, train_loss_step=-1.96e+5, val_loss=-1.88e+5, train_loss_epoch=-1.62e+5]\n",
            "Epoch 12:  97%|█████████▋| 57/59 [00:55<00:01,  1.02it/s, v_num=0, train_loss_step=-1.98e+5, val_loss=-1.88e+5, train_loss_epoch=-1.62e+5]\n",
            "Epoch 12:  90%|████████▉ | 53/59 [00:51<00:05,  1.02it/s, v_num=0, train_loss_step=-1.98e+5, val_loss=-1.88e+5, train_loss_epoch=-1.62e+5]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "Epoch 12:  98%|█████████▊| 58/59 [00:57<00:00,  1.01it/s, v_num=0, train_loss_step=-1.84e+5, val_loss=-1.88e+5, train_loss_epoch=-1.62e+5]\n",
            "Epoch 12: 100%|██████████| 59/59 [00:58<00:00,  1.01it/s, v_num=0, train_loss_step=-1.85e+5, val_loss=-1.88e+5, train_loss_epoch=-1.62e+5]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:05,  1.30it/s]\u001b[A\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:38:39. Total running time: 40min 3s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)          train_loss     train_loss_step     val/f1            val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000        2           1057.79      -6278.74           -55932.6           0           -35820.2         |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300       12            805.554   -161636             -167814             0.597285   -188295           |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20           1501.92         -5.50449e+08        -5.85041e+08   0.378698        -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20           1225.47         -4.8756e+08         -5.17014e+08   0.266667        -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                                    |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:04,  1.32it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:02<00:03,  1.30it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:03<00:03,  1.31it/s]\u001b[A\n",
            "Epoch 2:  14%|█▎        | 8/59 [01:03<06:46,  0.13it/s, v_num=0, train_loss_step=-3.7e+4, val_loss=-3.58e+4, train_loss_epoch=-6.28e+3] \n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:03<00:02,  1.32it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:04<00:01,  1.33it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:05<00:00,  1.34it/s]\u001b[A\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-05 11:38:43,656\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 8/8 [00:05<00:00,  1.47it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \r                                                                      \u001b[A\rEpoch 12: 100%|██████████| 59/59 [01:03<00:00,  0.92it/s, v_num=0, train_loss_step=-1.85e+5, val_loss=-2.25e+5, train_loss_epoch=-1.62e+5]\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \rEpoch 12: 100%|██████████| 59/59 [01:03<00:00,  0.92it/s, v_num=0, train_loss_step=-1.85e+5, val_loss=-2.25e+5, train_loss_epoch=-1.85e+5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/b3fd7458/checkpoint_000012)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-1.85e+5, val_loss=-2.25e+5, train_loss_epoch=-1.85e+5]\n",
            "Epoch 13:   2%|▏         | 1/59 [00:00<00:54,  1.07it/s, v_num=0, train_loss_step=-1.76e+5, val_loss=-2.25e+5, train_loss_epoch=-1.85e+5]\n",
            "Epoch 13:   3%|▎         | 2/59 [00:01<00:53,  1.07it/s, v_num=0, train_loss_step=-2.05e+5, val_loss=-2.25e+5, train_loss_epoch=-1.85e+5]\n",
            "Epoch 13:  12%|█▏        | 7/59 [00:06<00:51,  1.00it/s, v_num=0, train_loss_step=-2.03e+5, val_loss=-2.25e+5, train_loss_epoch=-1.85e+5]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 13:  20%|██        | 12/59 [00:12<00:48,  0.97it/s, v_num=0, train_loss_step=-1.84e+5, val_loss=-2.25e+5, train_loss_epoch=-1.85e+5]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 13:  31%|███       | 18/59 [00:17<00:40,  1.02it/s, v_num=0, train_loss_step=-2.01e+5, val_loss=-2.25e+5, train_loss_epoch=-1.85e+5]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 13:  39%|███▉      | 23/59 [00:23<00:36,  0.99it/s, v_num=0, train_loss_step=-2.07e+5, val_loss=-2.25e+5, train_loss_epoch=-1.85e+5]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:39:09. Total running time: 40min 33s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)          train_loss     train_loss_step     val/f1            val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000        2            1057.79     -6278.74           -55932.6           0           -35820.2         |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300       13             869.59   -184566             -185378             0.505263   -224573           |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20            1501.92        -5.50449e+08        -5.85041e+08   0.378698        -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20            1225.47        -4.8756e+08         -5.17014e+08   0.266667        -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                                    |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 13:  49%|████▉     | 29/59 [00:28<00:29,  1.00it/s, v_num=0, train_loss_step=-1.94e+5, val_loss=-2.25e+5, train_loss_epoch=-1.85e+5]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 13:  59%|█████▉    | 35/59 [00:33<00:23,  1.03it/s, v_num=0, train_loss_step=-1.9e+5, val_loss=-2.25e+5, train_loss_epoch=-1.85e+5] \u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 13:  66%|██████▌   | 39/59 [00:39<00:20,  1.00it/s, v_num=0, train_loss_step=-2.08e+5, val_loss=-2.25e+5, train_loss_epoch=-1.85e+5]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "Epoch 13:  76%|███████▋  | 45/59 [00:44<00:13,  1.01it/s, v_num=0, train_loss_step=-2e+5, val_loss=-2.25e+5, train_loss_epoch=-1.85e+5]   \u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 13:  86%|████████▋ | 51/59 [00:49<00:07,  1.02it/s, v_num=0, train_loss_step=-1.94e+5, val_loss=-2.25e+5, train_loss_epoch=-1.85e+5]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 13:  92%|█████████▏| 54/59 [00:54<00:05,  1.00it/s, v_num=0, train_loss_step=-2.1e+5, val_loss=-2.25e+5, train_loss_epoch=-1.85e+5] \n",
            "Epoch 13:  93%|█████████▎| 55/59 [00:55<00:04,  1.00it/s, v_num=0, train_loss_step=-2.01e+5, val_loss=-2.25e+5, train_loss_epoch=-1.85e+5]\n",
            "Epoch 13:  90%|████████▉ | 53/59 [00:52<00:05,  1.01it/s, v_num=0, train_loss_step=-2.23e+5, val_loss=-2.25e+5, train_loss_epoch=-1.85e+5]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:39:39. Total running time: 41min 3s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)          train_loss     train_loss_step     val/f1            val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000        2            1057.79     -6278.74           -55932.6           0           -35820.2         |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300       13             869.59   -184566             -185378             0.505263   -224573           |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20            1501.92        -5.50449e+08        -5.85041e+08   0.378698        -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20            1225.47        -4.8756e+08         -5.17014e+08   0.266667        -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                                    |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 13:  95%|█████████▍| 56/59 [00:56<00:03,  1.00it/s, v_num=0, train_loss_step=-2.12e+5, val_loss=-2.25e+5, train_loss_epoch=-1.85e+5]\n",
            "Epoch 13:  97%|█████████▋| 57/59 [00:57<00:02,  1.00it/s, v_num=0, train_loss_step=-2.14e+5, val_loss=-2.25e+5, train_loss_epoch=-1.85e+5]\n",
            "Epoch 13:  98%|█████████▊| 58/59 [00:57<00:00,  1.00it/s, v_num=0, train_loss_step=-2.13e+5, val_loss=-2.25e+5, train_loss_epoch=-1.85e+5]\n",
            "Epoch 13: 100%|██████████| 59/59 [00:58<00:00,  1.01it/s, v_num=0, train_loss_step=-2.18e+5, val_loss=-2.25e+5, train_loss_epoch=-1.85e+5]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:04,  1.61it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:04,  1.49it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Epoch 2:  27%|██▋       | 16/59 [02:06<05:40,  0.13it/s, v_num=0, train_loss_step=-7.32e+4, val_loss=-3.58e+4, train_loss_epoch=-6.28e+3]\n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:02<00:03,  1.42it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:02<00:02,  1.39it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:03<00:02,  1.40it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:04<00:01,  1.38it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:05<00:00,  1.37it/s]\u001b[A\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-05 11:39:47,590\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 8/8 [00:05<00:00,  1.50it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \r                                                                      \u001b[A\rEpoch 13: 100%|██████████| 59/59 [01:03<00:00,  0.93it/s, v_num=0, train_loss_step=-2.18e+5, val_loss=-2.17e+5, train_loss_epoch=-1.85e+5]\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \rEpoch 13: 100%|██████████| 59/59 [01:03<00:00,  0.92it/s, v_num=0, train_loss_step=-2.18e+5, val_loss=-2.17e+5, train_loss_epoch=-2.04e+5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/b3fd7458/checkpoint_000013)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-2.18e+5, val_loss=-2.17e+5, train_loss_epoch=-2.04e+5]\n",
            "Epoch 2:  29%|██▉       | 17/59 [02:12<05:28,  0.13it/s, v_num=0, train_loss_step=-1.13e+5, val_loss=-3.58e+4, train_loss_epoch=-6.28e+3]\n",
            "Epoch 14:   8%|▊         | 5/59 [00:06<01:07,  0.80it/s, v_num=0, train_loss_step=-2.19e+5, val_loss=-2.17e+5, train_loss_epoch=-2.04e+5]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 14:  19%|█▊        | 11/59 [00:11<00:49,  0.96it/s, v_num=0, train_loss_step=-2.25e+5, val_loss=-2.17e+5, train_loss_epoch=-2.04e+5]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 14:  29%|██▉       | 17/59 [00:17<00:42,  1.00it/s, v_num=0, train_loss_step=-2.21e+5, val_loss=-2.17e+5, train_loss_epoch=-2.04e+5]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:40:09. Total running time: 41min 34s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)          train_loss     train_loss_step     val/f1            val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000        2           1057.79      -6278.74           -55932.6           0           -35820.2         |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300       14            933.507   -204374             -218228             0.646552   -216523           |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20           1501.92         -5.50449e+08        -5.85041e+08   0.378698        -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20           1225.47         -4.8756e+08         -5.17014e+08   0.266667        -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                                    |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 14:  36%|███▌      | 21/59 [00:22<00:40,  0.95it/s, v_num=0, train_loss_step=-2.27e+5, val_loss=-2.17e+5, train_loss_epoch=-2.04e+5]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "Epoch 14:  47%|████▋     | 28/59 [00:27<00:30,  1.00it/s, v_num=0, train_loss_step=-2.2e+5, val_loss=-2.17e+5, train_loss_epoch=-2.04e+5] \u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "Epoch 14:  58%|█████▊    | 34/59 [00:34<00:25,  0.99it/s, v_num=0, train_loss_step=-2.31e+5, val_loss=-2.17e+5, train_loss_epoch=-2.04e+5]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 14:  68%|██████▊   | 40/59 [00:40<00:19,  1.00it/s, v_num=0, train_loss_step=-2.33e+5, val_loss=-2.17e+5, train_loss_epoch=-2.04e+5]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 14:  78%|███████▊  | 46/59 [00:45<00:12,  1.02it/s, v_num=0, train_loss_step=-2.21e+5, val_loss=-2.17e+5, train_loss_epoch=-2.04e+5]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 14:  85%|████████▍ | 50/59 [00:50<00:09,  1.00it/s, v_num=0, train_loss_step=-2.41e+5, val_loss=-2.17e+5, train_loss_epoch=-2.04e+5]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:40:39. Total running time: 42min 4s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)          train_loss     train_loss_step     val/f1            val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000        2           1057.79      -6278.74           -55932.6           0           -35820.2         |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300       14            933.507   -204374             -218228             0.646552   -216523           |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20           1501.92         -5.50449e+08        -5.85041e+08   0.378698        -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20           1225.47         -4.8756e+08         -5.17014e+08   0.266667        -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                                    |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 14:  92%|█████████▏| 54/59 [00:54<00:05,  1.00it/s, v_num=0, train_loss_step=-2.32e+5, val_loss=-2.17e+5, train_loss_epoch=-2.04e+5]\n",
            "Epoch 14:  93%|█████████▎| 55/59 [00:55<00:04,  1.00it/s, v_num=0, train_loss_step=-2.35e+5, val_loss=-2.17e+5, train_loss_epoch=-2.04e+5]\n",
            "Epoch 14:  95%|█████████▍| 56/59 [00:55<00:02,  1.00it/s, v_num=0, train_loss_step=-2.25e+5, val_loss=-2.17e+5, train_loss_epoch=-2.04e+5]\n",
            "Epoch 14:  90%|████████▉ | 53/59 [00:53<00:06,  0.99it/s, v_num=0, train_loss_step=-2.29e+5, val_loss=-2.17e+5, train_loss_epoch=-2.04e+5]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "Epoch 14:  97%|█████████▋| 57/59 [00:56<00:01,  1.00it/s, v_num=0, train_loss_step=-2.33e+5, val_loss=-2.17e+5, train_loss_epoch=-2.04e+5]\n",
            "Epoch 14:  98%|█████████▊| 58/59 [00:57<00:00,  1.01it/s, v_num=0, train_loss_step=-2.34e+5, val_loss=-2.17e+5, train_loss_epoch=-2.04e+5]\n",
            "Epoch 14: 100%|██████████| 59/59 [00:58<00:00,  1.01it/s, v_num=0, train_loss_step=-2.05e+5, val_loss=-2.17e+5, train_loss_epoch=-2.04e+5]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:04,  1.68it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:03,  1.53it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:02<00:03,  1.44it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:02<00:02,  1.42it/s]\u001b[A\n",
            "Epoch 2:  41%|████      | 24/59 [03:08<04:34,  0.13it/s, v_num=0, train_loss_step=-1.79e+5, val_loss=-3.58e+4, train_loss_epoch=-6.28e+3]\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:03<00:02,  1.27it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:05<00:01,  1.11it/s]\u001b[A\n",
            "Epoch 2:  42%|████▏     | 25/59 [03:17<04:27,  0.13it/s, v_num=0, train_loss_step=-9.88e+4, val_loss=-3.58e+4, train_loss_epoch=-6.28e+3]\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:06<00:00,  1.04it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:07<00:00,  1.12it/s]\u001b[A\n",
            "Epoch 14: 100%|██████████| 59/59 [01:05<00:00,  0.90it/s, v_num=0, train_loss_step=-2.05e+5, val_loss=-2.48e+5, train_loss_epoch=-2.04e+5]\n",
            "Epoch 14: 100%|██████████| 59/59 [01:05<00:00,  0.90it/s, v_num=0, train_loss_step=-2.05e+5, val_loss=-2.48e+5, train_loss_epoch=-2.22e+5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/b3fd7458/checkpoint_000014)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-2.05e+5, val_loss=-2.48e+5, train_loss_epoch=-2.22e+5]\n",
            "Epoch 15:   8%|▊         | 5/59 [00:04<00:46,  1.16it/s, v_num=0, train_loss_step=-2.41e+5, val_loss=-2.48e+5, train_loss_epoch=-2.22e+5]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 15:  19%|█▊        | 11/59 [00:09<00:41,  1.16it/s, v_num=0, train_loss_step=-2.43e+5, val_loss=-2.48e+5, train_loss_epoch=-2.22e+5]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 15:  25%|██▌       | 15/59 [00:14<00:43,  1.02it/s, v_num=0, train_loss_step=-2.4e+5, val_loss=-2.48e+5, train_loss_epoch=-2.22e+5] \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:41:09. Total running time: 42min 34s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)          train_loss     train_loss_step     val/f1            val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000        2           1057.79      -6278.74           -55932.6           0           -35820.2         |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300       15            999.133   -222226             -205390             0.652174   -248064           |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20           1501.92         -5.50449e+08        -5.85041e+08   0.378698        -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20           1225.47         -4.8756e+08         -5.17014e+08   0.266667        -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                                    |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 15:  36%|███▌      | 21/59 [00:19<00:36,  1.05it/s, v_num=0, train_loss_step=-2.24e+5, val_loss=-2.48e+5, train_loss_epoch=-2.22e+5]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 15:  46%|████▌     | 27/59 [00:25<00:30,  1.06it/s, v_num=0, train_loss_step=-2.36e+5, val_loss=-2.48e+5, train_loss_epoch=-2.22e+5]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 15:  53%|█████▎    | 31/59 [00:30<00:27,  1.01it/s, v_num=0, train_loss_step=-2.42e+5, val_loss=-2.48e+5, train_loss_epoch=-2.22e+5]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "Epoch 15:  63%|██████▎   | 37/59 [00:35<00:21,  1.03it/s, v_num=0, train_loss_step=-2.34e+5, val_loss=-2.48e+5, train_loss_epoch=-2.22e+5]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 15:  73%|███████▎  | 43/59 [00:42<00:15,  1.02it/s, v_num=0, train_loss_step=-2.08e+5, val_loss=-2.48e+5, train_loss_epoch=-2.22e+5]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:41:39. Total running time: 43min 4s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)          train_loss     train_loss_step     val/f1            val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000        2           1057.79      -6278.74           -55932.6           0           -35820.2         |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300       15            999.133   -222226             -205390             0.652174   -248064           |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20           1501.92         -5.50449e+08        -5.85041e+08   0.378698        -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20           1225.47         -4.8756e+08         -5.17014e+08   0.266667        -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                                    |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 15:  83%|████████▎ | 49/59 [00:47<00:09,  1.02it/s, v_num=0, train_loss_step=-2.46e+5, val_loss=-2.48e+5, train_loss_epoch=-2.22e+5]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 15:  92%|█████████▏| 54/59 [00:52<00:04,  1.03it/s, v_num=0, train_loss_step=-2.42e+5, val_loss=-2.48e+5, train_loss_epoch=-2.22e+5]\n",
            "Epoch 2:  54%|█████▍    | 32/59 [04:11<03:31,  0.13it/s, v_num=0, train_loss_step=-1.53e+5, val_loss=-3.58e+4, train_loss_epoch=-6.28e+3]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 15:  93%|█████████▎| 55/59 [00:53<00:03,  1.04it/s, v_num=0, train_loss_step=-2.43e+5, val_loss=-2.48e+5, train_loss_epoch=-2.22e+5]\n",
            "Epoch 15:  95%|█████████▍| 56/59 [00:53<00:02,  1.04it/s, v_num=0, train_loss_step=-2.51e+5, val_loss=-2.48e+5, train_loss_epoch=-2.22e+5]\n",
            "Epoch 15:  97%|█████████▋| 57/59 [00:54<00:01,  1.04it/s, v_num=0, train_loss_step=-2.5e+5, val_loss=-2.48e+5, train_loss_epoch=-2.22e+5] \n",
            "Epoch 15:  98%|█████████▊| 58/59 [00:56<00:00,  1.02it/s, v_num=0, train_loss_step=-2.39e+5, val_loss=-2.48e+5, train_loss_epoch=-2.22e+5]\n",
            "Epoch 15: 100%|██████████| 59/59 [00:57<00:00,  1.03it/s, v_num=0, train_loss_step=-2.46e+5, val_loss=-2.48e+5, train_loss_epoch=-2.22e+5]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:05,  1.20it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:05,  1.05it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:02<00:04,  1.11it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:03<00:03,  1.15it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:04<00:02,  1.20it/s]\u001b[A\n",
            "Epoch 2:  56%|█████▌    | 33/59 [04:20<03:25,  0.13it/s, v_num=0, train_loss_step=-2.52e+5, val_loss=-3.58e+4, train_loss_epoch=-6.28e+3]\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:04<00:01,  1.21it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:05<00:00,  1.23it/s]\u001b[A\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-05 11:41:56,998\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 8/8 [00:05<00:00,  1.35it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \r                                                                      \u001b[A\rEpoch 15: 100%|██████████| 59/59 [01:03<00:00,  0.93it/s, v_num=0, train_loss_step=-2.46e+5, val_loss=-2.26e+5, train_loss_epoch=-2.22e+5]\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \rEpoch 15: 100%|██████████| 59/59 [01:03<00:00,  0.93it/s, v_num=0, train_loss_step=-2.46e+5, val_loss=-2.26e+5, train_loss_epoch=-2.39e+5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/b3fd7458/checkpoint_000015)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-2.46e+5, val_loss=-2.26e+5, train_loss_epoch=-2.39e+5]\n",
            "Epoch 16:   8%|▊         | 5/59 [00:04<00:47,  1.15it/s, v_num=0, train_loss_step=-2.59e+5, val_loss=-2.26e+5, train_loss_epoch=-2.39e+5]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 16:  17%|█▋        | 10/59 [00:10<00:52,  0.94it/s, v_num=0, train_loss_step=-2.46e+5, val_loss=-2.26e+5, train_loss_epoch=-2.39e+5]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:42:09. Total running time: 43min 34s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)          train_loss     train_loss_step     val/f1            val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000        2            1057.79     -6278.74           -55932.6           0           -35820.2         |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300       16            1062.88   -238581             -246354             0.713755   -225536           |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20            1501.92        -5.50449e+08        -5.85041e+08   0.378698        -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20            1225.47        -4.8756e+08         -5.17014e+08   0.266667        -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                                    |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 16:  27%|██▋       | 16/59 [00:15<00:42,  1.02it/s, v_num=0, train_loss_step=-2.58e+5, val_loss=-2.26e+5, train_loss_epoch=-2.39e+5]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 16:  37%|███▋      | 22/59 [00:21<00:35,  1.04it/s, v_num=0, train_loss_step=-2.51e+5, val_loss=-2.26e+5, train_loss_epoch=-2.39e+5]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 16:  44%|████▍     | 26/59 [00:26<00:33,  0.98it/s, v_num=0, train_loss_step=-2.58e+5, val_loss=-2.26e+5, train_loss_epoch=-2.39e+5]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "Epoch 16:  54%|█████▍    | 32/59 [00:31<00:26,  1.01it/s, v_num=0, train_loss_step=-2.56e+5, val_loss=-2.26e+5, train_loss_epoch=-2.39e+5]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 16:  64%|██████▍   | 38/59 [00:37<00:20,  1.01it/s, v_num=0, train_loss_step=-2.54e+5, val_loss=-2.26e+5, train_loss_epoch=-2.39e+5]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:42:39. Total running time: 44min 4s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)          train_loss     train_loss_step     val/f1            val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000        2            1057.79     -6278.74           -55932.6           0           -35820.2         |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300       16            1062.88   -238581             -246354             0.713755   -225536           |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20            1501.92        -5.50449e+08        -5.85041e+08   0.378698        -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20            1225.47        -4.8756e+08         -5.17014e+08   0.266667        -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                                    |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 16:  73%|███████▎  | 43/59 [00:43<00:16,  1.00it/s, v_num=0, train_loss_step=-2.65e+5, val_loss=-2.26e+5, train_loss_epoch=-2.39e+5]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 16:  83%|████████▎ | 49/59 [00:48<00:09,  1.02it/s, v_num=0, train_loss_step=-2.62e+5, val_loss=-2.26e+5, train_loss_epoch=-2.39e+5]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 16:  92%|█████████▏| 54/59 [00:54<00:05,  1.00it/s, v_num=0, train_loss_step=-2.47e+5, val_loss=-2.26e+5, train_loss_epoch=-2.39e+5]\n",
            "Epoch 16:  90%|████████▉ | 53/59 [00:52<00:05,  1.01it/s, v_num=0, train_loss_step=-2.51e+5, val_loss=-2.26e+5, train_loss_epoch=-2.39e+5]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 16:  93%|█████████▎| 55/59 [00:55<00:04,  0.99it/s, v_num=0, train_loss_step=-2.58e+5, val_loss=-2.26e+5, train_loss_epoch=-2.39e+5]\n",
            "Epoch 16:  95%|█████████▍| 56/59 [00:56<00:03,  1.00it/s, v_num=0, train_loss_step=-2.65e+5, val_loss=-2.26e+5, train_loss_epoch=-2.39e+5]\n",
            "Epoch 16:  97%|█████████▋| 57/59 [00:57<00:02,  1.00it/s, v_num=0, train_loss_step=-2.58e+5, val_loss=-2.26e+5, train_loss_epoch=-2.39e+5]\n",
            "Epoch 16:  98%|█████████▊| 58/59 [00:57<00:00,  1.00it/s, v_num=0, train_loss_step=-2.61e+5, val_loss=-2.26e+5, train_loss_epoch=-2.39e+5]\n",
            "Epoch 16: 100%|██████████| 59/59 [00:58<00:00,  1.01it/s, v_num=0, train_loss_step=-2.63e+5, val_loss=-2.26e+5, train_loss_epoch=-2.39e+5]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:04,  1.66it/s]\u001b[A\n",
            "Epoch 2:  69%|██████▉   | 41/59 [05:21<02:21,  0.13it/s, v_num=0, train_loss_step=-1.2e+5, val_loss=-3.58e+4, train_loss_epoch=-6.28e+3] \n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:03,  1.53it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:02<00:03,  1.46it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:02<00:02,  1.40it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:03<00:02,  1.41it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:04<00:01,  1.40it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:05<00:00,  1.36it/s]\u001b[A\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/b3fd7458/checkpoint_000016)\n",
            "2025-05-05 11:43:01,003\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 8/8 [00:05<00:00,  1.49it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \r                                                                      \u001b[A\rEpoch 16: 100%|██████████| 59/59 [01:03<00:00,  0.92it/s, v_num=0, train_loss_step=-2.63e+5, val_loss=-2.75e+5, train_loss_epoch=-2.39e+5]\rEpoch 16: 100%|██████████| 59/59 [01:03<00:00,  0.92it/s, v_num=0, train_loss_step=-2.63e+5, val_loss=-2.75e+5, train_loss_epoch=-2.54e+5]\n",
            "Epoch 17:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-2.63e+5, val_loss=-2.75e+5, train_loss_epoch=-2.54e+5]\n",
            "Epoch 17:   2%|▏         | 1/59 [00:00<00:48,  1.19it/s, v_num=0, train_loss_step=-2.48e+5, val_loss=-2.75e+5, train_loss_epoch=-2.54e+5]\n",
            "Epoch 17:   3%|▎         | 2/59 [00:01<00:50,  1.12it/s, v_num=0, train_loss_step=-2.59e+5, val_loss=-2.75e+5, train_loss_epoch=-2.54e+5]\n",
            "Epoch 17:   5%|▌         | 3/59 [00:03<01:01,  0.91it/s, v_num=0, train_loss_step=-2.71e+5, val_loss=-2.75e+5, train_loss_epoch=-2.54e+5]\n",
            "Epoch 17:   8%|▊         | 5/59 [00:05<01:04,  0.84it/s, v_num=0, train_loss_step=-2.7e+5, val_loss=-2.75e+5, train_loss_epoch=-2.54e+5] \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:43:09. Total running time: 44min 34s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)          train_loss     train_loss_step     val/f1            val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000        2            1057.79     -6278.74           -55932.6           0           -35820.2         |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300       17            1126.86   -254357             -263031             0.705882   -275256           |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20            1501.92        -5.50449e+08        -5.85041e+08   0.378698        -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20            1225.47        -4.8756e+08         -5.17014e+08   0.266667        -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                                    |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 17:  19%|█▊        | 11/59 [00:11<00:49,  0.97it/s, v_num=0, train_loss_step=-2.73e+5, val_loss=-2.75e+5, train_loss_epoch=-2.54e+5]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 17:  29%|██▉       | 17/59 [00:16<00:40,  1.03it/s, v_num=0, train_loss_step=-2.56e+5, val_loss=-2.75e+5, train_loss_epoch=-2.54e+5]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 17:  36%|███▌      | 21/59 [00:22<00:39,  0.95it/s, v_num=0, train_loss_step=-2.53e+5, val_loss=-2.75e+5, train_loss_epoch=-2.54e+5]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 17:  46%|████▌     | 27/59 [00:27<00:32,  0.99it/s, v_num=0, train_loss_step=-2.71e+5, val_loss=-2.75e+5, train_loss_epoch=-2.54e+5]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 17:  56%|█████▌    | 33/59 [00:33<00:26,  0.99it/s, v_num=0, train_loss_step=-2.69e+5, val_loss=-2.75e+5, train_loss_epoch=-2.54e+5]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:43:39. Total running time: 45min 4s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)          train_loss     train_loss_step     val/f1            val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000        2            1057.79     -6278.74           -55932.6           0           -35820.2         |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300       17            1126.86   -254357             -263031             0.705882   -275256           |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20            1501.92        -5.50449e+08        -5.85041e+08   0.378698        -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20            1225.47        -4.8756e+08         -5.17014e+08   0.266667        -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                                    |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 17:  64%|██████▍   | 38/59 [00:38<00:21,  0.98it/s, v_num=0, train_loss_step=-2.62e+5, val_loss=-2.75e+5, train_loss_epoch=-2.54e+5]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 17:  75%|███████▍  | 44/59 [00:43<00:14,  1.00it/s, v_num=0, train_loss_step=-2.67e+5, val_loss=-2.75e+5, train_loss_epoch=-2.54e+5]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 17:  83%|████████▎ | 49/59 [00:49<00:10,  0.99it/s, v_num=0, train_loss_step=-2.61e+5, val_loss=-2.75e+5, train_loss_epoch=-2.54e+5]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 17:  92%|█████████▏| 54/59 [00:54<00:05,  0.99it/s, v_num=0, train_loss_step=-2.78e+5, val_loss=-2.75e+5, train_loss_epoch=-2.54e+5]\n",
            "Epoch 17:  93%|█████████▎| 55/59 [00:55<00:04,  1.00it/s, v_num=0, train_loss_step=-2.76e+5, val_loss=-2.75e+5, train_loss_epoch=-2.54e+5]\n",
            "Epoch 17:  90%|████████▉ | 53/59 [00:53<00:06,  0.99it/s, v_num=0, train_loss_step=-2.75e+5, val_loss=-2.75e+5, train_loss_epoch=-2.54e+5]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 17:  95%|█████████▍| 56/59 [00:56<00:03,  1.00it/s, v_num=0, train_loss_step=-2.75e+5, val_loss=-2.75e+5, train_loss_epoch=-2.54e+5]\n",
            "Epoch 17:  97%|█████████▋| 57/59 [00:56<00:01,  1.00it/s, v_num=0, train_loss_step=-2.7e+5, val_loss=-2.75e+5, train_loss_epoch=-2.54e+5] \n",
            "Epoch 17:  98%|█████████▊| 58/59 [00:57<00:00,  1.00it/s, v_num=0, train_loss_step=-2.84e+5, val_loss=-2.75e+5, train_loss_epoch=-2.54e+5]\n",
            "Epoch 17: 100%|██████████| 59/59 [00:58<00:00,  1.01it/s, v_num=0, train_loss_step=-2.83e+5, val_loss=-2.75e+5, train_loss_epoch=-2.54e+5]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:04,  1.73it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:04,  1.49it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:02<00:03,  1.36it/s]\u001b[A\n",
            "Epoch 2:  83%|████████▎ | 49/59 [06:24<01:18,  0.13it/s, v_num=0, train_loss_step=-5.1e+5, val_loss=-3.58e+4, train_loss_epoch=-6.28e+3] \n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:02<00:02,  1.35it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:04<00:02,  1.24it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:05<00:01,  1.11it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:06<00:00,  1.04it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:07<00:00,  1.12it/s]\u001b[A\n",
            "Epoch 17: 100%|██████████| 59/59 [01:05<00:00,  0.90it/s, v_num=0, train_loss_step=-2.83e+5, val_loss=-2.54e+5, train_loss_epoch=-2.54e+5]\n",
            "Epoch 17: 100%|██████████| 59/59 [01:05<00:00,  0.90it/s, v_num=0, train_loss_step=-2.83e+5, val_loss=-2.54e+5, train_loss_epoch=-2.67e+5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/b3fd7458/checkpoint_000017)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-2.83e+5, val_loss=-2.54e+5, train_loss_epoch=-2.67e+5]\n",
            "Epoch 18:   2%|▏         | 1/59 [00:01<01:05,  0.89it/s, v_num=0, train_loss_step=-2.82e+5, val_loss=-2.54e+5, train_loss_epoch=-2.67e+5]\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:44:09. Total running time: 45min 34s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)          train_loss     train_loss_step     val/f1            val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000        2            1057.79     -6278.74           -55932.6           0           -35820.2         |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300       18            1192.56   -266888             -283125             0.732075   -254266           |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20            1501.92        -5.50449e+08        -5.85041e+08   0.378698        -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20            1225.47        -4.8756e+08         -5.17014e+08   0.266667        -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                                    |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 18:  12%|█▏        | 7/59 [00:06<00:46,  1.12it/s, v_num=0, train_loss_step=-2.77e+5, val_loss=-2.54e+5, train_loss_epoch=-2.67e+5]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 18:  22%|██▏       | 13/59 [00:11<00:41,  1.12it/s, v_num=0, train_loss_step=-2.84e+5, val_loss=-2.54e+5, train_loss_epoch=-2.67e+5]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 18:  29%|██▉       | 17/59 [00:16<00:41,  1.01it/s, v_num=0, train_loss_step=-2.82e+5, val_loss=-2.54e+5, train_loss_epoch=-2.67e+5]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "Epoch 18:  39%|███▉      | 23/59 [00:21<00:34,  1.05it/s, v_num=0, train_loss_step=-2.79e+5, val_loss=-2.54e+5, train_loss_epoch=-2.67e+5]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 18:  49%|████▉     | 29/59 [00:28<00:29,  1.03it/s, v_num=0, train_loss_step=-2.69e+5, val_loss=-2.54e+5, train_loss_epoch=-2.67e+5]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "Epoch 2:  92%|█████████▏| 54/59 [07:04<00:39,  0.13it/s, v_num=0, train_loss_step=-5.86e+5, val_loss=-3.58e+4, train_loss_epoch=-6.28e+3]\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:44:39. Total running time: 46min 4s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)          train_loss     train_loss_step     val/f1            val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000        2            1057.79     -6278.74           -55932.6           0           -35820.2         |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300       18            1192.56   -266888             -283125             0.732075   -254266           |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20            1501.92        -5.50449e+08        -5.85041e+08   0.378698        -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20            1225.47        -4.8756e+08         -5.17014e+08   0.266667        -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                                    |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 18:  58%|█████▊    | 34/59 [00:33<00:24,  1.02it/s, v_num=0, train_loss_step=-2.64e+5, val_loss=-2.54e+5, train_loss_epoch=-2.67e+5]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 18:  68%|██████▊   | 40/59 [00:38<00:18,  1.04it/s, v_num=0, train_loss_step=-2.86e+5, val_loss=-2.54e+5, train_loss_epoch=-2.67e+5]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 2:  93%|█████████▎| 55/59 [07:11<00:31,  0.13it/s, v_num=0, train_loss_step=-3.18e+5, val_loss=-3.58e+4, train_loss_epoch=-6.28e+3]\n",
            "Epoch 18:  76%|███████▋  | 45/59 [00:44<00:13,  1.01it/s, v_num=0, train_loss_step=-2.84e+5, val_loss=-2.54e+5, train_loss_epoch=-2.67e+5]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "Epoch 2:  95%|█████████▍| 56/59 [07:19<00:23,  0.13it/s, v_num=0, train_loss_step=-3.41e+5, val_loss=-3.58e+4, train_loss_epoch=-6.28e+3]\n",
            "Epoch 18:  86%|████████▋ | 51/59 [00:50<00:07,  1.02it/s, v_num=0, train_loss_step=-2.86e+5, val_loss=-2.54e+5, train_loss_epoch=-2.67e+5]\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "Epoch 18:  93%|█████████▎| 55/59 [00:53<00:03,  1.03it/s, v_num=0, train_loss_step=-2.84e+5, val_loss=-2.54e+5, train_loss_epoch=-2.67e+5]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "Epoch 18:  90%|████████▉ | 53/59 [00:51<00:05,  1.02it/s, v_num=0, train_loss_step=-2.92e+5, val_loss=-2.54e+5, train_loss_epoch=-2.67e+5]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:01<00:07,  1.00it/s]\u001b[A\n",
            "Epoch 18: 100%|██████████| 59/59 [00:57<00:00,  1.02it/s, v_num=0, train_loss_step=-2.43e+5, val_loss=-2.54e+5, train_loss_epoch=-2.67e+5]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:02<00:06,  0.96it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:02<00:04,  1.02it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:03<00:03,  1.09it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:04<00:02,  1.13it/s]\u001b[A\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:45:09. Total running time: 46min 34s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)          train_loss     train_loss_step     val/f1            val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000        2            1057.79     -6278.74           -55932.6           0           -35820.2         |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300       18            1192.56   -266888             -283125             0.732075   -254266           |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20            1501.92        -5.50449e+08        -5.85041e+08   0.378698        -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20            1225.47        -4.8756e+08         -5.17014e+08   0.266667        -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                                    |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:05<00:01,  1.16it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:05<00:00,  1.19it/s]\u001b[A\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-05 11:45:10,964\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 8/8 [00:06<00:00,  1.30it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \r                                                                      \u001b[A\rEpoch 18: 100%|██████████| 59/59 [01:04<00:00,  0.92it/s, v_num=0, train_loss_step=-2.43e+5, val_loss=-2.62e+5, train_loss_epoch=-2.67e+5]\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \rEpoch 18: 100%|██████████| 59/59 [01:04<00:00,  0.92it/s, v_num=0, train_loss_step=-2.43e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/b3fd7458/checkpoint_000018)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m \rEpoch 2:  98%|█████████▊| 58/59 [07:35<00:07,  0.13it/s, v_num=0, train_loss_step=-4.69e+5, val_loss=-3.58e+4, train_loss_epoch=-6.28e+3]\rEpoch 2:  98%|█████████▊| 58/59 [07:35<00:07,  0.13it/s, v_num=0, train_loss_step=-4.83e+5, val_loss=-3.58e+4, train_loss_epoch=-6.28e+3]\n",
            "Epoch 19:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-2.43e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:   2%|▏         | 1/59 [00:00<00:52,  1.11it/s, v_num=0, train_loss_step=-2.86e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:   3%|▎         | 2/59 [00:01<00:49,  1.16it/s, v_num=0, train_loss_step=-2.87e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:   5%|▌         | 3/59 [00:02<00:47,  1.17it/s, v_num=0, train_loss_step=-2.87e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:   7%|▋         | 4/59 [00:03<00:47,  1.17it/s, v_num=0, train_loss_step=-2.73e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 2: 100%|██████████| 59/59 [07:39<00:00,  0.13it/s, v_num=0, train_loss_step=-5.78e+5, val_loss=-3.58e+4, train_loss_epoch=-6.28e+3]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 19:   8%|▊         | 5/59 [00:04<00:46,  1.16it/s, v_num=0, train_loss_step=-2.75e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  10%|█         | 6/59 [00:05<00:45,  1.17it/s, v_num=0, train_loss_step=-2.86e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  12%|█▏        | 7/59 [00:05<00:44,  1.18it/s, v_num=0, train_loss_step=-2.87e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  14%|█▎        | 8/59 [00:07<00:46,  1.10it/s, v_num=0, train_loss_step=-2.85e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  15%|█▌        | 9/59 [00:08<00:49,  1.01it/s, v_num=0, train_loss_step=-2.89e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:06<00:45,  0.15it/s]\u001b[A\n",
            "Epoch 19:  17%|█▋        | 10/59 [00:10<00:50,  0.97it/s, v_num=0, train_loss_step=-2.83e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  19%|█▊        | 11/59 [00:11<00:50,  0.96it/s, v_num=0, train_loss_step=-2.92e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  20%|██        | 12/59 [00:12<00:48,  0.97it/s, v_num=0, train_loss_step=-2.75e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  22%|██▏       | 13/59 [00:13<00:46,  0.99it/s, v_num=0, train_loss_step=-2.83e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  24%|██▎       | 14/59 [00:14<00:45,  1.00it/s, v_num=0, train_loss_step=-2.88e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  25%|██▌       | 15/59 [00:14<00:43,  1.01it/s, v_num=0, train_loss_step=-2.88e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:11<00:35,  0.17it/s]\u001b[A\n",
            "Epoch 19:  27%|██▋       | 16/59 [00:15<00:42,  1.01it/s, v_num=0, train_loss_step=-2.81e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  29%|██▉       | 17/59 [00:16<00:41,  1.02it/s, v_num=0, train_loss_step=-2.93e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  31%|███       | 18/59 [00:17<00:39,  1.03it/s, v_num=0, train_loss_step=-2.85e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  32%|███▏      | 19/59 [00:18<00:38,  1.04it/s, v_num=0, train_loss_step=-2.89e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  34%|███▍      | 20/59 [00:19<00:37,  1.04it/s, v_num=0, train_loss_step=-2.88e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  36%|███▌      | 21/59 [00:20<00:36,  1.05it/s, v_num=0, train_loss_step=-2.96e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:17<00:28,  0.17it/s]\u001b[A\n",
            "Epoch 19:  37%|███▋      | 22/59 [00:20<00:35,  1.05it/s, v_num=0, train_loss_step=-2.95e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  39%|███▉      | 23/59 [00:22<00:34,  1.03it/s, v_num=0, train_loss_step=-2.61e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  41%|████      | 24/59 [00:23<00:34,  1.01it/s, v_num=0, train_loss_step=-2.9e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5] \n",
            "Epoch 19:  42%|████▏     | 25/59 [00:25<00:34,  0.99it/s, v_num=0, train_loss_step=-2.9e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  44%|████▍     | 26/59 [00:26<00:33,  0.99it/s, v_num=0, train_loss_step=-2.89e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  46%|████▌     | 27/59 [00:27<00:32,  0.99it/s, v_num=0, train_loss_step=-2.83e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  47%|████▋     | 28/59 [00:27<00:30,  1.00it/s, v_num=0, train_loss_step=-2.88e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:24<00:24,  0.16it/s]\u001b[A\n",
            "Epoch 19:  49%|████▉     | 29/59 [00:28<00:29,  1.00it/s, v_num=0, train_loss_step=-2.83e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:45:39. Total running time: 47min 4s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)          train_loss     train_loss_step     val/f1            val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000        2            1057.79     -6278.74           -55932.6           0           -35820.2         |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300       19            1256.76   -279330             -242609             0.730038   -261638           |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20            1501.92        -5.50449e+08        -5.85041e+08   0.378698        -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20            1225.47        -4.8756e+08         -5.17014e+08   0.266667        -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                                    |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 19:  51%|█████     | 30/59 [00:29<00:28,  1.01it/s, v_num=0, train_loss_step=-2.91e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  53%|█████▎    | 31/59 [00:30<00:27,  1.01it/s, v_num=0, train_loss_step=-2.8e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5] \n",
            "Epoch 19:  54%|█████▍    | 32/59 [00:31<00:26,  1.02it/s, v_num=0, train_loss_step=-2.89e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  56%|█████▌    | 33/59 [00:32<00:25,  1.02it/s, v_num=0, train_loss_step=-2.92e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:29<00:17,  0.17it/s]\u001b[A\n",
            "Epoch 19:  58%|█████▊    | 34/59 [00:33<00:24,  1.02it/s, v_num=0, train_loss_step=-2.94e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  59%|█████▉    | 35/59 [00:34<00:23,  1.02it/s, v_num=0, train_loss_step=-2.96e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  61%|██████    | 36/59 [00:35<00:22,  1.03it/s, v_num=0, train_loss_step=-2.96e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  61%|██████    | 36/59 [00:35<00:22,  1.03it/s, v_num=0, train_loss_step=-2.92e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  63%|██████▎   | 37/59 [00:35<00:21,  1.03it/s, v_num=0, train_loss_step=-2.94e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  64%|██████▍   | 38/59 [00:37<00:20,  1.02it/s, v_num=0, train_loss_step=-2.98e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  66%|██████▌   | 39/59 [00:38<00:19,  1.00it/s, v_num=0, train_loss_step=-2.99e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:36<00:12,  0.16it/s]\u001b[A\n",
            "Epoch 19:  68%|██████▊   | 40/59 [00:40<00:19,  0.99it/s, v_num=0, train_loss_step=-2.91e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  69%|██████▉   | 41/59 [00:41<00:18,  0.99it/s, v_num=0, train_loss_step=-2.96e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  71%|███████   | 42/59 [00:42<00:17,  1.00it/s, v_num=0, train_loss_step=-2.98e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  73%|███████▎  | 43/59 [00:43<00:16,  1.00it/s, v_num=0, train_loss_step=-2.75e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  75%|███████▍  | 44/59 [00:43<00:14,  1.00it/s, v_num=0, train_loss_step=-2.85e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  76%|███████▋  | 45/59 [00:44<00:13,  1.00it/s, v_num=0, train_loss_step=-2.88e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:41<00:05,  0.17it/s]\u001b[A\n",
            "Epoch 19:  78%|███████▊  | 46/59 [00:45<00:12,  1.01it/s, v_num=0, train_loss_step=-3.02e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  80%|███████▉  | 47/59 [00:46<00:11,  1.01it/s, v_num=0, train_loss_step=-3e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]   \n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m \n",
            "Validation DataLoader 0: 100%|██████████| 8/8 [00:43<00:00,  0.18it/s]\u001b[A\n",
            "Epoch 2: 100%|██████████| 59/59 [08:22<00:00,  0.12it/s, v_num=0, train_loss_step=-5.78e+5, val_loss=-2.29e+5, train_loss_epoch=-6.28e+3]\n",
            "Epoch 19:  81%|████████▏ | 48/59 [00:47<00:10,  1.01it/s, v_num=0, train_loss_step=-3.07e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/33384398/checkpoint_000002)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m \rEpoch 2: 100%|██████████| 59/59 [08:23<00:00,  0.12it/s, v_num=0, train_loss_step=-5.78e+5, val_loss=-2.29e+5, train_loss_epoch=-1.6e+5] \n",
            "Epoch 19:  83%|████████▎ | 49/59 [00:48<00:09,  1.02it/s, v_num=0, train_loss_step=-2.82e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  85%|████████▍ | 50/59 [00:48<00:08,  1.02it/s, v_num=0, train_loss_step=-2.94e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  86%|████████▋ | 51/59 [00:49<00:07,  1.03it/s, v_num=0, train_loss_step=-3.08e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  88%|████████▊ | 52/59 [00:49<00:06,  1.04it/s, v_num=0, train_loss_step=-2.98e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  90%|████████▉ | 53/59 [00:50<00:05,  1.05it/s, v_num=0, train_loss_step=-2.89e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  92%|█████████▏| 54/59 [00:50<00:04,  1.06it/s, v_num=0, train_loss_step=-2.64e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  93%|█████████▎| 55/59 [00:51<00:03,  1.06it/s, v_num=0, train_loss_step=-2.95e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  95%|█████████▍| 56/59 [00:52<00:02,  1.07it/s, v_num=0, train_loss_step=-2.78e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19:  97%|█████████▋| 57/59 [00:53<00:01,  1.07it/s, v_num=0, train_loss_step=-3.01e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 3:   0%|          | 0/59 [00:00<?, ?it/s, v_num=0, train_loss_step=-5.78e+5, val_loss=-2.29e+5, train_loss_epoch=-1.6e+5]\n",
            "Epoch 19:  98%|█████████▊| 58/59 [00:54<00:00,  1.06it/s, v_num=0, train_loss_step=-2.95e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Epoch 19: 100%|██████████| 59/59 [00:55<00:00,  1.07it/s, v_num=0, train_loss_step=-2.89e+5, val_loss=-2.62e+5, train_loss_epoch=-2.79e+5]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  12%|█▎        | 1/8 [00:00<00:04,  1.51it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  25%|██▌       | 2/8 [00:01<00:04,  1.43it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 3/8 [00:02<00:03,  1.39it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  50%|█████     | 4/8 [00:02<00:02,  1.37it/s]\u001b[A\n",
            "Trial status: 2 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:46:10. Total running time: 47min 34s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)          train_loss     train_loss_step     val/f1            val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000        3            1564.89   -159572             -577558             0          -229024           |\n",
            "| TorchTrainer_b3fd7458   RUNNING                           4                     2300                        1                      300       19            1256.76   -279330             -242609             0.730038   -261638           |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20            1501.92        -5.50449e+08        -5.85041e+08   0.378698        -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20            1225.47        -4.8756e+08         -5.17014e+08   0.266667        -4.6718e+08  |\n",
            "| TorchTrainer_0b4850a9   PENDING                           3                     1400                        1                     1200                                                                                                    |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  62%|██████▎   | 5/8 [00:03<00:02,  1.38it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  75%|███████▌  | 6/8 [00:04<00:01,  1.37it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "Validation DataLoader 0:  88%|████████▊ | 7/8 [00:05<00:00,  1.36it/s]\u001b[A\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-05 11:46:12,012\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 8/8 [00:05<00:00,  1.49it/s]\u001b[A\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \r                                                                      \u001b[A\rEpoch 19: 100%|██████████| 59/59 [01:00<00:00,  0.97it/s, v_num=0, train_loss_step=-2.89e+5, val_loss=-2.83e+5, train_loss_epoch=-2.79e+5]\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \rEpoch 19: 100%|██████████| 59/59 [01:00<00:00,  0.97it/s, v_num=0, train_loss_step=-2.89e+5, val_loss=-2.83e+5, train_loss_epoch=-2.89e+5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35/b3fd7458/checkpoint_000019)\n",
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m `Trainer.fit` stopped: `max_epochs=20` reached.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(RayTrainWorker pid=9465)\u001b[0m \rEpoch 19: 100%|██████████| 59/59 [01:01<00:00,  0.97it/s, v_num=0, train_loss_step=-2.89e+5, val_loss=-2.83e+5, train_loss_epoch=-2.89e+5]\n",
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m \rEpoch 3:   2%|▏         | 1/59 [00:07<07:14,  0.13it/s, v_num=0, train_loss_step=-5.78e+5, val_loss=-2.29e+5, train_loss_epoch=-1.6e+5]\rEpoch 3:   2%|▏         | 1/59 [00:07<07:14,  0.13it/s, v_num=0, train_loss_step=-5.3e+5, val_loss=-2.29e+5, train_loss_epoch=-1.6e+5] \n",
            "\n",
            "Trial TorchTrainer_b3fd7458 completed after 20 iterations at 2025-05-05 11:46:13. Total running time: 47min 37s\n",
            "+----------------------------------------------------------+\n",
            "| Trial TorchTrainer_b3fd7458 result                       |\n",
            "+----------------------------------------------------------+\n",
            "| checkpoint_dir_name                    checkpoint_000019 |\n",
            "| time_this_iter_s                                 61.0162 |\n",
            "| time_total_s                                  1317.78094 |\n",
            "| training_iteration                                    20 |\n",
            "| epoch                                                 19 |\n",
            "| step                                                1180 |\n",
            "| train_loss                                  -288726.6875 |\n",
            "| train_loss_epoch                            -288726.6875 |\n",
            "| train_loss_step                            -288593.34375 |\n",
            "| val/f1                                            0.7197 |\n",
            "| val_loss                                    -282806.8125 |\n",
            "+----------------------------------------------------------+\n",
            "Epoch 3:   3%|▎         | 2/59 [00:19<09:02,  0.11it/s, v_num=0, train_loss_step=-4.04e+5, val_loss=-2.29e+5, train_loss_epoch=-1.6e+5]\n",
            "Epoch 3:   5%|▌         | 3/59 [00:25<08:00,  0.12it/s, v_num=0, train_loss_step=-4.81e+5, val_loss=-2.29e+5, train_loss_epoch=-1.6e+5]\n",
            "\n",
            "Trial TorchTrainer_0b4850a9 started with configuration:\n",
            "+---------------------------------------------+\n",
            "| Trial TorchTrainer_0b4850a9 config          |\n",
            "+---------------------------------------------+\n",
            "| train_loop_config/depth                   3 |\n",
            "| train_loop_config/ffn_hidden_dim       1400 |\n",
            "| train_loop_config/ffn_num_layers          1 |\n",
            "| train_loop_config/message_hidden_dim   1200 |\n",
            "+---------------------------------------------+\n",
            "Epoch 3:   7%|▋         | 4/59 [00:34<07:50,  0.12it/s, v_num=0, train_loss_step=-4.93e+5, val_loss=-2.29e+5, train_loss_epoch=-1.6e+5]\n",
            "\n",
            "Trial status: 3 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:46:40. Total running time: 48min 4s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)          train_loss     train_loss_step     val/f1            val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000        3            1564.89   -159572             -577558             0          -229024           |\n",
            "| TorchTrainer_0b4850a9   RUNNING                           3                     1400                        1                     1200                                                                                                    |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20            1501.92        -5.50449e+08        -5.85041e+08   0.378698        -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20            1225.47        -4.8756e+08         -5.17014e+08   0.266667        -4.6718e+08  |\n",
            "| TorchTrainer_b3fd7458   TERMINATED                        4                     2300                        1                      300       20            1317.78   -288727             -288593             0.719697   -282807           |\n",
            "| TorchTrainer_5e50785f   PENDING                           5                     1500                        2                     1100                                                                                                    |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 3:   8%|▊         | 5/59 [00:40<07:18,  0.12it/s, v_num=0, train_loss_step=-6.54e+5, val_loss=-2.29e+5, train_loss_epoch=-1.6e+5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(TorchTrainer pid=14937)\u001b[0m Started distributed worker processes: \n",
            "\u001b[36m(TorchTrainer pid=14937)\u001b[0m - (node_id=85d1826776a8b04eb71395ec08130d8b282f617a724b5c6551ccdd9f, ip=172.28.0.12, pid=15066) world_rank=0, local_rank=0, node_rank=0\n",
            "\u001b[36m(RayTrainWorker pid=15066)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m \rEpoch 3:  10%|█         | 6/59 [00:48<07:11,  0.12it/s, v_num=0, train_loss_step=-6.54e+5, val_loss=-2.29e+5, train_loss_epoch=-1.6e+5]\rEpoch 3:  10%|█         | 6/59 [00:48<07:11,  0.12it/s, v_num=0, train_loss_step=-2.91e+5, val_loss=-2.29e+5, train_loss_epoch=-1.6e+5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=15066)\u001b[0m Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "\u001b[36m(RayTrainWorker pid=15066)\u001b[0m GPU available: False, used: False\n",
            "\u001b[36m(RayTrainWorker pid=15066)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(RayTrainWorker pid=15066)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[36m(RayTrainWorker pid=15066)\u001b[0m 2025-05-05 11:46:56.679300: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(RayTrainWorker pid=15066)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "\u001b[36m(RayTrainWorker pid=15066)\u001b[0m E0000 00:00:1746445616.736201   15183 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(RayTrainWorker pid=15066)\u001b[0m E0000 00:00:1746445616.751603   15183 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(RayTrainWorker pid=15066)\u001b[0m 2025-05-05 11:46:56.801761: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(RayTrainWorker pid=15066)\u001b[0m To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(RayTrainWorker pid=8293)\u001b[0m \rEpoch 3:  12%|█▏        | 7/59 [00:55<06:49,  0.13it/s, v_num=0, train_loss_step=-2.91e+5, val_loss=-2.29e+5, train_loss_epoch=-1.6e+5]\rEpoch 3:  12%|█▏        | 7/59 [00:55<06:49,  0.13it/s, v_num=0, train_loss_step=-6.75e+5, val_loss=-2.29e+5, train_loss_epoch=-1.6e+5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-05 11:47:01,815\tWARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
            "2025-05-05 11:47:01,830\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/content/chemprop/examples/hpopt/ray_results/TorchTrainer_2025-05-05_10-58-35' in 0.0137s.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial status: 3 TERMINATED | 2 RUNNING | 1 PENDING\n",
            "Current time: 2025-05-05 11:47:01. Total running time: 48min 26s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/0 GPUs\n",
            "Current best trial: 8da927e1 with val_loss=-533901632.0 and params={'train_loop_config': {'depth': 2, 'ffn_hidden_dim': 2000, 'ffn_num_layers': 2, 'message_hidden_dim': 500}}\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name              status         ...loop_config/depth     ...ig/ffn_hidden_dim     ...ig/ffn_num_layers     ...essage_hidden_dim     iter     total time (s)          train_loss     train_loss_step     val/f1            val_loss |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| TorchTrainer_33384398   RUNNING                           2                      600                        2                     2000        3            1564.89   -159572             -577558             0          -229024           |\n",
            "| TorchTrainer_0b4850a9   RUNNING                           3                     1400                        1                     1200                                                                                                    |\n",
            "| TorchTrainer_8da927e1   TERMINATED                        2                     2000                        2                      500       20            1501.92        -5.50449e+08        -5.85041e+08   0.378698        -5.33902e+08 |\n",
            "| TorchTrainer_c410a352   TERMINATED                        2                     2200                        2                      400       20            1225.47        -4.8756e+08         -5.17014e+08   0.266667        -4.6718e+08  |\n",
            "| TorchTrainer_b3fd7458   TERMINATED                        4                     2300                        1                      300       20            1317.78   -288727             -288593             0.719697   -282807           |\n",
            "| TorchTrainer_5e50785f   PENDING                           5                     1500                        2                     1100                                                                                                    |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(RayTrainWorker pid=15066)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n",
            "\u001b[36m(RayTrainWorker pid=15066)\u001b[0m \n",
            "\u001b[36m(RayTrainWorker pid=15066)\u001b[0m   | Name            | Type                    | Params | Mode \n",
            "\u001b[36m(RayTrainWorker pid=15066)\u001b[0m --------------------------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=15066)\u001b[0m 0 | message_passing | BondMessagePassing      | 3.1 M  | train\n",
            "\u001b[36m(RayTrainWorker pid=15066)\u001b[0m 1 | agg             | MeanAggregation         | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=15066)\u001b[0m 2 | bn              | BatchNorm1d             | 2.4 K  | train\n",
            "\u001b[36m(RayTrainWorker pid=15066)\u001b[0m 3 | predictor       | BinaryClassificationFFN | 1.7 M  | train\n",
            "\u001b[36m(RayTrainWorker pid=15066)\u001b[0m 4 | X_d_transform   | Identity                | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=15066)\u001b[0m 5 | metrics         | ModuleList              | 0      | train\n",
            "\u001b[36m(RayTrainWorker pid=15066)\u001b[0m --------------------------------------------------------------------\n",
            "\u001b[36m(RayTrainWorker pid=15066)\u001b[0m 4.8 M     Trainable params\n",
            "\u001b[36m(RayTrainWorker pid=15066)\u001b[0m 0         Non-trainable params\n",
            "\u001b[36m(RayTrainWorker pid=15066)\u001b[0m 4.8 M     Total params\n",
            "\u001b[36m(RayTrainWorker pid=15066)\u001b[0m 19.024    Total estimated model params size (MB)\n",
            "\u001b[36m(RayTrainWorker pid=15066)\u001b[0m 24        Modules in train mode\n",
            "\u001b[36m(RayTrainWorker pid=15066)\u001b[0m 0         Modules in eval mode\n",
            "\u001b[36m(RayTrainWorker pid=15066)\u001b[0m /usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n",
            "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:05<00:05,  0.20it/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "ray.shutdown()\n",
        "ray.init()\n",
        "\n",
        "# define the scaler before using it\n",
        "\n",
        "scheduler = FIFOScheduler()\n",
        "\n",
        "# Scaling config controls the resources used by Ray\n",
        "scaling_config = ScalingConfig(\n",
        "    num_workers=1,\n",
        "    use_gpu= False, # change to True if you want to use GPU\n",
        ")\n",
        "\n",
        "# Checkpoint config controls the checkpointing behavior of Ray\n",
        "checkpoint_config = CheckpointConfig(\n",
        "    num_to_keep=1, # number of checkpoints to keep\n",
        "    checkpoint_score_attribute=\"val_loss\", # Save the checkpoint based on this metric\n",
        "    checkpoint_score_order=\"min\", # Save the checkpoint with the lowest metric value\n",
        ")\n",
        "\n",
        "run_config = RunConfig(\n",
        "    checkpoint_config=checkpoint_config,\n",
        "    storage_path=hpopt_save_dir / \"ray_results\", # directory to save the results\n",
        ")\n",
        "\n",
        "ray_trainer = TorchTrainer(\n",
        "    lambda config: train_model(\n",
        "        config, train_dset, val_dset, num_workers, scaler\n",
        "    ),\n",
        "    scaling_config=scaling_config,\n",
        "    run_config=run_config,\n",
        ")\n",
        "\n",
        "search_alg = HyperOptSearch(\n",
        "    n_initial_points=1, # number of random evaluations before tree parzen estimators\n",
        "    random_state_seed=42,\n",
        ")\n",
        "\n",
        "# OptunaSearch is another search algorithm that can be used\n",
        "# search_alg = OptunaSearch()\n",
        "\n",
        "tune_config = tune.TuneConfig(\n",
        "    metric=\"val_loss\",\n",
        "    mode=\"min\",\n",
        "    num_samples=20, # number of trials to run\n",
        "    scheduler=scheduler,\n",
        "    search_alg=search_alg,\n",
        "    trial_dirname_creator=lambda trial: str(trial.trial_id), # shorten filepaths\n",
        "\n",
        ")\n",
        "\n",
        "tuner = tune.Tuner(\n",
        "    ray_trainer,\n",
        "    param_space={\n",
        "        \"train_loop_config\": search_space,\n",
        "    },\n",
        "    tune_config=tune_config,\n",
        ")\n",
        "\n",
        "# Start the hyperparameter search\n",
        "results = tuner.fit()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7lxD6GjIRa1"
      },
      "source": [
        "## Hyperparameter optimization results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJCwOJP3IRa1",
        "outputId": "c749e887-a659-4370-fb0b-edad04bf94b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ResultGrid<[\n",
              "  Result(\n",
              "    metrics={'train_loss': 0.09904231131076813, 'train_loss_step': 0.16821686923503876, 'val/rmse': 0.8613682389259338, 'val/mae': 0.7006751298904419, 'val_loss': 0.7419552206993103, 'train_loss_epoch': 0.09904231131076813, 'epoch': 19, 'step': 40},\n",
              "    path='/home/knathan/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-10-22_09-03-37/f1a6e41a',\n",
              "    filesystem='local',\n",
              "    checkpoint=Checkpoint(filesystem=local, path=/home/knathan/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-10-22_09-03-37/f1a6e41a/checkpoint_000019)\n",
              "  ),\n",
              "  Result(\n",
              "    metrics={'train_loss': 0.06969495117664337, 'train_loss_step': 0.11989812552928925, 'val/rmse': 0.902579665184021, 'val/mae': 0.7176367044448853, 'val_loss': 0.8146500587463379, 'train_loss_epoch': 0.06969495117664337, 'epoch': 19, 'step': 40},\n",
              "    path='/home/knathan/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-10-22_09-03-37/d775c15d',\n",
              "    filesystem='local',\n",
              "    checkpoint=Checkpoint(filesystem=local, path=/home/knathan/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-10-22_09-03-37/d775c15d/checkpoint_000019)\n",
              "  )\n",
              "]>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AjPJZhkIRa1",
        "outputId": "4a34e809-b0c9-4aa0-e88e-92d709409358"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_loss</th>\n",
              "      <th>train_loss_step</th>\n",
              "      <th>val/rmse</th>\n",
              "      <th>val/mae</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>train_loss_epoch</th>\n",
              "      <th>epoch</th>\n",
              "      <th>step</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>checkpoint_dir_name</th>\n",
              "      <th>...</th>\n",
              "      <th>pid</th>\n",
              "      <th>hostname</th>\n",
              "      <th>node_ip</th>\n",
              "      <th>time_since_restore</th>\n",
              "      <th>iterations_since_restore</th>\n",
              "      <th>config/train_loop_config/depth</th>\n",
              "      <th>config/train_loop_config/ffn_hidden_dim</th>\n",
              "      <th>config/train_loop_config/ffn_num_layers</th>\n",
              "      <th>config/train_loop_config/message_hidden_dim</th>\n",
              "      <th>logdir</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.099042</td>\n",
              "      <td>0.168217</td>\n",
              "      <td>0.861368</td>\n",
              "      <td>0.700675</td>\n",
              "      <td>0.741955</td>\n",
              "      <td>0.099042</td>\n",
              "      <td>19</td>\n",
              "      <td>40</td>\n",
              "      <td>1729602279</td>\n",
              "      <td>checkpoint_000019</td>\n",
              "      <td>...</td>\n",
              "      <td>24873</td>\n",
              "      <td>Knathan-Laptop</td>\n",
              "      <td>172.31.231.162</td>\n",
              "      <td>49.881516</td>\n",
              "      <td>20</td>\n",
              "      <td>2</td>\n",
              "      <td>2000</td>\n",
              "      <td>2</td>\n",
              "      <td>500</td>\n",
              "      <td>f1a6e41a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.069695</td>\n",
              "      <td>0.119898</td>\n",
              "      <td>0.902580</td>\n",
              "      <td>0.717637</td>\n",
              "      <td>0.814650</td>\n",
              "      <td>0.069695</td>\n",
              "      <td>19</td>\n",
              "      <td>40</td>\n",
              "      <td>1729602299</td>\n",
              "      <td>checkpoint_000019</td>\n",
              "      <td>...</td>\n",
              "      <td>24953</td>\n",
              "      <td>Knathan-Laptop</td>\n",
              "      <td>172.31.231.162</td>\n",
              "      <td>56.653336</td>\n",
              "      <td>20</td>\n",
              "      <td>2</td>\n",
              "      <td>2200</td>\n",
              "      <td>2</td>\n",
              "      <td>400</td>\n",
              "      <td>d775c15d</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 27 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   train_loss  train_loss_step  val/rmse   val/mae  val_loss  \\\n",
              "0    0.099042         0.168217  0.861368  0.700675  0.741955   \n",
              "1    0.069695         0.119898  0.902580  0.717637  0.814650   \n",
              "\n",
              "   train_loss_epoch  epoch  step   timestamp checkpoint_dir_name  ...    pid  \\\n",
              "0          0.099042     19    40  1729602279   checkpoint_000019  ...  24873   \n",
              "1          0.069695     19    40  1729602299   checkpoint_000019  ...  24953   \n",
              "\n",
              "         hostname         node_ip time_since_restore iterations_since_restore  \\\n",
              "0  Knathan-Laptop  172.31.231.162          49.881516                       20   \n",
              "1  Knathan-Laptop  172.31.231.162          56.653336                       20   \n",
              "\n",
              "   config/train_loop_config/depth  config/train_loop_config/ffn_hidden_dim  \\\n",
              "0                               2                                     2000   \n",
              "1                               2                                     2200   \n",
              "\n",
              "   config/train_loop_config/ffn_num_layers  \\\n",
              "0                                        2   \n",
              "1                                        2   \n",
              "\n",
              "  config/train_loop_config/message_hidden_dim    logdir  \n",
              "0                                         500  f1a6e41a  \n",
              "1                                         400  d775c15d  \n",
              "\n",
              "[2 rows x 27 columns]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# results of all trials\n",
        "result_df = results.get_dataframe()\n",
        "result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3l-U482eIRa1",
        "outputId": "df2c2f43-b834-4df9-d22d-641f6bed0fbd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'depth': 2,\n",
              " 'ffn_hidden_dim': 2000,\n",
              " 'ffn_num_layers': 2,\n",
              " 'message_hidden_dim': 500}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# best configuration\n",
        "best_result = results.get_best_result()\n",
        "best_config = best_result.config\n",
        "best_config['train_loop_config']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIIAjMZSIRa1",
        "outputId": "05d414b8-b172-4061-8746-020419274b27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best model checkpoint path: /home/knathan/chemprop/examples/hpopt/ray_results/TorchTrainer_2024-10-22_09-03-37/f1a6e41a/checkpoint_000019/checkpoint.ckpt\n"
          ]
        }
      ],
      "source": [
        "# best model checkpoint path\n",
        "best_result = results.get_best_result()\n",
        "best_checkpoint_path = Path(best_result.checkpoint.path) / \"checkpoint.ckpt\"\n",
        "print(f\"Best model checkpoint path: {best_checkpoint_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNjtwQT0IRa1"
      },
      "outputs": [],
      "source": [
        "ray.shutdown()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}